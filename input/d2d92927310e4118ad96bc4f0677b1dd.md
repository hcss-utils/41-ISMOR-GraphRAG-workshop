# ID: d2d92927310e4118ad96bc4f0677b1dd
# Title: Undergoverned Spaces and the Challenges of Complex Infinite Competition
# Database: Google Scholar
# Year: 2022.0
# Fulltext:
The RAND Corporation is a research organization that develops solutions to public policy challenges to help make communities
tain models regarding how the world works, and uncertain valuations of possible states of the world. Collectively, they discuss new approaches for harnessing computing technologies to support strategic decisionmaking without relying on prediction, an array of decisionmaking tools and approaches for aggregating and exploiting fragmented knowledge from quantitative and qualitative sources, and the role of multiple stakeholders in decisionmaking processes-serving as both the subjects of research and the cocreators and consumers of research and analysis.
The fourth and final part presents six chapters covering several lines of investment and exploration for engaging in UGS. These chapters explore capabilities that would enhance DoD's ability to understand and adapt to changing conditions within UGS. Although these chapters are diverse in their explorations, each chapter centers on decisionmakers and decisionmaking in one form or another-some chapters discuss how DoD and the NSE can improve decisionmaking processes using organizational digital twin models or new approaches to gaming; others are intended to provide a better understanding of soldiers and society through microlevel models of human agents. Collectively, the chapters presented in this part, as well as the second and third parts, offer prognostic perspectives examining the value of potential investments in emerging technologies and concepts that meet the challenges posed in the first part.
The examination of UGS performed in this report is preliminary, but the following themes have emerged:
• UGS will remain a strategic challenge regardless of whether U.S. national strategy emphasizes great-power competition, the promotion and expansion of international governance institutions, the countering of violent extremist groups, or other objectives. • UGS challenge the decisionmaking processes of DoD and the NSE, and effective engagement will require greater emphasis on adapting how decisions are made, who participates in making them, and how policy and operations are executed in complex, openended competition. • Long-term competition will require new concepts and approaches that improve the integration of research, analysis, operations, and strategy. • Investments in the social sciences are crucial to better understanding of and competing in UGS. • UGS will require new tools and rationales for policymaking that pay explicit attention to uncertainty and seek robustness and adaptiveness as a means for coping with it. • AI will be important but will have a limited impact on strategic decisionmaking and planning in UGS. • Research and analysis to support UGS will need more-robust infrastructure and organizations that can continue to accumulate knowledge and support the development of technologies as policy organizations adapt their structures, goals, and operations at a faster pace.
Although these themes do not present a comprehensive list of all the needs required to understand and compete in UGS, they do offer a starting point from which policymakers, researchers, and research sponsors can think about how to better equip the United States for long-term competition in spaces where traditional approaches have proven unsuccessful.
In seeking to develop the concept of UGS, it is necessary to start with defining governance itself. This is not a simple task but serves as a useful starting point for understanding what is missing or deficient when determining whether something is undergoverned. As a starting point, governance will be defined in functional or institutional terms.
A functional or institutional view of governance considers how individuals within a system manage interdependence, or "the sum of the many ways individuals and institutions, public and private, manage their common affairs." 2 Broadly, governance refers to the rules for coordinating behavior and managing conflicts established from the top down by formal authorities; norms that develop from the bottom up as a matter of practice; identities that prescribe the roles, rights, privileges, and obligations that actors have toward one another; and the shifting interpretations of these practices that occur over time-for example, the continual interpretation and reinterpretation of the U.S. Constitution. 3 Questions that arise, then, deal with the substance of the rules; who participates in developing, changing, and enforcing them; and whether the rules facilitate behaviors, most notably exchanges of goods, services, information, and more, that would otherwise not occur in their absence. 4 These issues are further discussed in Chapters Two and Four.
Even with a definition of governance, UGS remain difficult to define. Efforts to provide a concise and clear definition with sharp contours that allow for uncontestable categorization or quantification are unlikely to emerge. Nevertheless, examining what should be encompassed in the definition of UGS-what may be considered and what should be excludedprovides a useful point of departure, and later chapters in this report offer a richer investigation of these issues. Here, however, it is enough to note two important conceptual challenges. Any discussion of international relations and national security starts with the state as the point of departure, from which many questions proceed:
• Are relations between states governed by a higher authority or constrained by strong institutions?
• Are states threatened by the actions of nonstate actors that operate outside national or international law? • Are states governed in a fashion that produces ethical and effective decisions and actions?
Whether analysts are specifically interested in these questions or others, the workings and failures of the consolidated state serve as the frame of reference from which almost all assessments proceed. This is not to suggest that a state-centric view of international relations and security is always the most important framework (or even a relevant one) to consider, but it is almost always the one with which alternatives are compared.
The primacy of state-centric analysis carries with it the seeds of ambiguity that limit definitions of UGS. Specifically, because the state is an unnatural kind-a unit of analysis that does not exist outside human consciousness-it lacks an objective, independent basis in reality and thus remains contested. 5 Whatever problems exist about defining the state as the central node of governance from which national and international order and stability emanate also affect any investigation into its weakness or absence. 6  The focus on how governance occurs-i.e., the institutional perspective-allows for the shedding of much of the conceptual baggage inherited from state-centric models of governance. 7 Broadened perspectives on management, policy, and security have encouraged increasingly functional perspectives on the purposes and mechanisms by which resources are allocated and exchanged within and between populations, and on how these facilitate or inhibit exchange. From this functional perspective, governance contains a broad variety of dimensions-notably the allocation, in the context of what both states and nonstate actors do, of rights, privileges, obligations, wealth, and services (such as health care, security, and justice).
These functional perspectives allow observers to identify (1) aspects of governance in areas that are outside the purview of traditional analyses of national security, such as the governance of markets and data, and ( 
2
Keim Campbell, Michael O'Rourke, and Matthew H. Slater, eds.
MIT Press, 2011;
Nancy Cartwright and Rosa Runhardt, "Measurement," in Nancy Cartwright and Eleonora Montuschi, eds.
Migdal, State in Society: Studying How States and Societies Transform and Constitute One Another, New York: Cambridge University Press, 2001
Pauly, eds., Complex Sovereignty, Toronto, Canada: University of Toronto Press, 2007;
Christopher Pierson, The Modern State, 3rd ed
doing so, observers can also identify pathways from undergoverned to governed status (and back)-many of which exist outside the state or its alternatives.
The concept of undergovernance applies to the traditional domain of security at the domestic and international levels of analysis; the inner workings of complex organizations in which failures to coordinate inputs and outputs affect competitiveness in the production or consumption of goods, services, and information; and domains in which state and nonstate actors are on equal footing, as is increasingly the case in such digital domains as cyberspace. Thus, there are many ways in which a space may be considered undergoverned (see the text box).
Given definitional challenges discussed here and in subsequent chapters, this report has adopted a big-tent approach that has allowed the authors (and the interviewees whose perspectives are presented in Chapter Five) to develop and advance arguments based on their own views of the needs, challenges, and opportunities posed by UGS. Some chapters in this report take on the challenge of defining UGS, or at least offer criteria that would allow future research to proceed on firmer conceptual or empirical foundations. Other chapters employ instrumentally useful definitions that allow for the advancement of specific arguments about science and technology, planning and strategy, and engagement in ways that would improve U.S. competitiveness regardless of the ways in which specific definitions of UGS might evolve. The result is that definitions of UGS remain a work in progress, awaiting future study to be honed into a reliable analytic concept.
Between States • Disregard for international law, institutions, or norms 
The 2017 National Security Strategy (NSS) and 2018 National Defense Strategy (NDS) have each placed long-term competition with strategic rivals at the forefront of national security and military planning. 8 As the NDS stated:
The central challenge to U.S. prosperity and security is the reemergence of long-term, strategic competition by what the [NSS] classifies as revisionist powers. It is increasingly clear that China and Russia want to shape a world consistent with their authoritarian model-gaining veto authority over other nations' economic, diplomatic, and security decisions. 9   For many, achieving the vision set forward in the NSS and NDS requires shifting attention away from weak, failing, and failed states; nonstate actors; and the stresses caused by technological and climate change, instead directing attention toward enduring and defeating the nation's most capable rivals-Russia and China-in direct political competition and military conflict. The result is a return to great-power politics and a balance-of-power approach to dealing with the world's most formidable and consolidated governments-a far cry from the challenges posed by state weakness.
Such a characterization is misleading for several reasons. First, the notion that engaging in UGS constitutes an alternative to great-power conflict mischaracterizes the conduct of long-term competition between great powers. As numerous historians have noted, the Cold War-the last protracted great-power competition-may have been waged over the political influence and security of postwar Europe, but its hot conflicts were fought in Asia, Africa, and Central and South America, with Korea, Cuba, Vietnam, Angola, and Afghanistan providing a few prominent examples. 10 Contemporary challenges posed by gray-zone conflict, 11 hybrid warfare, 12 New-Generation Warfare, 13 virtual societal warfare, 14 and more suggest that great-power rivalries have given rise to a new Great Game, in which the great powers compete for control over territory, access to resources and markets, and international influence on a global scale. 15  Second, the NDS also identified broad U.S. interest in preserving the rules-based international order, which forms the backbone of international prosperity and security. It noted that this order is weakening and facing challenges by actors that simultaneously seek to reap rewards from the security and opportunities its institutions provide while undercutting their principles. 16 As John Ikenberry, one of the most forceful proponents of the international order, noted:
Great powers-China and Russia-are offering forceful illiberal challenges to the Western liberal order. Equally profound challenges are coming from within the liberal democratic world itself-reactionary nationalism, populist authoritarianism, and attacks on openness and the rule of law. 17   Central to international order is the belief that the construction, maintenance, and modification of international institutions are not an alternative to balance-of-power politics; 18  rather, defining and enforcing their prescriptions and prohibitions serve as a venue for conducting balance-of-power politics. 19 Thus, the United States faces the challenge of determining when to bolster, reform, rebuild, or abandon the complex web of international institutions that form the spine of the international order. 20  Third, the NSS does not focus exclusively on great-power competition. It notes the continued need to pursue threats to their sources, such as terror groups and criminal organizations. This requires maintaining capabilities to monitor, influence, and project power into areas governed by states that are unwilling or unable to fulfill their obligations to prevent attacks emanating from their physical or virtual territory. By implication, then, the NSS accepts that the United States will continue to be threatened by regional powers and nonstate actors; the United States requires the ability to counter and project power against aggressors.
Viewed together, shifts in emphasis toward great-power rivalries with Russia and China could alter the logic of interventions into UGS, but they do not eliminate demand for doing so. Conflicts in Syria, the South China Sea, the Arctic, the cyber world, and elsewhere will all continue to demand the attention of the United States regardless of what Russia or China might do.
Therefore, a pragmatic consideration of UGS and their place in national security policy and strategy is warranted. Specifically, the features of UGS in terms of their connectivity with national security priorities and governance institutions (be they international, national, corporate, or nonstate) might provide the best arguments for engaging or abstaining within UGS. Considerations and consequences that motivate engaging within UGS are presented in the related text box and discussed in greater detail in Chapter Three.
The JPC has served as the backbone for U.S. Department of Defense (DoD) planning and engagement since the mid-2000s and presents a cyclical model that divides relations between states into phases labeled Phase 0 through Phase V (Figure 
1
A recent alternative for planning engagements in the international system was developed as part of a model of adaptive campaigning. 21 This model adopts a complex adaptive systems approach to engagement, seeing military forces as embedded in the environment, not standing separate from it. 22 In this model, organizations compete for a better understanding of the environment and the opportunity to shape it through the performance of the ASDA cycle 
(Figure 1.2)
Whereas the JPC imagines a cyclic pattern of conflict, suggesting that military conflict might be an inevitable step in the relations between competitors and therefore encouraging a race to achieve a decisive advantage, the ASDA cycle is more flexible and reflexive, demanding that organizations continuously reevaluate their beliefs about their positions in the system and relations with others. This does not preclude the possibility of conflict and violence, but it does allow that parties could learn how to maintain a contentious relationship beneath thresholds that would trigger a movement into Phase II or III of the JPC and could even seek mutually beneficial, cooperative relations. 21 Head Modernisation and Strategic Planning-Army, 2009; Michael Bassingthwaighte, Adaptive Campaigning Applied: Australian Army Operations in Iraq and Afghanistan, Fort Leavenworth, Kan.: School of Advanced Military Studies, United States Army Command and General Staff College, 2010. 22 Head Modernisation and Strategic Planning- 
Army, 2009
Journal, Vol. 6, No. 3, Summer 2009
Increasing numbers of actors capable of entering and competing in a given space Increased complexity resulting from heterogeneous goals and capabilities of competitors Limited presence or weakness of existing governance institutions within a space Risks of undermining established governance institutions, whether formal rules, such as internal law, or informal norms of behavior that make actors less predictable
Risks posed to undermining governance institutions on which stable and managed behaviors in other spaces rely
The need to cope with novelty and uncertainty in an open-ended system
A continuous demand for shifting organizational designs and decisionmaking processes to adapt to changes in the composition of the space and the behaviors and capabilities of the actors within it
The ASDA cycle opens the door to a broader set of concepts for engaging in UGS. These concepts could vary in terms of formal planning structures, but they all emphasize three core features: 
(1)
(3)
1
The first part of this report, Chapters Two through Six, provides an extended discussion of the UGS concept and the demands for engaging in UGS. These chapters offer a variety of perspectives taken from the diverse expertise and experiences of the authors. The authors of these chapters were specifically asked to look at the theoretical and practical problems of defining and engaging in UGS from a diagnostic perspective; i.e., to present a body of experience and concepts that shed light on the challenges posed by UGS without specific demands for offering solutions, though each identifies promising paths. Together, these chapters address theoretical, empirical issues associated with defining UGS and the practical demands of engaging in them.
In Chapter Two, Aaron B. Frank examines the prospect of developing a formal definition of UGS and examines the many ways that spaces can be undergoverned. In doing so, he identifies how UGS might threaten national security and offers a set of considerations that policymakers and military planners should think about when determining whether the approaches to planning and engagement described in this report should be pursued in lieu of more-conventional methods and processes.
In Chapter Three, Adam R. Grissom explores the puzzle of DoD's historically unimpressive performance in UGS, finding that the fundamental challenge may be analytical in nature. He describes DoD's apparent weaknesses in accurately perceiving complex and informal social, political, economic, and military dynamics in UGS, and he concludes that new analytical methods are required to allow DoD to develop the improved understanding necessary to achieve better operational and strategic results.
In Chapter Four, Jonathan S. Blake examines the concept of UGS through the lens of contemporary theories and empirical models of governance. He notes that before determining whether an area or issue is undergoverned (a quantitative notion), it is necessary to first understand qualitative properties about who governs and how. He notes that the international system is rife with examples of functional governance that only appear undergoverned if the state and the provision of governmental goods and services are conflated. Instead, he notes that alternative governance is a necessary concept that allows broad and unconven-tional arrangements on how resources are allocated within societies and that alternative governance is a necessary component of any operational definition of UGS.
In Chapter Five, Gabrielle Tarini and Kelly Elizabeth Eusebi provide insights based on a set of 33 semistructured interviews with policymakers, academic researchers, and technologists. In these interviews, a diverse group of experts were asked to identify challenges and opportunities for DoD and the National Security Enterprise (NSE) to improve adaptability and ability to succeed in long-term competition. Interviewees identified many barriers to success, starting from the fact that UGS have traditionally been low-priority environments and efforts to engage in them have therefore been hampered by numerous challenges, such as limited and inconsistent access to resources, limited and often low-quality information and intelligence, and missing analytic capabilities tailored to exploring the space of the possible and the mitigation of risks (as opposed to increasing the efficiency of resource allocations). Likewise, the authors identified several opportunities to increase the competitive and adaptive capabilities of the United States and its national security organizations. Among these opportunities are multiple investmentsspanning computational tools to organizational designs and incentives-that share a common purpose: sustained commitments to exploration and discovery of new frameworks for understanding the environment and solutions for problems within it.
In Chapter Six, Aaron B. Frank continues the previous discussion on the shortcomings of the JPC and examines a set of concepts that might better serve the needs of long-term competition in UGS: the notion of infinite games; the ASDA cycle; problem-centric government; adaptive governance; and differentiating among hierarchies, markets, and networks as alternative modes of governance.
The second part of this report examines how investments in scientific research, most notably social science and social scientists, can support the development of the knowledge and capabilities to improve engagements in UGS. These four chapters (Seven through Ten) provide an interrelated set of perspectives on scientific research and the ability to develop technologies that would enable a better basis for understanding and engaging in UGS. Those perspectives involve considering the design of scientific research programs, looking at the development of the infrastructure to support more-effective social science research, improving the conduct of social science research in service of DoD, and ultimately exploring how research into artificial intelligence (AI) would need to proceed to create a basis for supporting decisionmaking under uncertain, adaptive, and open-ended conditions.
In Chapter Seven, Joseph N. Mait discusses the challenges posed by structuring research programs on complex subjects that increasingly meld elements from the physical, human, and cyber realms. Drawing on his experience as a researcher, program manager, and chief scientist at the Army Research Laboratory, he describes the basic organization of R&D programs in the physical and computational sciences and considers how increasing links between these sciences and the psychological and social sciences could affect program design, management, and evaluation. He argues that although human-centric disciplines increase the complexity of research, the goal of research remains the same: to increase understanding through scientific study and to use that understanding to engineer systems and ultimately solve problems.
In Chapter Eight, Andrew M. Parker discusses the transformative potential of new research infrastructure for the social sciences. He notes that interest and resources in the social sciences do not materialize until a crisis has occurred, thus resulting in an explosive but uncoordinated demand for research. He provides several recommendations for how investments in research infrastructure could improve the ability for social scientists to support policymaking during crises by providing economic efficiencies, coordination of funding, and enhanced research collaboration through a variety of mechanisms that have proven successful in other domains. He notes, however, that the achievement of these outcomes depends on overcoming important challenges, such as institutional biases among research sponsors for investing in the physical sciences and the complexities of managing infrastructure to remain adaptive and sustainable over the long term.
In Chapter Nine, Elisa Jayne Bienenstock discusses fundamental principles of social science research and argues that DoD has consistently attempted to apply social science to specific and pressing issues prematurely. Echoing Parker, she argues that great investments in understanding the mundane general features of complex social systems are needed before the leap can be made to examining situationally specific research and applications. She recommends integrating scientific practices that emphasize the discovery and documentation of nomothetic features into DoD operations to improve the basis from which idiosyncratic properties can be identified and examined.
In Chapter Ten, Edward Geist examines the history of AI in strategic decisionmaking. By drawing on the history of AI's origins supporting the maintenance of the nuclear stalemate between the United States and Russia, he shows that promising research never matured to application because of the problems posed by uncertainty and continuously evolving capabilities. He further notes that while contemporary AI systems are built on a different foundation of problem representation and input data, the real-world consequences of uncertainty and its impact remain. He argues that, although task-specific AI has advanced, UGS are unlikely to present the strategic conditions for which these systems will flourish and that continued research into the handling of uncertainty and ambiguity remains a priority.
Part Three: Supporting Long-Term Planning in the Face of Uncertainty and Change
The third part, Chapters Eleven through Thirteen, presents several perspectives on planning and decisionmaking performed under uncertainty. These chapters examine the challenges posed by decisionmaking in complex, uncertain environments. The authors address the uncertainties posed by long-term competition and identify complementary pathways for achieving robust, adaptive strategies by emphasizing different features of strategic engagement-the flexibility of the planning system to cope with complexity, the flexibility of models of complex systems, and the flexibility of decisionmaking processes to be open and involve multiple stakeholders that may serve as both the subjects and the consumers of analysis.
In Chapter Eleven, Steven W. Popper puts forward the proposition that for the NSE to operate effectively, the means for deliberating policies must also change. Processes need to be better suited to conditions of deep uncertainty where arguing over which assumptions about the future are correct will prove increasingly fruitless. Instead, the exploration of alternative assumptions (as well as narratives, causal explanations, and competing interests) should occur as part of the search for robust portfolios of actions-those tuned to yield satisfactory outcomes across a variety of potential futures. He argues that uncertainties should be characterized not by unknowable probabilities but in terms of reference framed around understanding the apparent choices; he also describes how adaptiveness should be made an integral feature of planning as opposed to being an ad hoc and ex post activity (as it often becomes). Importantly, he notes that such changes could further reposition the Intelligence Community to look more toward using its existing capabilities to provide decision support to the policy community while deemphasizing prediction and forecasting. He concludes by describing an alternative, computationally enabled, analytic, and deliberative policymaking process that is better suited than current approaches to produce robust and adaptive policy decisions.
In Chapter Twelve, Paul K. Davis argues that a new analytical architecture is needed to aid strategic planning for competing with great powers in UGS, specifically in the area of competition in which the rules that govern how powers deal directly with one another or their allies are increasingly contested, as in the case of the gray zone between the United States and its great-power rivals, Russia and China. Such planning must deal with developments in a complex adaptive system, so the analytical architecture needs to be conceived accordingly-a radical departure from the past. Analytic tools should help in 
(1)
(2)
In Chapter Thirteen, Robert J. Lempert, Kelly Klima, and Sara Turner provide an overview of how multiple stakeholders can be involved in the research process and consider how different types of involvement relate to one another. Given the openness of UGS, understanding how to engage with them and how to develop and maintain decisionmaking processes that scale as both the number of subjects and the number of participants increase will be a feature of any effective engagement strategy. With this in mind, the authors focus on approaches that center stakeholders as either the focus of research or the coproducers of research and how each approach can add value for policymakers. The authors then examine each of these modalities in turn, identifying the general principles of practice, the different approaches that can be taken, and the tools that can be used in conducting research in these modalities, as well as the challenges of doing so. Finally, the authors examine future investments that could catalyze improvements in multi-stakeholder research, and they make the case for how these investments could drive improved multi-stakeholder governance.
The final part of this report explores emerging concepts and technologies that offer transformative opportunities for understanding and engaging in UGS. These chapters offer a variety of perspectives drawn from the R&D experiences of their authors. In each case, the authors focus on decisionmaking within complex systems, using such various methods as digital twins, Agent-Based Modeling (ABM), distributed computing, and gaming as techniques that can enable new ways of understanding and acting within UGS. In all cases, technical and methodological approaches are presented that place decisionmaking at the center of research and analysis. In some cases, the objectives are to create better representations of decisionmaking agents within systems, such as soldiers in the military or citizens within social networks and economic markets. In other cases, the objectives are to model decisionmaking processes to better understand and improve the processes themselves with the goal of increasing adaptability and competitiveness.
In Chapter Fourteen, Zev Winkelman adopts the unusual perspective that DoD is an undergoverned space in its own right, despite its size and scope. The basis of his argument is that the major decisions made along the path from generating military forces by the individual services to handing those forces over to the joint warfighting commands is simultaneously laden with the formal steps of the Planning, Programming, Budgeting, and Execution process and underspecified interfaces connecting them. As a result, transaction costs associated with supporting and participating in data collection, analysis, and conflict resolution among the multitude of stakeholders are high. He imagines how digital twin technologies used to monitor and simulate organizational processes might improve governance by adding transparency and speed to these large-scale, bureaucratic processes. Reducing the time and expense of these decisions might be one pathway for all of DoD to become more agile, more adaptive, and ultimately more competitive.
In Chapter Fifteen, Ben Connable discusses the trajectory of the RAND Corporation's Will-to-Fight research program. He notes that DoD's formal analytic tools that support threat assessment; force sizing; and planning at strategic, operational, and tactical levels underrepresent human motivations for fighting. In doing so, the department risks making serious errors. He argues for developing an integrated, computational model of individual behavior that embeds individuals into groups and larger environmental context-a biopsychosocial model. Building on the work of systems theorists, he argues for a modeling strategy that is both expansive and modest-expansive in seeing the value of incorporating a variety of factors that could not be credibly omitted and modest in that, although building a computational model that is true is currently beyond the reach of science, the capabilities exist to build one that is useful and can be continuously improved.
In Chapter Sixteen, Robert L. Axtell describes the motivations for parallel execution of agent-based models, and he reviews rationales for large-scale ABM. These areas are two of the most important avenues for future progress of the field of ABM, which is arguably the most fertile new methodology in the social sciences in a generation-a kind of computationally enabled and data-driven social science. The possibility of automated synthesis of ABM from "big data" is discussed. Bottlenecks slowing progress are identified and possible barriers to accelerated progress are highlighted. Certain workarounds are suggested.
In Chapter Seventeen, Justin Grana examines the prospects of measuring the complexity of strategic interaction by using computational complexity. He determines that, although significant and important research has advanced algorithmic game theory and the characterization of computational-complexity classes, these approaches have not provided generalizable insights that can map game structures to the complexity of solutions and solution concepts. Instead, he argues that research shows that while game structures matter (e.g., whether games are zero sum or general sum, whether games are played as a single shot or repeated), idiosyncratic properties, such as the size of the game space, dominate the search for solutions. As a result, games that should be computationally tractable given their properties might actually require large commitments of computational resources to solve, while seemingly complicated games might be solved quickly and with relative ease. In Chapter Eighteen, James R. Watson, Michael J. Gaines, and Aaron B. Frank explore the value of applying concepts from biological evolution and ecology to long-term competition. They specifically examine the application of the immune system as a model for defeating disinformation attacks on populations, extending the model's application to the prevention of disease into more-speculative considerations of healing the body politic from infection. They also explore the implications of long-term competition between increasingly capable global powers through the lens of the Darwinian Demon, a theoretical organism imagined to be unconstrained by trade-offs in the adaptive trait space. Such examples serve to illustrate the richness and relevance of concepts that biological evolution and ecology can provide to national security, particularly with regard to adaptation and long-term competition.
In Chapter Nineteen, Elizabeth M. Bartels, Aaron B. Frank, Yuna Huh Wong, Jasmin Léveillé, and Timothy Marler examine the value of games as a tool for researching and exploring the complex, interactive dynamics of UGS. The authors argue that the games are a highly effective tool to help decisionmakers make sense of UGS because the games allow exploration of key elements in new problems and the relationships among those elements. The authors then explore the potential value of gaming in policymaking for UGS, describe two common failure modes for gaming of systems with high levels of complexity or indeterminacy, and offer several approaches for improving games to explore these spaces. The chapter concludes with a vision for a new game concept-a contest arena-that combines advances in several areas that hold the potential to improve the ability of games to inform adaptive planning in UGS.
This report is large, and readers are encouraged to follow their interests, sampling and skimming from the whole while more closely reading those chapters of greater interest and relevance to their needs. Each piece stands alone, and, although chapters are connected by common themes and interests, no efforts were made to coordinate responses to questions on UGS or encourage agreement or consensus. Thus, perspectives vary, which we believe is healthy and encouraging at such an early phase in research-indicating that there are both questions to be answered and opportunities to address them.
The chapters that follow, then, should be viewed for what they are-the first steps in a journey that we believe will benefit the nation's security by providing new perspectives on UGS, long-term competition, and the capabilities to engage, endure, and ultimately thrive in an increasingly complex and interdependent international system.
CHAPTER TWO Undergoverned Spaces: Problems and Prospects for a Working Definition Aaron B. 
Frank, RAND Corporation
(1)
Serving as a point of departure for the rest of the report, this chapter provides a highlevel characterization of governance and undergovernance, while admitting that a precise definition of UGS is unlikely to be forthcoming. Before discussing the three topics already mentioned, I provide a definition of governance. Afterward, I discuss different ways in which UGS might be found, considering governance between states, governance within a state, and pathways for governance to arise in the absence of states. Finally, I explore the difficulties of trying to define UGS, notably those arising from within the U.S. Department of Defense's (DoD's) culture. I also consider philosophical and operational problems emerging from the study of unnatural kinds in science. Despite these immutable definitional challenges, labeling spaces as undergoverned according to their possession and accumulation of features offers a pragmatic and functional perspective that is both possible and desirable.
Before examining the difficulties associated with defining and understanding UGS, it is useful to define governance itself. Doing so is not simple. As Adam R. Grissom notes in Chapter Three and Jonathan S. Blake examines in Chapter Four, 1 an empirical study of gov-ernance models around the world reveals much higher levels of diversity than students of international relations, political theorists, or military leaders generally acknowledge. 
2
• formal rules, for example, those developed from the top down by such authorities as national governments or corporate executives • norms of behavior adopted by actors that have arisen endogenously from the bottom up • rules that regulate appropriate behavior, e.g., identities and the rules of culture • interpreted rules that result from evolution and interpretation (e.g., the rules of the U.S.
Constitution are not what the original signatories considered them to be, but what nine contemporary Supreme Court justices determine them to be). 
3
As the remainder of this chapter will show, a simple examination of the presence, absence, and adherence to the rules that regulate interaction and exchange between actors provides a powerful lens through which many different domains can be examined. These can be relations between or within states and interactions between nonstate actors, which can be formal organizations; individuals; or, increasingly, nonhuman agents, such as autonomous systems that are empowered to adapt to novel conditions and opportunities. The emphasis on UGS, as opposed to well-governed spaces, focuses attention on the empirical limitations that result from a lack of rules, the shortcomings of those rules that exist, and the prospects for change; it views governance as dynamic and malleable, even though it is not outright controllable.
Challenges, and Prospects for New Approaches, Santa Monica, Calif.: RAND Corporation, RR-A1275-1, 2022.
A survey of the contemporary international system shows a vast array of challenges and challengers to U.S. national security and the rules-based order that has sustained relative peace and prosperity since the conclusion of World War II. Importantly, the success of the rulesbased order, predicated on the tenets of international liberalism, is not absolute-the "peaceful" decades of the Cold War and after were quite violent and punctuated by crises that posed existential risks to all humanity from nuclear weapons. 
4
11,
2001
5
Broadly, UGS and degrees of governance can be seen at four levels of analysis. The first and primary one is the level of relations between states and other international actors. At this level, the primary concerns over maintaining the balance of power and defense of the rules-based international order are most visible. The second deals with the levels of governance within states-from state failure and divided governance to kleptocracies, in which regimes use the power of the state to pursue private interests rather than the public good. These concerns link the internal governance of states with their ability to credibly participate in the international system, fulfilling their obligations to abide by the institutions of international governance. A third level considers broad questions of organizational, bureaucratic, and corporate governance that enable effective participation in long-term competition. This perspective might be counterintuitive, given that many of the organizations involved are among the most complex and managed in the world (e.g., DoD), yet long-term competition challenges organizations to be both efficient and effective in their employment of resources and adaptive to changing strategic conditions. Finally, a fourth level of analysis seeks to understand pathways by which undergoverned interactions between actors might become governed without the state or its equivalent serving as the arbiter of conflicts. I discuss each of these levels of analysis.
The security threats that the United States faces are varied. The most immediate threat is the great-power competition playing out in areas all over the globe, space, and cyberspace through subversion, proxies, and measures intended to coerce, influence, or simply disrupt U.S. allies and partners. 
6
7
8
12
13
14
15
16
17
In addition to the challenges posed by international governance, UGS might also include challenges posed by the need to engage and operate in spaces within states. The most obvious need is rooting out terrorists from what have traditionally been referred to as ungoverned spaces-a term that referred to areas within weak, failed, or collapsed states in which terrorist groups found safe haven and the freedom to plan, train, and enjoy sanctuary from reprisals. 18 These spaces might also become arenas where violent, aggrieved parties, who might be unable to challenge authorities in other domains, find a motive, an opportunity, and even an obligation to fight. In these cases, national governments are unable to maintain (1) a monopoly over the legitimate use of violence in their territories and ( 
2
72, No. 1, 2018
Policy, Vol. 40, No. 3, July 3, 2019
Vol. 11, No. 5, 2020;
Michael J. Mazarr, "Virtual Territorial Integrity: The Next International Norm," Survival, Vol. 62, No. 4, July 3, 2020
Review, Vol. 23, No. 3, September 2021
September 19, 2005;
Thomas E. Ricks, Fiasco: The American Military Adventure in Iraq, 2003
to 2005
, New York: Penguin Books, 2006;
Task Force on Combatting Terrorist and Foreign Fighter Travel, Final Report of the Task Force on Combatting Terrorist and Foreign Fighter Travel, Washington, D.C., October 2015;
Efraim Benmelech and Esteban F. Klor, What
University, April 2016;
Evans, Milton, and Young, 2021.
Challenges might also be posed by malgovernance, or kleptocracies, where national governments might have consolidated authority and control over the state, yet government officials might be unconstrained in their use of the power and privileges of their offices. In these spaces, a unified national government might have the trappings of a functional state yet govern with the goal of self-enrichment rather than public interest. In recent years, corruption scandals have forced the resignation or removal of heads of state in South Korea, Brazil, and more, and efforts to purge corrupt officials have resulted in the removal of entire staffs of governmental branches or ministries; this is what occurred when Mikheil Saakashvili came to power in Georgia in 2004 and removed all members of the ministry of education, along with 15,000 police officers, and when President Paul Kagame of Rwanda fired all 503 members of the Rwandan Judiciary. 24  Recent events have highlighted how national governance and corrupt regimes affect the international system. The 2016 Panama Papers, a leak of more than 11 million documents, revealed a vast network of companies, foundations, trusts, banks, and governments that were involved in tax avoidance and fraud. 25 As the International Consortium of Investigative Journalists noted, the results of the leaked papers were significant:
Pakistan's prime minister was sent to prison for corruption, New Zealand changed its laws, the United Kingdom recovered hundreds of millions of dollars in taxes and fines, 21 Augustus Richard Norton, Hezbollah, Princeton, N.J.: Princeton University Press, 2007. 22 Anatol Lieven, Pakistan: A Hard Country, New York: Public Affairs, 2012. 23 John P. Sullivan, From Drug Wars to Criminal Insurgency: Mexican Cartels, Criminal Enclaves and Criminal Insurgency in Mexico and Central America: Implications for Global Security, Paris, France: Fondation Maison des sciences de l'homme, April 2012; John P. Sullivan, "Criminal Insurgency: Narcocultura, Social Banditry, and Information Operations," Small Wars Journal, December 3, 2012; John P. Sullivan and Robert J. Bunker, eds., The Rise of the Narcostate, Xlibris US, 2018. 24 Sarah Chayes, "Kleptocracy in America," Foreign Affairs, Vol. 
96, No. 5, October 2017, p.
April 4, 2016
April 3, 2021;
Frederik Obermaier, Bastian Obermayer, Vanessa Wormer, and
Algeria opened a money-laundering probe into a corporate titan, and Colombia doubled its tax revenue collection. 
26
27
28
29
From the perspective of organizational governance, long-term competition presents a special challenge. Bureaucracies are designed to provide stable, predictable, and reliable organizational performance in stationary environments. 33 The need to be flexible, adaptive, and innovative enough to capitalize on opportunities and mitigate risks requires organizations to possess the ability to identify the need to change and alter their own decisionmaking and processes as situations require. The result is that organizations need to monitor and manage the continuous demand for exploration and exploitation-where exploration searches for new ways to frame problems and solve problems, and exploitation improves previously established processes and designs. 34  Perspectives on organizational governance and national competitiveness are at the heart of questions about how states marshal resources and efficiently convert them into power. During the Cold War, U.S. strategists, led by the Pentagon's Office of Net Assessment, came to view the intimate relationship between bureaucratic organization and national competitiveness as the key to winning a long-term competition with the Soviet Union: 35   The United States and the Soviet Union are engaged in a long-term competition, a competition with a fairly fixed stream of resources supporting their military establishments. If one looks at the rivalry in this way, it is clear that the efficiency with which each side converts its resources into useful military strength is of great importance. Whether it is the United States or the Soviet Union that makes best use of the technologies that develop in the next several decades will, in a major war, determine which is militarily ahead at the end of this century. 36   The surprising result of this line of reasoning is that within the state, questions about undergovernedness might extend beyond the consolidation of power and shared governance and involve more-mundane matters of organizational behavior and the need for highperformance governance. 37   
The preceding discussions of undergovernance focused on relations between actors that directly involve the state and its instruments of decisionmaking and action. Yet there are many domains where cooperative and competitive interaction occur in which the state is either absent or remains sidelined because of limited capacity or will to govern. In these cases, it is best to consider how governance might arise in the absence of the state.
First, it is important to consider the extent to which undergovernedness results from a lack of state capacity or will. For example, in the later years of the Cold War, international terrorism was largely viewed as violence that resulted from active state support or implicit state permission. 38 Contemporary conflict in cyberspace is viewed through a similar lens, in which many of the most damaging and worrisome acts-cybercrime and cyberespionage-are viewed as committed by state actors or agents operating at state actors' explicit or implicit behest. This is shown most acutely by the activities performed by Russian criminal organizations that have ties to the regime of Russian President Vladimir Putin; one such notable activity was the recent Solar Winds penetration of U.S. government and commercial networks. 39 In addition, high-profile ransomware attacks have targeted such critical infrastructure as energy and food distribution networks. 40   36 Peter deLeon and James Digby, Workshop on Asymmetries in Exploiting Technology as Related to the U.S.-Soviet Competition: Unclassified Supporting Papers, Santa Monica, Calif.: RAND Corporation, R-2061/1-NA, January 1, 1976. 37 Robert Klitgaard and Paul C. Light, eds., High-Performance Government: Structure, Leadership, Incentives, Santa Monica, Calif.: RAND Corporation, MG-265-PRGS, 2005. 38 Uri Ra'anan, Robert L. Pfaltzgraff, Jr., Richard H. Shultz, Ernst Halprin, and Igor Lukes, eds., Hydra of Carnage: The International Linkages of 
Terrorism and Other Low-Intensity Operations-The Witnesses Speak, Lexington, Mass.: Lexington Books, 1985
April 15, 2021
May 24, 2021
May 10, 2021
June 10, 2021;
Tom
(1)
(2)
Alternatives nevertheless exist. As previously noted, actors other than states might arise in the absence of the state and take on state-like functions, such as contract enforcement and the protection of property rights, by arrogating the power to coerce others. Such an occurrence is common around the world and naturally fits within the models of alternative governance discussed by Jonathan S. Blake in Chapter Four of this report. 49 The fact that nonstate " 
Cyberwar and Its Strategic Context," Georgetown Journal of International Affairs, Vol. 18, No. 3, 2017;
Nazli Choucri and David D. Clark, International
Vol. 99, No. 3, 2020
Norton & Company, 1997;
Kenneth N. Waltz, Theory of International Politics, Long Grove, Ill.: Waveland Press, [1979]
Charles Jones, and Richard Little, The Logic of Anarchy: Neorealism to Structural Realism, New York: Columbia University Press, 1993.
The challenge of governance is ultimately one of regularizing exchange. Outside systems of hierarchical command and authoritarian control, markets have been seen as the primary means by which voluntaristic exchange occurs; this has led social scientists from all disciplinary backgrounds to search for mechanisms by which participants might be assured that deals reached with other parties will be honored and enforced. Thus, the coercive powers of the state or its alternatives have served as the backdrop against which mutual exchange could be reliably conducted, making the state, war, and markets inextricably linked. 50 Yet new mechanisms have arisen that might eliminate the need for the state or an alternative authority and for the implicit scaffolding of coercion altogether.
Game theorists have noted that the equilibrium solutions in several games characterizing social interactions prescribe rational choices that leave actors worse off than they would be if they could credibly communicate and coordinate their actions. This difference, often referred to as the "price of anarchy," denotes the losses to the players and society at large from the inability to arrive at outcomes that would be preferable if trust were present. 51 One way to minimize the price of anarchy is for actors to engage in repeated interactions. As most famously examined in Robert Axelrod's study of the iterated prisoner's dilemma game, situations in which it is rational not to cooperate with other parties might become cooperative if the game were to be repeated indefinitely. 52 In these infinitely repeated games, if the losses of future cooperative interactions exceed the short-term gains of defecting from an agreement, rational players might cooperate without the temptation to defect. 53 In these cases, a set of social interactions might exist such that cooperation might be self-reinforcing, making a pathway to self-governance possible.
An alternative to this problem of mechanism design-i.e., the structuring of payoffs in ways that reinforce desired behaviors-is emerging through technical approaches to transparency and automation. For example, despite limited adoption, blockchain and the notion of a public ledger against which transactions can be audited have significant implications for governance without the presence of a central authority. 54 Blockchain technology is most commonly associated with cryptocurrencies, such as Bitcoin and Ethereum; 55 however, it has broader implications for supply chains, industrial processes, service delivery, etc. Anonymized public ledgers could displace more-traditional forms of regulation most commonly associated with governmental institutions. 56 The emergence of such concepts as smart contracts that automatically execute when specified conditions are met presents a new mechanism by which actors might replace coercive and costly forms of enforcement with transparency and automation, creating a path toward governance without dependence on the state or even on parties knowing one another's identities. Software executes transactions only if predetermined conditions are met, guaranteeing that each party's obligations will be metprovided conditions allow. 57  The prospects for such technologies to shift how governance occurs are profound. Entire regulatory structures designed to oversee supply chain sourcing, product safety, labor conditions, etc., might become unnecessary, calling into question whether the costly regulations and oversight provided by governmental authorities remain necessary. The rise of cryptocurrencies shows how alternatives to traditional state-centric monetary systems are possible, although the extent to which these currencies might eventually fall under government regulatory control remains unknown.
Less speculative, the prevalence of markets on the dark web that facilitate the sale of illicit goods shows the ability of exchange to flourish outside the guarantees of the state enforcing agreements between parties. The combination of anonymity and reputation-based mecha- 54 Don Tapscott, "Blockchain: The Ledger That Will Record Everything of Value to Humankind," World Economic Forum, July 5, 2017; Sloane Brakeville and Bhargav Perepa, "IBM Blockchain Basics: Introduction to Distributed Ledgers," IBM, March 18, 2018.   55 Bitcoin.org, homepage, undated; Ethereum.org, homepage, undated. 56 Brakeville and Perepa, 2018; Mario Dobrovnik, David M. Herold, Elmar Fürst, and Sebastian Kummer, "Blockchain for and in Logistics: What to Adopt and Where to Start," Logistics, 
Vol. 2, No. 3, September 2018;
Chris Speed, Deborah Maxwell, and Larissa Pschetz, "Blockchain City: Economic, Social and Cognitive Ledgers," in Rob Kitchin, Tracey P. Lauriault, and Gavin McArdle, eds., Data and the City, New York: Routledge, 2018;
Sangeet Paul Choudary, Marshall W. Van Alstyne, and Geoffrey G. Parker, "Platforms and Blockchain Will Transform Logistics," Harvard Business Review, June 19, 2019;
Edvard Tijan, Saša Aksentijević, Katarina Ivanić, and Mladen Jardas, "Blockchain Technology Implementation in Logistics," Sustainability, Vol. 11, No. 4
May 26, 2021.
58
59
Finally, the boundary between the state and nonstate governance might shift because of technology's ability to transform public goods, or commons, into private goods. According to the standard definition, public goods refers to items that are both non-rivalrous and nonexcludable (i.e., one person's consumption of a good does not prevent others from consuming it) and to items that people cannot prevent others from consuming, such as the light from a lighthouse. 63 Likewise, commons are resources for which consumption is rivalrous and exclusion is difficult; they are, therefore, prone to depletion. Examples of commons are water reservoirs or pastures for cattle. 64  Technologies could affect what is considered a public good, or the commons, and what is a private good. In doing so, they could recast what should be governed and by whom. It has been noted that digital products generally have a marginal cost of production of zero-i.e., once the first digital item has been developed, the cost of providing more is simply the cost of making a copy-thus redefining what is considered rivalrous consumption. A more intriguing change might arise from predictive and prescriptive models that can accurately model and forecast phenomena at micro scales. 65 The prospect that outcomes of policy actions can be estimated for specific individuals suggests that previously non-excludable goods might be rendered excludable. For example, highly accurate models of contagious disease, fire, or damage from foreign cyber or physical attacks might enable governing decisionmakers to provide or withhold protective services with the understanding that not defending specific individuals does not imperil the larger community. Such a development would radically transform governance and further accelerate trends of the privatization and localization of public services to exclusive communities. 66  In summary, UGS span a variety of domains and situations, many of which appear to have little in common. Yet the preceding discussion demonstrates that there are many pathways by which governance and its limitations affect the international system and U.S. national security. This is not to argue that all governance challenges pose a threat to national security or risk the unraveling of the international system. Rather, it shows that questions of governance retain relevance across a broad swath of international relations and strategic priorities. The text box summarizes these alternative types of UGS. 65 See Chapter Sixteen of this report (Robert L. Axtell, "Short-Term Opportunities, Medium-Run Bottlenecks, and Long-Time Barriers to Progress in the Evolution of an Agent-Based Social Science," in Aaron B. Frank and Elizabeth M. 
Bartels, eds.
(Blake, 2022)
Between States • Disregard for international law, institutions, or norms The Difficulty of Defining Undergoverned Spaces
Having now discussed the many ways that DoD and the National Security Enterprise (NSE) might encounter UGS, this section explores the prospects of developing a precise definition of UGS and tests for determining whether policymakers, military operators, international aid organizations, and other actors are engaged in them. Defining UGS is conceptually difficult for several reasons. First, for those involved in national security, the term undergoverned spaces might be viewed as just one more entry in a long list of terms designed to draw attention between conventional and unconventional conflict. Second, because the state itself is not easily defined (i.e., it is an unnatural kind), the logical point of departure for considerations of governance in its many alternative forms and capacities, including the conditions and consequences of its weakness and absence, rests on an unstable and ambiguous foundation.
Considering these challenges, a viable approach to defining UGS might rely on a functional approach that considers the consequences of governance and its shortcomings on a case-bycase basis to determine whether the approaches to strategy and operations discussed in later chapters merit consideration.
UGS have always played a role in U.S. defense strategy, with the military actively engaged in them. 67 Without belaboring the history of U.S. military operations and foreign policy, the U.S. military has engaged in far more interventions and conflicts since the Cold War's end in conditions that do not resemble conventional battle between peer or near-peer competitors-this despite the fact that planning to deter and perform large conventional military operations provides dominant narratives and mental models of how wars should be fought. 68  The variety of terms that have emerged over the past three decades highlights the gap between the conduct of war in its imagined, ideal form and the realities of circumstances in which the U.S. military has been called on to act on behalf of the nation's interests. Thus, the emergence of such terms as military operations other than war, humanitarian and disaster assistance, stability operations, and peace operations-alongside counterinsurgency, counterterrorism, and asymmetric warfare-has served to simultaneously highlight the gap between warfare as it is imagined and the broad variety of military engagements experienced in the real world. The terms also signal demand for specialized capabilities and resources outside the portfolio prescribed by the dominant model of warfare. 69 From this perspective, the con- 67 See Chapter Three 
(Grissom, 2022)
Combat and Culture, Boulder, Colo.: Basic Books, 2003
Since 2001, such concepts as those mentioned in the previous paragraph have evoked concerns about terrorism, insurgency, and the prospects of "safe havens," from which violent groups can plan, train, and operate. To address these threats, the term ungoverned areas entered the DoD lexicon more than a decade ago, because policymakers were concerned about how the failure and absence of local governance created places in which adversaries could act freely and threaten the security of the United States and its allies. Ungoverned areas were defined in the 2008 report Ungoverned Areas and Threats from Safe Havens as 
[a]
Contemporary concerns over UGS modernize prior eras of geopolitics, such as the great game, where states competed for geographic buffers, trade routes, control over markets and resources, political and ideological influence, and more. 75 UGS have become arenas for conflict-new fronts where great-power competition emerges and upstarts can challenge established powers. Too little engagement in UGS risks allowing threats to fester and grow, ceding access, influence, and the ability to shape the future direction of the international system to rivals. Too much engagement and the United States risks becoming immersed in costly conflicts for which victory cannot be achieved. 76   The State Is Not a Natural Kind of Object of Study Philosophers of science note that objects of study might be natural kinds or unnatural kinds. 77  Natural kinds refer to those things that exist independently of human minds and are discovered in nature, such as electrons, planets, and trees. These objects are generally insensitive to whatever labels humans choose to apply to them. By contrast, unnatural kinds are those things that are invented by humans, whether they are technological artifacts, such as the axel of a car, or concepts, such as the nation state and system of interdependent interactions that bind states into the international system. The status of UGS as unnatural kinds poses challenges to three basic yet critical aspects of science: definitions, measurement, and inference.
States-whether absolute, consolidated, weak, fragmented, failed, or otherwise describedare unnatural units. Definitions vary, and determination of when a polity is a state or something else depends on context and the features one chooses to emphasize. 78 For some, state- 74 G. John Ikenberry, A World Safe for Democracy: Liberal Internationalism and the Crises of Global Order, New Haven, Conn.: Yale University Press, 2020; McMaster, 2020. 75 Peter Hopkirk, The Great Game: The Struggle for Empire in Central Asia, New York: Kodansha USA, 1992. 76 Political scientists refer to this as "bait-and-bleed" to describe conflicts in which great powers lose their strength and wealth fighting unproductive conflicts of little value to their national security. See John J. Mearsheimer, The Tragedy of Great Power Politics, updated ed., New York: W. W. 
Norton & Company, 2014
Keim Campbell, Michael O'Rourke, and Matthew H. Slater, eds.
Ecology and Systematics, Vol. 3, 1972;
Max Weber, Economy and Society, Vol. 1, eds. Gunther Roth and Claus Wittich, Berkeley, Calif.: University of California Press, 1978a;
Max Weber, Economy and Society, Vol. 2, eds. Gunther Roth and Claus Wittich, Berkeley, Calif.: University of California Press, 1978b;
Hendrick Spruyt, The Sovereign State and Its Competitors, Princeton, N.J.: Princeton University Press, 1994;
Joel S. Migdal, State in Society:
The challenges and implications of definitional choices on the state and its failure as a political unit were examined by Nancy Cartwright and Rosa Runhardt, who considered whether the violence that erupted in Syria during the 2010 Arab Spring uprisings qualified as a civil war. 79 They noted that how one chooses to define statehood, conflict, and casualties from acts of violence produced different conclusions and implied different judgments about the severity of conflict, the legitimacy of its participants, and the suitability of prospective responses based on the employment of such labels as civil conflict, civil war, terrorism, and insurgency. 80 They concluded their examination of definitional implications by noting the following:
Asking whether Syria is at civil war is not sensible unless we say to what end we would like to classify Syria as at civil war or not. If we want to know whether the conflict will have certain effects, so that we can act to prevent these, then we will most likely give a different answer than if we wanted to explain the development of the conflict since 2010. Neither of these two answers will be simply right or wrong; they will only be right for a certain purpose. 81   Cartwright's and Runhardt's examination of the Syrian civil war suggests that a broadly accepted definition of UGS, an intimately related phenomenon, will not be forthcoming, yet exploring the concept and creating purpose-built definitions might nevertheless be worthwhile.
Cartwright and Runhardt noted that unnatural kinds might be best measured as a categorical variable. The pathway toward effectively analyzing UGS, then, rests on determining the most-salient features across a broad variety of circumstances, thus admitting that not all features will be present in all cases. Determining whether an undergoverned space is present rests on the belief that a sufficient set of features is present, placing a given space within the Studying How States and Societies Transform and Constitute One Another, New York: Cambridge University Press, 2001; Philip Bobbitt, The Shield of Achilles: War, Peace, and the Course of History, New York: Anchor Books, 2011; 
Christopher Pierson, The Modern State, 3rd ed., New York: Routledge, 2012
Cartwright and Runhardt, 2014.
In practical terms, determining whether the United States is engaging in an undergoverned space might rest on making careful comparisons with other cases; this involves examining similarities and differences in the composition and dynamics of UGS and their connections to broader elements of the international system rather than tallies of whether specific features are present or not.
Finally, matters of inference about UGS, which project from what is known onto cases and into times that are unknown, are further complicated by the status of UGS as unnatural kinds. Peter Godfrey-Smith has noted that depending on the status of the objects in question-whether they are natural or unnatural kinds (i.e., stable and immutable or mutable and dynamic in nature)-the characteristics of reliable inference might change:
[W]e can recognize two kinds of inference. The first is generalization from random samples. This form of inference has the following features: sample size matters, randomness matters, and "law-likeness" or "naturalness" does not matter. The second kind of inference is generalization based on causal structure and kinds. In these cases, sample size per se does not matter, randomness does not matter, but the status of the kinds matters enormously. These two strategies of inference involve distinct "bridges" between observed 82  Cartwright and Runhardt, 2014, p. 268.   83 In Chapter Four of this report, Jonathan S. Blake takes a different perspective by separating UGS into two conceptual components: a qualitative dimension referred to as "alternative governance," which then allows for undergovernedness to be rendered quantitatively within the context set by the qualitative properties and configuration of governance structures and patterns 
(Blake, 2022)
Thomas A.
Classification and Kinds," in Catherine Kendig, ed., Natural Kinds and Classification in Scientific Practice, New York: Routledge, 2015, p. 59.
(1)
(1)
(2)
Keim Campbell, Michael O'Rourke, and Matthew H. Slater, eds.
Waltz, [1979]
Bartels, eds.
Relations, Vol. 19, No. 3, September 1, 2013
Bartels, eds.
Although a precise definition of UGS might not be forthcoming, the placement of governance practices, capacities, and purposes offers hints about when and why engagement might be beneficial for the United States. As previously discussed, undergovernedness might occur in several ways, each presenting different challenges and motivations for engagement and intervention. Broadly, however, four considerations might be active, each of which contributes to the demand for action and the tailoring of engagements to conditions that differ from traditional state-to-state interactions.
The first consideration is the extent to which the space is accessible to competitors-both states and nonstate actors. Accessible spaces are where open conflict-covert and subversive influence or the threat of either-could provide for the escalation and expansion of conflict. In cases where escalation between a fixed number of competitors might occur, yet no additional actors can enter the conflict, it might be unwarranted to categorize the contested space as an undergoverned one. Alternatively, even with low stakes, the prospects of many new actors entering an arena to compete-e.g., the arrival of foreign fighters, private military organizations, criminal networks, regional peacekeeping forces, nongovernmental organizations, and more-might add increasing levels of complexity, all of which contribute to, and change, the complexity of a civil war. With each actor, complexity is added as new grievances and opportunities to pursue their redress indicate the possibilities that a vacuum or conflict might spread. In such cases, the source of the conflict itself might not directly involve U.S. national interests, yet its potential expansion might nevertheless threaten them. Consider challenges posed by great-power competition and the variation in form and place in which it occurs. In some cases, rivalries might appear as direct confrontations between competitors in competitive spaces that have high barriers of entry, require exquisite scientific and engineering expertise, and address specific strategic needs that most actors do not face (e.g., global military power projection via long-range precision-strike systems and the logistics systems that can support sustained combat operations across long distances). Alternatively, competition might exist within spaces that have a low barrier of entry that enables the full complement of actors-agents of the world's most-sophisticated states, criminal organizations, terrorist networks, political activists, and citizens, each acting with different motivations-to compete (e.g., in cyberspace).
The second consideration is the extent to which the domain is governed by institutions, whether formal or informal, that moderate conflict. In some cases, governance institutions might exist and remain effective in shaping the behavior of competitors. In other cases, however, institutions might exist yet be ignored and might be challenged by alternative institu-Engagement for Undergoverned Spaces: Concepts, Challenges, and Prospects for New Approaches, Santa Monica, Calif.: RAND Corporation, RR-A1275 
-1, 2022;
and Bienenstock, 2022)
tions or might simply be nonexistent. In each case, whether institutions effectively serve their purpose of proscribing and prohibiting certain behaviors to facilitate interactions should be examined. Important considerations might be whether institutions offer only partial coverage of the space or the actors within it, such as proposed applications of international law that might restrict the legal actions of state actors yet might not apply the same rules to commercial and other nonstate actors. 90 Likewise, compliance with international institutions can be difficult to discern. For example, recent scholarship on subversion and covert action has noted that such efforts are made because states are both unable to make a legal case for violating the sovereignty of their targets and unwilling to openly defy international law and overtly violate the sovereignty of their targets. 91   Institutional Interdependence A third and related consideration is the extent to which changes in governance practices in one space might affect governance in others. Governance institutions are often layered; organizations, practices, and behaviors established to govern one application transfer to others when opportunities and demand align. Institutions might be regarded as "solutions looking for problems." 92 These would form the building blocks or design patterns used to establish and extend governance. 93 In cases of thinking about diffusion and governance, two questions might be considered. First, if competition in a given space is ineffectively governed, does an opportunity exist to import institutions from other spaces to manage it? Alternatively, a second question reverses this logic by asking whether the occurrence of competition in one space might weaken or undermine governance institutions in that space and also jeopardize related institutions in other spaces.
Finally, a fourth set of considerations concerns the temporal nature of the competition and the extent to which it has a logical termination point or represents a condition that must be endured. In most cases of long-term competition in UGS, coping with aggression resembles a brawl in which each participant seeks to survive and develop their own often unique goals and interpretations of success and failure. 94 Long-term competition in UGS creates new chal- 90 New Zealand Ministry of Foreign Affairs and Trade, 2020. 91 Austin Carson, Stephanie Carvin, Jon R. Lindsay, and Ryan Scoville, In the Shadow of International Law: Secrecy and Regime Change in the Postwar World, International Security Studies Forum, December 8, 2020; Michael Poznansky, In the Shadow of International Law: Secrecy and Regime Change in the Postwar World, 1st ed., New York: Oxford University Press, 2020. 92 Michael D. Cohen, James G. March, and Johan P. Olsen, "A Garbage Can Model of Organizational Choice," Administrative Science Quarterly, Vol. 17, No. 1, 1972.   93 Erich Gamma, Richard Helm, Ralph Johnson, John Vlissides, and Grady Booch, Design Patterns: Elements of Reusable Object-Oriented Software, Reading, Mass.: Addison-Wesley Professional, 1994. 94 George Skaff Elias, Richard Garfield, and K. Robert Gutschera, Characteristics of Games, Cambridge, Mass.: 
MIT Press, 2012.
Together, these four considerations, shown in the text box, might not precisely define UGS. But they might indicate when such a label is warranted by considering the implications of competition. More specifically, the extent to which competition might expand to involve increasingly numerous and diverse actors, address threats to established governance institutions, create risks to-and opportunities for-governance institutions elsewhere, and require continuous attention to novelty, innovation, and adaptation would indicate a demand for the approaches offered and examined in this report.
Regardless of how UGS are defined, they will remain an important strategic challenge that DoD and the broader NSE will be called upon to engage in. The motives for doing so might 95 Kenneth O. Stanley, Joel Lehman, and Lisa Soros, "Open-Endedness: The Last Grand Challenge You've Never Heard Of," O'Reilly Media, 
December 19, 2017
25, No. 2, May 2019;
Kenneth O. Stanley, "Why Open-Endedness Matters," Artificial Life, Vol. 25, No. 3, August 2019
Undergoverned Spaces and the Challenges of Complex Infinite Competition
Since its inception in 1947, the U.S. Department of Defense (DoD) has struggled to develop a cost-effective approach to safeguarding the nation's interests in undergoverned spaces (UGS) around the globe. DoD efforts in UGS have alternated between long periods of neglect and occasional spasms of large-scale interventionism; the efforts have produced results that typically range from outright failure to ambiguous stalemate, but they rarely, if ever, have produced a clear, positive, and strategic return on investment. This ambiguous performance is puzzling. The U.S. armed forces are among the most professional and capable in history, and, although UGS have typically been a secondary priority for DoD, even a fraction of a more than $700 billion defense budget represents substantial resources and capacity. 1 Whatever the economies taken by DoD with steady-state funding, when the United States has chosen to intervene on a large scale-such as in Vietnam, Afghanistan, and Iraq-it has spared no expense. 2 Yet all the resources, professional forces, and sophisticated capabilities have typically delivered ambiguous results at best in U.S. military operations in UGS.
This chapter explores this puzzle and the possibility that the problem might be fundamentally analytical in nature-that despite the support of a well-funded intelligence apparatus and the world's most developed defense analytical community, DoD's undistinguished record might be rooted in an inability to perceive and understand the strategic dynamics of UGS well enough to support effective operations.
This chapter has two major sections. The first explores the role of UGS in U.S. defense strategy and the broad patterns of DoD's performance in UGS. The second examines UGS as an analytical problem, using the example of Africa and seeking to identify those characteristics that pose the greatest challenges for DoD's ability to perceive and understand the stra- tegic dynamics at play in UGS. We close with some thoughts on the need for a new approach to guide DoD's analysis of UGS.
UGS is a contemporary defense planning term denoting geographical regions or domains of interaction (e.g., cyberspace) that are not under the full control of government institutions that have legitimacy and a monopoly on armed forces. 3 From a defense planning perspective, UGS are therefore regions and domains where armed actors other than government forces, perhaps including nonstatutory armed forces aligned with government actors, wield militarily significant capabilities and enjoy politically significant freedom of action. 4  To an extent often underappreciated by outside observers, UGS have been an important defense planning priority for the United States for most of its history. From the period of independence through the 19th century, the United States was fixated first on UGS of the western frontier and later on UGS abroad, where the United States competed with European empires and local powerbrokers. 5 In the American West, warfare against Native American nations, some of them quite militarily potent, was an ever present reality until the 1890s. 6 The nation's first overseas war was, famously, against pirate lairs on the "shores of Tripoli" along the ragged undergoverned edges of the Ottoman Empire. 7   3 For a discussion on the definitions of UGS, see Chapters Two and Four of this report (Aaron B. Frank, "Undergoverned Spaces: Problems and Prospects for a Working Definition," in Aaron B. Frank and Elizabeth M. Bartels, eds., Adaptive Engagement for Undergoverned Spaces: Concepts, Challenges, and Prospects for New Approaches, Santa Monica, Calif.: RAND Corporation, RR-A1275-1, 2022; and Jonathan S. Blake, "Perspectives on State Governance, Undergovernance, and Alternative Governance," in Aaron B. Frank and Elizabeth M. Bartels, eds., Adaptive Engagement for Undergoverned Spaces: Concepts, Challenges, and Prospects for New Approaches, Santa Monica, Calif.: RAND Corporation, RR-A1275-1, 2022). 4 For use of the term in DoD, see Jim Garamone, "Middle East, Africa Commanders Discuss Terror Threats," DoD News, 
March 10, 2020
York: Hawthorn Books, 1968, pp. 1-26;
Army Counterinsurgency and Contingency Operations Doctrine, 1860
-1941
, Washington, D.C.: Army Center for Military History, 2001, pp. 1-86.
The 20th century marked the transition to "professional" U.S. defense planning focused on the problem of conventional warfare against other major powers, which came with mobilization, logistics, and materiel challenges. 
8
9
10
UGS did not arise again as a significant defense planning problem until the post-World War II era, when the collapse of European empires created opportunities for the Soviet Union and other communist powers to expand their influence into restive colonies and weakly governed, newly independent states. 
11
Dee, 2006;
Bard E. O'Neill, William R. Heaton, and Donald J. Alberts, eds., Insurgency in the Modern World, Boulder, Colo.: Westview, 1980
Benjamin Schwarz, American Counterinsurgency Doctrine and El Salvador:
: The Soviet War in Afghanistan, New York: Harper, 2009
Kentucky, 2004.
26
September 11, 2001
27
The first intervention after the terrorist attacks of 
September 11, 2001
James Dobbins, Keith Crane, Seth G. Jones, Andrew Rathmell, Brett Steele, Richard Teltschik, John G. McGinn, Rollie Lal, Rachel Swanger, and Anga Timilsina, The RAND History of Nation-Building, Santa Monica, Calif.
James Dobbins, John G. McGinn, Keith Crane, Seth G. Jones, Rollie Lal, Andrew Rathmell, Rachel Swanger, and Anga Timilsina, America's
The United States withdrew from Iraq in 2010-too early-and the Islamic State in Iraq and the Levant (ISIL) arose to seize western Iraq and eastern Syria. U.S. forces returned and led a multiyear campaign to destroy ISIL. The withdrawal of U.S. forces from Afghanistan led to the immediate collapse of the Western-backed regime in Kabul. 29  The costs have been substantial. In Operation Enduring Freedom, the expenditure totals approximately $1 trillion, and 1,845 U.S. servicemembers have been killed in action to date. The United States remains unable to say that it accomplished its objective of creating a stable and democratic Afghanistan that will no longer serve as a sanctuary for terrorist networks. In Operation Iraqi Freedom, with another $1 trillion spent and 3,481 U.S. personnel killed in action, the United States still cannot claim to have achieved its objectives of a stable, free, and democratic Iraq. 30  Since 2001, the United States has also conducted counterterrorism (CT) campaigns in a wide swath of UGS beyond Afghanistan and Iraq, albeit with a smaller footprint and a relatively greater reliance on special operations forces, airpower, and partner forces. 31 These campaigns have been conducted across Northwest Africa, North Africa, Central Africa, the Horn of Africa, the Levant, Yemen, Pakistan, and Southeast Asia. 32 Thousands of jihadists have been killed or captured, and numerous networks have been disrupted and defeated. 33 Yet U.S. CT efforts are widely regarded as containment measures that have done little to resolve the 29 The definitive U.S. histories of the wars in Iraq and Afghanistan are yet to be written. For the closest existing approximations, see 
Theo Farrell, Unwinnable: Britain's War in Afghanistan, 2001
-2014
, London, United Kingdom: Penguin, 2017;
Ronald Neumann, The Other War: Winning and
Grissom and Karl P. Mueller, Airpower in Counter-Terrorist Operations: Balancing Objectives and Risks, Santa Monica, Calif.: RAND Corporation, EP-67403, 2017
Stephen Watts, Jason H. Campbell, Patrick B. Johnston, Sameer Lalwani, and Sarah H. Bana, Countering Others' Insurgencies: Understanding U.S. Small-Footprint Interventions in Local Context, Santa Monica, Calif.
A Case History Analysis, Santa Monica, Calif.: RAND Corporation, RR-2026
-A, 2018, pp. 81-204.
Again, this is puzzling. Why has the American military experience in UGS been so equivocal? The United States has tried numerous variations in its strategic and operational approach to the challenge of UGS. It has attempted to ignore, contain, mitigate, and transform UGS at various times and places. It has conducted limited-liability special operations campaigns, small-footprint air-centric campaigns, and full-scale direct stability campaigns. It has emphasized the development of local government forces, empowered local nongovernment proxies, flooded UGS with its own forces, relied on allied forces from abroad, operated through international organizations and alliance structures, and virtually all the combinations thereof. And yet across these permutations, the results have remained consistently modest at best. This suggests that the causes of the ambiguous U.S. experience in UGS might be more fundamental than operational technique and design. It suggests that the problem might be analytical in character. The United States appears to lack perception and awareness in UGS, struggling to identify key actors, ascertain the sources of their behavior, and understand the dynamics and incentives that shape conflict in UGS. For that reason, regardless of the operational approach used-small footprint or large scale, unilateral or partner focused, CT or COIN-the results appear to regress to the ambiguous mean.
This apparent analytical weakness is of more than just historical interest because the nation's defense strategy continues to place strong emphasis on the strategic challenges presented by UGS. The 2018 National Defense Strategy defines three key strategic challenges for the U.S. armed forces: improving conventional deterrence vis-à-vis China and Russia, competing more effectively with revisionist great powers and regional rogue regimes in the gray zone below the threshold of armed conflict, and maintaining more cost-effective pres- 34 For useful overviews, see  35 For a trenchant statement, see Richard Ganske, "Counter Terrorism, Continuing Advantage, and a Broader Theory of Victory," Strategy Bridge, 
March 13, 2014
Analytical errors of the kind DoD encountered in UGS typically result from a mismatch between the available analytical lenses and the relevant conditions on the ground. In this context, the memorable formulation of James C. Scott that the U.S. government (USG) "sees like a state" is relevant. 36 As the most powerful state in the international system and the de facto guarantor of the system itself, the USG naturally views UGS through the lens of state capacity, legitimacy, and influence. 37 From a legal and policy perspective, the United States finds it most natural to work with other states, and it tends to measure governance in terms of the influence of the central state apparatus in an area. 38 When seeking to understand the strategic dynamics in an undergoverned space, the USG also, therefore, tends to focus on the activities and capabilities of the central state apparatus. 39 36 James C. Scott, Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed, New Haven, Conn.: Yale University Press, 1999. Scott's analysis certainly has its weaknesses, but the idea that states "see" society in particular ways has enriched theory and practice. For an overview of the theoretical importance of Scott's book, see, for example, Shannon  37 Robert J. Art, A Grand Strategy for America, New York: Century Foundation, 2003; Robert J. Art, America Abroad: Why the Sole Superpower Should Not Pull Back from the World, Oxford, United Kingdom: Oxford University Press, 2018. 38 For a programmatic statement, see 
James Dobbins, Seth G. Jones, Keith Crane, and Beth Cole Degrasse, The Beginner's Guide to Nation-Building, Santa Monica, Calif.: RAND Corporation, MG-557-SRF, 2007
James Dobbins, Laurel E. Miller, Stephanie Pezard, Christopher S. Chivvis, Julie E. Taylor, Keith Crane, Calin Trenkov-Wermuth, and
40
It is entirely possible that a system in which the government sees like a state, DoD sees like an institution, and individual analysts see like generalists works well for a great number of the strategic, policy, and analytical problems confronting the nation. It might be that the very scale of the American military instrument might require such an approach and that this might help explain the popularity of systems analysis and other highly rationalist modes of thought in DoD. 45 However, given the enduring struggle to develop an effective military approach to UGS, it is worth exploring whether those lenses are appropriate to the analytical problem of undergovernedness. In the following section, I discuss the strategic characteristics of UGS, looking at Africa as an example.
In seeking to understand whether there might be a mismatch between DoD's perceptual capacity and the characteristics of UGS, it is useful to begin with Africa. 46 This continent remains an active operational theater for the United States and, for many, exemplifies the idea of an "undergoverned area." 47  At the individual level, working on DoD Africa policy tends to be an uncomfortable experience in Washington, in Stuttgart (at U.S. Africa Command), or on the continent itself. Most U.S. military and DoD civilians who become involved with Africa have virtually zero background knowledge of the region. They learn about Africa on the job. This creates encounters with Africa that are both strange and nearly universal, becoming a kind of badge of honor for those working in the region. Virtually everyone has a story to tell of a meeting with African government officials that produced agreement on joint action, only for there to be no follow-up and no explanation from African interlocutors for the lack of follow-up-if such interlocutors could be located at all. What those working on Africa policy encounter is the following:
• high-profile policy initiatives that dissolve into thin air • key decisions awaiting approval by power brokers in agencies outside the respective ministries of defense (MoDs) • meetings in which senior African representatives appear oddly, to American eyes, deferential to lower ranking members of their delegation • stories of learning about close family ties between senior military leaders and key business, cultural, and political figures, or of meeting African government and military interlocutors who are themselves also business owners or politicians • stories of the moment when it dawned on the American that their African interlocutors are not powerful because they hold a senior position in the armed forces but instead hold a senior position in the armed forces because they are already powerful. 48 46 The following discussion draws heavily on previous analyses conducted for U.S. Africa Command and its components on U.S. military strategy on the continent. 47 See Theresa Whelan, "Remarks to the Portuguese National Defense Institute," May 24, 2006, reprinted  in Nacao Defensa, Vol. 114, No. 3, 2006.   48 This is based on my experience in and around the civil service for more than two decades. Of course, Africans have the equivalent stories about working with Americans, including anticipated long-term relationships, such as staff talk series, in which U.S. officers and appointees rotated so quickly that no two meetings were attended by the same leaders on the U.S. side. There are also stories of senior U.S. leaders, even cabinet secretaries, being unable to deliver on simple commitments; of countless visits by large teams of American personnel collecting unending information that never appeared to be shared or retained; of interminable delays while the unimaginably vast and complex DoD bureaucracy ground its way to a deci-These are common experiences that unite DoD's Africa community.
The individual experiences of DoD military and civil servants have institutional analogues:
• equipment and materiel delivered to African partners at great expense and effort that rapidly deteriorate or "sprout feet" and disappear • training events for which students fail to turn up • units that attend collective training events, only to be disbanded, their personnel scattered to other units • African MoDs that insist on acquiring showy systems that provide little or no useful capability • MoDs that allow carefully developed capabilities to disintegrate without notice or apparent concern • African armed forces that absorb enormous amounts of individual and collective training without any apparent improvement.
The overall experience is one of Sisyphean futility, interrupted by unexpected episodes of progress and growth. Within DoD circles, the common and lazy explanation for these experiences is that African armed forces lack capacity and professionalism. They are not enough like the United States. The prescription that follows this diagnosis is to admit defeat and withdraw or to significantly escalate the U.S. effort to develop and professionalize the African partner. The fact that DoD has been pursuing this general pattern for 75 years without much apparent return on investment raises the possibility that something deeper is happening.
To understand what that something might be, one must begin with a fresh understanding of the continent, its societies, and their armed forces-Africa's strategic context. 
49
50
As a result, the continent is largely a single plateau of extraordinarily old and unproductive topsoil that has little topographical relief and astounding quantities of rare minerals lying just beneath the surface. 52  Africa is the cradle of humankind, but, because of its poor topsoil and weather patterns, Africa in 2021 is not particularly conducive to human life. 53 Only roughly 10 percent of the continent is covered by alluvial or volcanic soils able to support intense agriculture. 54 Much of the continent is buried under the Saharan, Namibian, and Kalahari deserts or the rainforests of the Congo Basin. 55 Many regions of Africa that do possess decent soil for agriculture are, unfortunately, located in agricultural pest and disease zones. 56 Thus, Africa is said to have a low geographic "carrying capacity." 57 As a result of this low carrying capacity, for much of recorded history, the population density of the continent has been lower than other regions of the globe where environmental conditions are more favorable. 58 Today, even after five decades of rapid population growth, Africa is only approximately 33 percent as densely populated as Asia and half as populated as Europe. 59 Although there are pockets of dense population in Africa where conditions are more favorable, the continent is comparatively very sparsely inhabited. 60  These geographic and demographic characteristics have traditionally shaped African economic systems. Low carrying capacity of the land has typically led Africa's inhabitants toward subsistence dry field cultivation and nomadic pastoralism. 61 In dry field cultivation, farmers subsist by clearing land to raise dispersed fields of low-yield crops for a few seasons and then move on when soil quality and moisture have been exhausted. 62 Nomadic pastoralism revolves around herds of livestock that constantly move in search of good pasture land. 63  Today, of course, much has changed, and a substantial proportion of Africans live in urban concentrations. A century of gradual industrialization and the more recent advent of the service and information economies have altered the patterns of life for many in some regions. However, more than 60 percent of Africans continue to live as subsistence farmers, and the patterns of agricultural life still provide the de facto foundation for contemporary economic and social life in many regions of the continent. 64  The other fundamental feature of African economics is the oil and mineral wealth that is concentrated in certain regions of the continent's geology. Africa possesses approximately 30 percent of the globe's mineral reserves, and it is a leading world producer of aluminum, bauxite, cobalt, diamonds, platinum, and gold. 65 Africa also has a significant share of global oil production and reserves, estimated at 12.2 percent of production and 9.5 percent of reserves. 66 These extractive industries create enormous wealth for some but contribute less to overall economic development and poverty reduction in Africa than might be assumed. 67  The reasons for this are debated by economists and social scientists, but the point rent nature of African extractive industries and the heavy involvement of foreign companies are commonly cited factors. 68 Some economists also point to a "resource curse" by which very lucra- 62 Stephen Twomlow, "Dry Land Farming in Southern Africa," in Gary A. Peterson, Paul W. Unger, and Chris C. Du Preez, eds., Dryland Agriculture, Madison, Wisc.: American Society of Agronomy, 2006. 63 United Nations Food and Agricultural Organization, Pastoralism in Africa's Drylands: Reducing Risks, Addressing Vulnerability, and Enhancing Resilience, Rome, Italy: United Nations, 2018. 64 Paul Collier and Jan Willem Gunning, The Microeconomics of African Growth, Washington, D.C.: World Bank, 
May 18, 1999b, p. 5;
Paul Collier and Jan Willem Gunning, "Explaining African Economic Performance," Journal of Economic Literature, Vol. 37, March 1999a
FAO Agricultural Outlook 2016
-2025
, Special Focus: Sub-Saharan Africa, Paris, France: OECD Publishing, 2016, p. 60
tive extractive industries appear to fuel corruption and political instability, ironically making the most resource-endowed nations in Africa some of the least economically developed. 69  Because of these common structural, geographic, and economic factors, anthropologists and sociologists argue that African societies tend to share many key characteristics, notably a segmented social model that is highly decentralized, kinship based, geographically diffuse, network-centric, and "acephalous." 70 According to this perspective, whereas most Western societies are defined by formal hierarchies, such as socioeconomic classes, African societies are typically notable for their lack of formalized hierarchy. 71 They are more typically defined, instead, by informal social structures, such as generational age sets, lineage associations, and, most importantly, patronage networks. 72 In such societies, the family tends to be the fundamental social unit-rather than class, caste, profession, or ethnicity. 73 Extended kinship networks operate as primary sources of identity for individuals; business, political, and other types of networks are assembled from the building blocks of kinship networks. 74 Driven by intergenerational migration patterns, kinship groups sprawl over large regions, and social relationships in a given area of Africa will often be a highly decentralized latticework of loosely interacting informal networks. 75  In turn, Africa's subsistence economies and decentralized societies shape the continent's politics in important ways. 76 Political scientists argue that there has traditionally been insufficient economic capacity to support centralized political units in Africa, which tend to arise where there are accumulations of wealth and capital to support ruling elites and bureaucra-cies. 77 Moreover, according to this argument, centralized political systems tend to emerge where the financial gains from controlling territory justify the costs of control. 78 Most of Africa's land is not especially economically valuable on an average basis; this suggests that, generally, there has historically been neither the incentives nor the means to create large, powerful, and expensive centralized political systems in most African societies. 79  The longstanding tendency in Africa is, instead, toward distributed, diffuse, overlapping networks of hybrid political power, reflecting the social networks that underlie political systems. 80 A substantial body of anthropological, sociological, and political science research indicates that the natural political unit in African politics is therefore the patrimonial network. 81  Often built on kinship ties, patrimonial networks are informal webs of relationships and influence supported by patronage. 82 Powerful patrimonial networks in Africa are archetypally led by what are termed big men, placing powerbrokers at the heart of politics on the continent. 83  Political scientists have described the political systems created by interactions among these patrimonial networks as limited access political orders. 84 In limited access political orders, powerful patrimonial networks control entrée to activities that produce economic and social value within the society. 85 The most-important aspects of society are incorporated into these networks, from the commercial and financial to the administrative, religious, and political. 86 Controlling access to these activities allows the networks to extract resources from the society to support themselves and to compete with one another for influence. 87 The most-powerful patrimonial networks collaborate to prevent new powerbrokers from emerging, creating a kind of dynamic stability. 88 Where powerbrokers frequently resort to force to settle disputes between rival patrimonial networks, in many areas of the continent, the result is what William Reno has described as "warlord politics." 89  One important characteristic of limited-access political orders is that government institutions are not typically independent actors themselves but rather contested domains in which patrimonial networks compete for influence. In the memorable words of Jean-Francois Bayart, government bureaucracies in Africa are "institutional trees in the factional forest." 90  In such a context, government institutions take form primarily because international law makes them necessary for interactions between segmentary African societies and the broader international system. 91 State bureaucracies thereby become yet another source of financial resources and political influence, and hence the subject of competition among patrimonial networks. 92 As a consequence, African states tend to be weak by global standards for reasons deeply rooted in geography, economics, social structure, and history.
As a corollary, political networks both above and below the state level (supranational and subnational) tend to be more salient in Africa than in most regions of the world. 93 In Africa, subnational patrimonial networks are often the most-important actors in domestic political dynamics. 94 Supranational and transnational networks-such as the United Nations, the African Union, multinational corporations, and extracontinental powers-play key roles as well. 95 There is a longstanding tradition of both subnational networks and transnational networks cooperating so that both of these combined forces can compete against each other for influence in African states. 96  Given the Clausewitzian dictum that war is politics by other means, it comes as no surprise that the decentralized political systems in Africa influence how military force is employed for political purposes. Although African conflicts are diverse and complex, they tend toward unconventionality, network-centricity, and a strategic emphasis on coercion rather than conquest. These patterns might collectively be labeled patrimonial warfare. 97  A primary characteristic of patrimonial warfare is its unconventionality. Belligerents in Africa frequently lack the internal resources necessary to support large-scale, standing statutory forces. 98 Control of territory is much less valuable politically and economically than in many more-developed regions of the international system, where conventional warfare evolved. 99 Decisive military results are difficult, perhaps even frequently impossible, to achieve in these decentralized political and social systems. 100  Second, the belligerents in African conflicts tend to be networks rather than statutory militaries. Political violence in Africa tends to occur between patrimonial networks vying for position or control of some asset. 101 Even when one or more of the belligerents have institutional trappings and statutory forces, the fundamental dynamic of the conflict tends to be patrimonial. 102  Third, conflicts in Africa tend to draw in actors from outside the continent. Africa's informal, network-based social and political systems make its conflicts uniquely permeable to outside involvement. 103 Moreover, there is a longstanding tradition of local networks seeking to involve external actors to alter the local balance of power. External actors have their own objectives, of course. Great powers are routinely intent on countering African networks that they perceive to be dangerous to their interests. Foreign corporate interests-whether stateowned enterprises from China or France or purely commercial corporations, such as Anglo-American Mining and Royal Dutch Shell-seek to capitalize on Africa's incredibly rich natural resources. Transnational networks, such as ISIL and Lebanese Hezbollah, encroach into Africa in search of influence, undergoverned space, and willing recruits. The result is that African conflicts are unusually likely to draw in actors from the outside.
Fourth, force is typically employed to coerce rather than to defeat an adversary network. Because decisive victory is unlikely in most African political contexts, military conflicts in 97 For a useful review of the literature on patrimonialism and neopatrimonialism as it relates to African security, see Paul D. Williams, War and Conflict in Africa, Cambridge, United Kingdom: Polity Press, 2011,  pp. 55-71.   98 Richard J. Reid, Warfare in African History, Cambridge, United Kingdom: Cambridge University Press, 2012, pp. 
1-18.
14, No. 4, Spring 1990;
Jeffrey Herbst, "African Militaries and Rebellion: The Political Economy of Threat and Combat Effectiveness," Journal of Peace Research, Vol. 41, No. 3, May 2004.
Herbert Howe, Ambiguous Order: Military Forces in African States, Boulder, Colo.: Lynne Rienner, 2001.
North and West Africa," Terrorism and Political Violence, Vol. 33, No. 5, 2021.
This sketch portrait of African strategic dynamics based on the expert literature highlights the informal, distributed, network-centric, and patrimonial nature of competition and conflict in Africa's vast UGS. And although space limitations prevent anything more than generalization, this description does suggest why the United States struggles to perceive the strategic and military dynamics of African UGS. The USG sees like a state and DoD sees like an institution, which could not be more diametrically opposed to the structure and fundamental dynamics of the African context. It could be said that DoD analysts and policymakers suffer from selective blindness, like color blindness, in which certain aspects of African strategic dynamics are visible but others are completely invisible or indistinguishable from those that are visible.
Although this blindness might be particularly evident in the context of Africa's UGS, it is certainly not limited to Africa. The long war in Afghanistan has also occurred in a country with minimal carrying capacity, low population density, distributed subsistence agriculture, diffuse and segmented social networks, a patronage-based political system, and patrimonial patterns of warfare. 108 This strategic context affects how wars are fought in Afghanistan. 109  Efforts by the United States and its coalition partners to build a highly centralized, capable Afghan state apparatus with reach throughout the hinterlands-supported by professional and apolitical, centralized security forces-suggest categorical blindness to the fundamen-tal characteristics of Afghanistan's UGS.
110
Similarly, when we describe the structural characteristics of African and other UGS as informal, diffuse, network-centric, patronage-based, and patrimonial, what we are describing is a complex adaptive system. 113 Similar to other such systems, the strategic dynamics in UGS feature rich and nonlinear interactions among actors, feedback loops, and adaptive, emergent behavior. 114 The functioning of the overall system cannot be predicted from the behavior of the individual units, nor will the "inputs" of an external actor, such as the United States, produce linearly predictable "outputs" in terms of effects on a competition and its participants. 
115
Taken together, these observations indicate that when DoD operates in UGS, it is participating (however unwillingly) in complex infinite games. Moreover, at the government, organizational, and individual levels, DoD is uniquely ill suited to accurately perceive and understand the strategic dynamics of these complex infinite games. Selective blindness is an apt metaphor for this situation, in which the United States observes UGS from the governmental, institutional, and individual perspectives but is unable to perceive and distinguish essential features ranging from actors to causal dynamics. Unsurprisingly, despite best efforts, the results are ambiguous. More insidiously, the results might be impossible to discern in the first place.
Moreover, while the United States might initially encounter complex infinite games as analytical problems, the games clearly pose challenges that extend beyond seeing to the realm of doing. 
116
Although the United States, and DoD in particular, finds UGS uncongenial and frustrating, the historical record suggests that UGS cannot simply be ignored. Since the early 20th century, the United States has repeatedly attempted to eschew involvement in UGS, only to be pulled in by threats and opportunities that repeatedly arise in them. The 2018 National Defense Strategy, with its emphasis on competing with revisionist great powers and suppressing the threat posed by VEO networks, points directly toward continued and perhaps even growing DoD commitments in UGS in Africa, the Middle East, Central Asia, Southeast Asia, and perhaps Latin America.
There is thus a need for new analytical techniques that can provide new lenses for DoD to perceive and understand the complex infinite competitions it encounters in UGS. These lenses must incorporate the information and signifiers that locals use to understand and navigate these competitions, and they must be general tools that can be rendered bespoke to specific times and places. Perhaps most challengingly, they must translate the informal complexity of UGS to individuals who see like generalists, a department that sees like an institution, and a government that sees like a state. The USG and DoD will not transform themselves wholesale to operate more effectively in complex infinite games. Instead, new analytical lenses are required that will translate effectively between the two types of reality.
Recent advances in artificial intelligence, machine learning, modeling, and gaming could individually and collectively contribute to the development of such lenses. In particular, from an analytical perspective, the development of artificial intelligence or machine-learning tools might permit the sociocultural and political signatures of African societies to be collected at comprehensive scale, correlated longitudinally, visualized in real time, and, ultimately, rendered legible to generalists in DoD and other USG institutions. Emerging modeling and gaming capabilities, on the other hand, might allow this analytical portrait to be projected forward in time to help policymakers understand the likely results of their choices. Several components of mosaic warfare point the way toward these capabilities.
Jonathan S. 
Blake, Berggruen Institute
One way to conceptualize the spaces that fall between governed and ungoverned is as undergoverned. 2 The concept of undergoverned spaces (UGS) calls attention to the territories, populations, and issues that are neither black nor white but shades of gray. Yet the spectrum that runs from governed through undergoverned to ungoverned is only one dimension of governance. In addition to this quantitative dimension focused on how much governance 1 Thomas Hobbes, Leviathan, Richard Flathman and David Johnston, eds., New York: W. W. 
Norton & Company, [1651]
2 Some earlier U.S. Department of Defense conceptualizations of ungoverned spaces used the term as a general category that included undergoverned spaces. However, as this chapter argues, it is more useful to separate the two terms. See, for example, Robert D. Lamb, who defined ungoverned areas as follows:
A place where the state or the central government is unable or unwilling to extend control, effectively govern, or influence the local population, and where a provincial, local, tribal, or autonomous government does not fully or effectively govern, due to inadequate governance capacity, insufficient political will, gaps in legitimacy, the presence of conflict, or restrictive norms of behavior. For the purposes of this report, the term "ungoverned areas" encompasses under-governed, misgoverned, contested, and exploitable areas as well as ungoverned areas. In this sense, ungoverned areas are considered potential safe havens. 
(Robert D. Lamb, Ungoverned Areas and Threats from Safe Havens, Washington, D.C.: Ungoverned Areas Project, 2008, p. 6)
The qualitative dimension reveals the limitations of conventional state-centric notions of UGS. In much of the world, the state is not the only governance provider. Thus, deviations from the idealized model of absolute state sovereignty are quite normal. Alongside state undergovernance, we often observe alternative forms of governance. Many actors working in many configurations provide governance, resulting in a diverse array of outcomes for the population.
This chapter explores these alternative governance arrangements around the world, filling in the qualitative dimension and providing a more realistic view of governance as it actually exists. I start with a discussion of concepts and definitions. Then, I discuss undergovernance and alternative governance and how to distinguish between them. I end with some concluding thoughts about the two.
Before identifying governance systems, it is crucial to briefly define and differentiate two key terms: the state and governance. The state, for our purposes here, is defined, very minimally, as "the functioning of executive branches and their bureaucracies." 3 This definition presupposes no notion of the strength of the state or what goods or services it provides. 3 Francis Fukuyama, "What Is Governance?" Governance, Vol. 26, No. 3, 2013. Although much of my discussion here draws on Stephen D. Krasner and Thomas Risse, I prefer Fukuyama's definition of the state to theirs. Following Max Weber, Krasner and Risse conceptualize "statehood as an institutionalized structure with the ability to rule authoritatively (Herrschaftsverband) and to legitimately control the means of violence" 
(Stephen D. Krasner and Thomas Risse, "External Actors, State-Building, and Service Provision in Areas of Limited Statehood: Introduction," Governance, Vol. 27, No. 4, 2014, p. 549)
Post-Apartheid South Africa, New York: Oxford University Press, 2019, p. 192
(Max Weber, From Max Weber: Essays in Sociology, H. H. Gerth and C. Wright Mills, eds., New York: Oxford University Press, 1948, p. 78
4
2
10
11
12
13
One way to sharpen our thinking about governance outside "domestically sovereign" states is to shift our perspective from a focus on what is absent to a focus on what is present. 
14
15
The focus on alternative governance arrangements is not intended as necessarily laudatory. It is important to consider both state governance and its alternatives, not because the alternatives are always better than the state, but because alternative governance is the day-today reality for much of the world's population. To promote such a focus, as Paul Stacey and Christian Lund argue, is "not to romanticize" alternative governance or to view it as the consummation of an emancipatory project of popular rule. For many, life in places like Old Fadema [an informal settlement in Accra, Ghana] remains nasty, brutish, and short, despite efforts at self-governance and information regulation. Yet, as actual governance, it deserves actual attention. 16   Alternative governance is important not because it is desired or desirable but because it shapes the lives of many millions of people around the world. 17 We cannot understand the dynamics of these places without understanding the alternative arrangements that govern them. Therefore, we ignore alternative governance at our own peril.
Alternative governance has identifiable forms, many of which have been conceptualized and theorized as distinct phenomena. For instance, in recent years scholars have published excellent studies on hybrid governance; 18 rebel governance; 19 criminal governance; 20 human- 16   17 Lessing estimates that "tens if not hundreds of millions of people live under some form of criminal governance," which is just one of many alternative governance arrangements found today 
(Benjamin Lessing, "Conceptualizing Criminal Governance," Perspectives on Politics, Vol. 19, No. 3, September 2021, p. 854
21
22
The remainder of this chapter builds on the existing scholarship to develop a more general framework for understanding governance outside consolidated statehood. To do that, I discuss the following questions: Who governs alternatively? What is governed alternatively? What spaces are governed alternatively? What is the interaction between alternative governance and state governance?
Alternative governance can involve a wide array of actors. It has been observed that a diverse cast provides governance in settings around the world-from international humanitarian nongovernmental organizations (NGOs) and transnational corporations to traditional ethnic organizations and religious institutions to criminal organizations and rebel groups. These actors might appear to have little in common, and some of them appear to have little to do with governance-or might be better known for undermining governance-but they all govern territory or issues, even if their governance is not readily apparent when viewed through a state-centric lens. This diversity of actors is a hallmark of alternative governance: It can be provided by anyone who can provide it. 
23
24
Unarmed alternative governance providers take many shapes and forms, such as community-based organizations, religious institutions, ethnic organizations, international Plural Orders in Rio de Janeiro, Brazil," Current Sociology, Vol. 65, No. NGOs, and private-sector firms. 25 Take, for example, the work of Old Fadama Development Association, a community-based organization that delivers varied and important governance functions in an informal settlement in Accra, Ghana, which is home to 80,000 people but "legally invisible" to the state: They look out for new constructions that block pathways; they call on emergent businesses; they ensure broader access roads are kept clear of containers and vehicles; they caution young people riding motorbikes carelessly; if they spot leaky pipes they contact volunteer plumbers; they identify fire hazards and endeavour to "keep the rubbish moving" to minimise problems with vermin; they look in on recurring domestic disputes and shoo children to school; they follow up on complaints of theft and damage to property, and pursue disagreements over rental payments; they give newcomers advice on building; after heavy rain they inspect low-lying areas for flooding; they rally communal labour to clear blocked waterways and ensure unsafe buildings are demolished after outbreaks of fire; they also organise the collection of contributions to cover medical bills, funeral expenses and support to families when a deceased person must be returned to what is often a remote northern village; and in some instances they cover bail money when it cannot be raised by relatives. 26   In the absence of state-provided governance, residents are compelled to "make an active and conscious effort beyond their own doorsteps" to "uphold common standards to make life bearable" in the community. 27 Unarmed actors can even sometimes play this role in places where armed actors dominate governance provision. 28 This is particularly the case when unarmed actors have access to authority rooted in economic, social, cultural, or charismatic power.
Nonstate armed groups are not often known for their governance provision, but many of them provide it. Armed actors have an advantage when it comes to governance provision: the ability to use force to enforce their rules and regulations. As Lessing explains, While these groups have the ability to resort to coercion, not all of their governance is provided at gunpoint. As with state governance, the use of force can fade from immediate view and operate more as a background condition for day-to-day governing.
Most rebel groups and criminal organizations do not govern people and territory or only do so in limited ways-such as armed groups that extract taxes from civilians but provide little in return. 30 But some offer a wide array of rules, public goods, and services. Armed groups provide some of the most comprehensive governance of any alternative governors. This is particularly the case for rebel governors, who, Mampilly argues, must "replicat[e] some of the functions and forms of the nation-state . . . [in order] to derive support for [their] political authority and achieve some form of legitimacy" from the civilian population. 31 Having displaced the state from the territory that they control, rebel groups are able to provide any amount of governance they are capable of and choose to provide. 32 Some go so far as to create de facto states with all the characteristics of a state except for international legal recognition. For example, Somaliland, a de facto state within the borders of Somalia, has its own government, legislature, court systems, and police. The enclave engages in regularized taxation, provides public services such as health and education, conducts trade with international partners, and even boasts a separate central bank that issues currency (the Somaliland shilling). 
33
Megan A. Stewart, "Civil War as State-Making: Strategic Governance in Civil War," International Organization, Vol. 72, No. 1, 2018.
(Mancur Olson, "Dictatorship, Democracy, and Development," American Political Science Review, Vol. 87, No. 3, 1993)
See Arjona, 2016
Brazil," Studies in Comparative International Development, Vol. 48, No. 3, September 1, 2013.
Alternative governance can, likewise, govern a wide variety of behaviors, both quantitatively and qualitatively. That is, some alternative governance structures govern many aspects of life, while others govern a very limited set (the quantitative dimension). And the behaviors that the structures govern run the gamut (the qualitative dimension), covering many of the things that states do-even if in a somewhat different way. For instance, Lessing lists six governance functions that are widely found to be provided by criminal organizations: policing and enforcement; emergency response; and judicial, fiscal, regulatory, and political operations. 37  But not all criminal organizations or other nonstate actors that govern people or territory perform all of these functions; they can strategically select which functions to carry out and to what degree. 38 Even taxation, a function that we might assume all alternative governors would take part in, is not universal. Some alternative governors certainly collect a tax from the people and activities that they govern, while others eschew taxation and gain revenue in other ways. 39  At times, even the same alternative governor governs different features of life in different places. For example, civilian behaviors governed by the Revolutionary Armed Forces of Colombia (FARC) could vary from village to village. In one village, the FARC 34  
Lessing, 2021;
Barnes, 2017;
Beatriz Magaloni, Edgar Franco-Vivanco, and Vanessa Melo, "
A Case Study of Children in Organised Armed Violence in Rio de Janeiro, Rio de Janeiro, Brazil: Child Rights Resource Centre, 2003, p. 64)
"came here, walked by, told us things, asked that we did certain things like not talking to the army . . . We had to obey them in certain ways, of course, because they have the weapons. But [the peasant leaders] are the authority here . . . They didn't rule us." But in a village just two kilometers away, a resident recalled, "The FARC were everything . . . They had the last word on every single dispute among neighbors. They decided what could be sold at the stores, the time when we should all go home, and who should leave the area never to come back . . . They also managed divorces, inheritances, and conflicts over land borders. They were the ones who ruled here, not the state." 40   While the degree of governance by the FARC was clearly geographically uneven, taken together, the insurgent administration provided substantial services to the inhabitants of its territory, including health and education systems, a police force to maintain stability, courts to adjudicate civil and criminal disputes, and even loans to farmers and small businessmen. It also engaged in extensive public works projects such as building roads and other infrastructure construction. 41   Places that look from the outside to be chaotic can contain strong alternative governance structures that rule and regulate various aspects of political, social, and economic life.
Alternatively governed spaces are found all over the world, and not only in places considered to be weak or failed states. They exist in low-income, middle-income, and high-income countries; in the global south and north; and in rural peripheries and urban centers. These spaces can be vast, such as much of the Sahel Desert and Amazon Rainforest, or very small, such as a single neighborhood or village. And they can border spaces of entirely different governance structures. While it is useful to think of governance as a spectrum, in terms of quantity and quality, on the ground, the transition between governance systems can be abrupt. For instance, in Brazilian cities, such as Rio de Janeiro and São Paulo, neighborhoods where state governance predominates can sit right next to favelas governed alternatively.
In the wealthy countries of the global north, alternatively governed spaces are not only relegated to marginal areas where we might expect to find them. In the United States, the state's limited presence in low-income urban areas, prisons, and sparsely populated rural areas has allowed for the emergence of alternative governance structures for the often poor and marginalized people who live there. 42 Less recognized is how wealthier citizens create alterna- 40 Arjona, 2016, p. 1. 41 Mampilly, 2011, p. 2. 42 See, for example, David Skarbek, The Social Order of the Underworld: How Prison Gangs Govern the American Penal System, New York: Oxford University Press, 2014; and Harel Shapira, Waiting for José: The tively governed spaces for themselves. American private security companies, for instance, employ 1.1 million people, nearly double the national number of state-provided police and sheriff's patrol officers. 43 Beyond security, many wealthy Americans opt out of many of the state's governance functions, from education to emergency response, relying instead on alternative sources. 44 Gated communities, in this sense, constitute sites of alternative governance right in the heart of spaces of strong state control. 
45
However, alternative governance is more common in middle-and low-income countries, where the state is often not fully consolidated or institutionalized. As in the global north, there is a two-tiered system of alternative governance-one for the wealthy and one for the poor-but the divergence is even starker. For the upper and (at times) middle classes, this means opting for privately provided governance arrangements: paying private suppliers for a regulatory order and service delivery that is superior to the state's. 
46
47
September 23, 2020;
and Robert J. Bunker and Pamela Ligouri Bunker, eds., Plutocratic Insurgency Reader, McLean, Va.: Small Wars Foundation, 2019.
Alternative governance can emerge where the state is weak or absent, but it does not have to. Alternative governance is not an automatic response to insufficient state governance. In some cases, alternative governance structures do not or cannot emerge, and disorder reigns. 52 Similarly, it does not only emerge where the state is weak or absent. Alternative governance, as noted already, can also exist in spaces of relative state strength. Like most aspects of alterna-tive governance, its relationship to the state can vary widely, and there are multiple observed relationships between alternative governance and state governance. As Lessing remarks about criminal governance, "States may actively contest 
[it]
But the most-interesting examples of cooperation are the most-counterintuitive ones: cooperation among actors engaged in armed conflict against each other. For instance, criminal organizations in Brazil "often collaborate with a variety of state actors to create varied systems of localized order that perpetuate criminal power and undermine most policy efforts to control crime and violence." 57 In Afghanistan, the government's service delivery ministries have struck deals with local Taliban; most provincial or districtlevel government health or education officials interviewed said they were in direct contact with their Taliban counterparts, and some have even signed formal memoranda of understanding with the Taliban, outlining the terms of their cooperation. 58  incompatible. This is most clear in the case of governance provided by rebel groups who are actively contesting-and are actively contested by-the state. Rebel governance is thus often "a process of competitive state building" targeting the existing state. 60 For instance, some rebel groups have established fairly robust institutions for providing a variety of governance functions and services in the areas they control. These institutions are designed to replace, not merely supplement, the state. Therefore, they are often confined to territories that have been "liberated" by a rebel group. 61 When the state attempts to implement state governance in these areas, the effort generally requires the use of force and is often a deeply destabilizing process.
Yet there are many relationships that fall somewhere between cooperation and contestation. For instance, there are alternative governance structures where nonstate actors have essentially replaced state governance in certain areas or for certain issues, yet the state does not contest the alternative governance. The state might be happy to outsource this governance to other actors, it might simply tolerate alternative governance, it might be too weak to stop alternative governance, or it might opt to abdicate its provision of governance as a political strategy. Rachel Kleinfeld and Elena Barham find that even some high-capacity, democratic states choose not to provide order and security to certain segments of the population as a strategy to maintain power. 62  In other situations, the relationship between state and nonstate actors is so entangled that the governance that emerges is described as a hybrid of the two. Hybrid governance sometimes refers to arrangements where both state and nonstate actors provide governance functions, 63  but there is also a narrower meaning promoted by such scholars as Colona and Jaffe. For them, hybrid governance refers not merely to situations where nonstate actors perform statelike functions but to "contexts in which state and non-state actors are highly intertwined and merged, often to the extent that we can speak of a new or emergent political formation that is neither state nor non-state." 64 This formation of alternative governance thus blurs the line, often thought to be quite rigid and clear, between state and nonstate actors and governance. 65   60 Stathis N. Kalyvas, The Logic of Violence in Civil War, New York: Cambridge University Press, 2006, p. 218. 61 As with so much about alternative governance, this is not absolute and there is a spectrum. Governance provided by the Taliban, for instance, often precedes territorial control rather than follows from it 
(Jackson, 2018)
Blake, and Karishma Patel, "Hybrid Governance of Disaster Management in Freetown, Monrovia, and Dar Es Salaam," Disasters, Vol. 46, No. 2, April 2022
Colona and Jaffe, 2016, p. 176;
Gupta, Verrest, and Jaffe, 2015;
Jaffe, 2013
25, No. 2, 2012
How can one know whether a place is undergoverned and, if it is, what the alternative governance arrangements are? Answering the first question requires paying attention to the state and its institutions, while answering the second requires a more expansive focus. For ease of analysis, the questions can be answered sequentially. For any place of interest, one must ask:
1. Who makes the rules and regulations that are actually obeyed? 2. Who provides the goods and services that are actually received?
If the answer to both questions is the state and only the state, the place in question is likely not undergoverned. The state is consolidated and institutionalized and projects its authority throughout the space. The state governs. There will almost always be other actors involved in the process-from subcontractors hired to implement governance to criminals seeking to undermine it-but the state is the ultimate authority.
If the answer to either question is not the state or not only the state, then the place is undergoverned. 
66
Scholars have developed numerous metrics of state capacity that can be used to identify governed and undergoverned spaces. 67 The recent turn to collecting subnational measures of state capacity provides analysts with especially useful data. For instance, Lee and Zhang calculate the state's ability to collect accurate age data in national censuses. Inaccurate data collection, which can be measured at national and subnational levels, suggests that the state's presence is limited. 68 Luna and Soifer suggest using surveys to ask populations directly about their experiences of several aspects of state capacity, in particular "the state's reach across territory, its ability to impose taxation, and its effectiveness in the provision of property rights." 69  However, identifying the alternative governance arrangements is a more fraught endeavor, one for which consistent measures that are valid cross-nationally or even across a single country are difficult to come by. Alternative governance arrangements, as this chapter has discussed, are often highly localized and temporally specific. For this reason, empirical studies of alternative governance are often based on fieldwork, and ethnographic research in particular. With fine-grained, locally specific data, whether quantitative or qualitative, analysts can gain a clear understanding of the forms and functions of alternative governance structures. Getting to know UGS can be difficult and costly, but, ultimately, they are not unknowable.
UGS are often seen as marginal places set apart from modernity. Because they lack key elements of state-based order that are a hallmark of Western modernity, UGS are thought of as outside modernity, perhaps even untouched by it: Such terms as traditional and barbaric often come up in descriptions of UGS. This is not the case. UGS are as much a part of the contemporary global order as places of consolidated state governance. They are not cut off from the rest of the world; they are highly embedded in national and international political, economic, and social orders. They enable, are connected to, and are created by these local, national, and global orders. 70 Some UGS are directly and deliberately created by foreign actors. 71   68 Melissa M. Lee and Nan Zhang, "Legibility and the Informational Foundations of State Capacity," Journal of Politics, Vol. 79, No. 1, October 20, 2016.   69 Juan Pablo Luna and Hillel David Soifer, "Capturing Sub-National Variation in State Capacity: A Survey-Based Approach," American Behavioral 
Scientist, Vol. 61, No. 8, July 1, 2017, p. 892
Ledeneva, Anna Bailey, Sheelagh Barron, Costanza Curro, and Elizabeth Teague, eds., Global Encyclopaedia of Informality, Vol. 1, London, United Kingdom: UCL Press, 2018, p. 5)
(Lessing, 2021)
At the same time as these spaces are considered outside the bounds of modern institutions, it is often assumed that the governance arrangements that exist within them are a pathway to modern statehood. Alternative governance is commonly framed as state-making, and the institutions that are developed are considered "states in waiting." While this is sometimes true, especially among rebel governors, it is not always the case. Alternative governance arrangements can be proactive and constructive without necessary being geared toward creating a state or state-like institutions. As this chapter has highlighted, some alternative governors are quite content to only govern certain aspects of life and have no interest in taking on all the responsibilities of statehood. Others create new governance structures where state and nonstate actors are so entangled that the hybrid state that emerges barely resembles a state as classically conceived. 72 These forms of governance are not always stepping-stones to statehood; they can be endpoints themselves. Analysts based in the global north typically assume that everyone wants to be like "us" (Western), but not everyone does. There are "multiple modernities" and many ways to govern people and territories in the modern world. 73  The concept of UGS still carries a statist bias. As a result, UGS are often looked for within the borders of states. But there are several critical, global issues that inherently transcend state boundaries, such as climate change and pandemics. These global issues are undergoverned, but not because states lack consolidation or institutionalization. Rather, the nature of these problems is fundamentally global in a way that makes state governance insufficient; states are an inadequate institution to govern such issues as global climate change at the planetary scale. Governing these issues requires finding alternative arrangements to the international system of sovereign states. 74 However, what those alternative governance arrangements must look like is an open question. 72 Jaffe, 2013. 73 Shmuel N. 
Eisenstadt, "Multiple Modernities," Daedalus, Vol. 129, No. 1, 2000
Chicago Law Review, Vol. 72, No. 4, September 1, 2005
, p. 1167)
March 9, 2021
1
5
Moreover, given the diversity of interviewees' backgrounds, discussions yielded insights on a variety of focus areas, such as explorations of policymaking in specific cases, examinations of the process by which the U.S. government acquires technologies, and considerations of the value of particular tools and approaches for engagement in UGS. Table 
5
Interviewees' perspectives are presented in the next three sections. The first section provides interviewees' views on engagement in UGS. The second section discusses analytical challenges associated with analysis and adaptive decisionmaking in UGS. The third section discusses specific areas of investment that interviewees identified as having the potential to provide relevant, high-impact capabilities for improving engagement in UGS and increasing the adaptive capabilities of DoD and the broader NSE. 3 Two interviews were conducted with multiple experts participating. 
Throughout the interview process, participants identified numerous challenges to policymakers' engagement in UGS. Interviewees presented a wide variety of perspectives on the relative priority and preferred outcomes of U.S. policy in UGS in the policymaking process, the complex bureaucratic environment in which UGS sit, and the organizational capacities that DoD and the NSE should adopt for greater success in UGS. The following section explores these themes in greater detail.
Engaging in UGS Presents Uncertain Payoffs for U.S. National Security Objectives Among several interviewees, there was a sense that UGS largely represent low-level DoD policy priorities relative to high-intensity conflict with great and regional powers and that UGS were better addressed by other agencies within the NSE. 4 There was little agreement 4 RAND Interview E9A4, December 2020; RAND Interview D6B0, December 2020. a This table demonstrates the diversity of views posed by interviewees based on their professional backgrounds and experiences but provides minimal contextual details on their backgrounds and positions. For the remainder of this chapter, all participants will only be identified as "interviewee" or "interviewees" to maintain anonymity.
across the bureaucracy about whether and how much the United States should be engaged in UGS. At the core of this disagreement was a lack of consensus on the criteria that would justify whether to engage in UGS, support other governments or actors, monitor, or completely disengage. While interviewees accepted these spaces as arenas where extended great-power competition occurs, it was not clear what U.S. policy goals and risk estimates would need to be to unambiguously warrant engagement in specific cases. 5 Policymakers need to sharpen strategic assessments and national objectives to better differentiate among necessary engagements, opportunities to consider engagement, and potentially costly blunders.
Several interviewees noted the limitations of planning and analysis for UGS to support the highest levels of strategic assessment. Resources and long-term planning processes are dominated by traditional warfare, thus leaving a gap in policymakers' understanding of UGS. As one interviewee noted, wargaming and planning are built around preparing for "World War Three." 6 Similarly, another interviewee noted that the U.S. armed services, for example, are primarily concerned with how they spend money in governed spaces, leaving little time or resources for UGS: "Undergoverned spaces are sort of an afterthought. The services don't get excited about this." 7  Finally, engaging in UGS could pose significant challenges at the political and operational levels. As one interviewee noted, it is one thing to engage in places where governance has collapsed and is openly contested (e.g., failed states). It is quite another to engage in shadowy corners of sovereign states, such as the Federally Administered Tribal Areas in Pakistan. 8   The United States Might Not Be Able to Achieve Its Preferred Policy Outcomes in UGS, Leading to Disagreements About Achievable Goals
Interviewees disagreed on the extent to which the United States could reasonably expect to achieve its preferred outcomes in UGS. Skeptical interviewees argued that the ideal outcome is often simply unrealistic. Other interviewees noted that high-level goals in UGS were fairly uniform across policymakers; where disagreements arose was over feasible goals-given the constraints on U.S. power, what is realistic and possible? 9 One interviewee argued that the United States is working against broad, systemic trends in many areas, and thus persistent, costly, long-term engagements in UGS would likely be unproductive in reversing these dynamics. 10 Similarly, two other interviewees saw UGS as unlikely to receive the sustained 5 RAND Interview E9A4, December 2020. 6 RAND Interview B6D1, September 2020. 7 RAND Interview C3A0, October 2020. 8 RAND Interview C7C9, January 2021. 9 RAND Interview C7C9, January 2021. 10 RAND Interview E9A4, December 2020. attention and resources that effective engagement would require. As a result, they predicted that engagements would be more costly and less productive than desired. 11  In one salient example, an interviewee noted that shifting claims and norms in the South China Sea-which could represent an undergoverned space in the U.S.-China relationshipare never going to be resolved in favor of the United States because of China's structural advantages in geographical distance and regional knowledge. 12 The interviewee argued that DoD's understanding of competition (e.g., the United States lost its historical balance of power in Asia to China and therefore must recover it) is insufficient, insensitive to these structural concerns, and anchored in returning to an unrealistic position of military and economic dominance in the region. 13  Other interviewees spoke about the lack of understanding about acceptable risk and the expected consequences of U.S. policy choices in UGS. For example, one interviewee noted that the most helpful analysis they encountered on Afghanistan was an assessment of uncertainties and identification of risk and how to understand the second-and third-order effects of policy decisions rather than the optimal allocation of U.S. resources. 14 Another interviewee argued that the high-level goals across the government on engaging in UGS are often shared by stakeholders, yet consensus cannot be reached once real-world constraints are imposed, and policymakers need to prioritize among multiple objectives; this creates confusion about what a "good enough" solution looks like. 15   
Beyond the challenges posed by developing well-formed goals and strategies within DoD, there are many players across the NSE who have a stake in the policymaking process for UGS. Interviewees often highlighted differences between DoD and U.S. Department of State (DoS) perspectives. DoD typically has a specific, actionable end state that it is trying to achieve, whereas DoS views continued engagement as an important goal in and of itself. 16 This difference creates tension between offices with different missions and capabilities. In general, DoD is not the lead agency for policy decisions in these spaces and must constantly work within a larger group of stakeholders with varied incentives and interests. Yet DoD's resources dwarf those of underfunded civilian agencies, resulting in DoD being asked to take on more and different roles-despite the fact that its personnel often do not have the expertise and are 11 RAND Interview D6B0, December 2020; RAND Interview C3A0, October 2020.
12 RAND Interview E9A4, December 2020. 13 RAND Interview E9A4, December 2020. 14 RAND Interview D3A8, October 2020. 15 RAND Interview C7C9, January 2021. not well suited for key engagement tasks in UGS, such as postconflict reconstruction, law enforcement, humanitarian assistance, and cultural outreach. 
17
The role of the National Counter Terrorism Center was to bring together all of the different department and agency views and preferences-[National Security Agency] NSA, [U.S. Cyber Command] CYBERCOM, [Central Intelligence Agency] CIA, [Federal Bureau of Investigation] FBI-and come to a resolution. Each organization comes to the process with their equities and we had to be a trusted party to represent their views objectively. 
18
19
Bureaucratic structures, barriers, and habits compound the inherent difficulty of reaching decisions with large numbers of actors. One interviewee emphasized that because of the political process, leadership is turned over in a fashion that is not conducive to long-term policymaking. 20 Moreover, the relative power of certain offices over others can cause confusion about goals and strategies on complex issues. For example, regional offices in the Office of the Secretary of Defense for Policy, which generally deal with issues through bilateral relationships with allies and partners, often compete with functional offices, which generally are charged with long-term strategy formulation. Many interviewees also observed that the desire to centralize policymaking creates bottlenecks in reaching decisions. One interviewee noted that far too many issues percolate up to senior levels because subordinates are unable or unwilling to settle issues at the working level. 21 Another interviewee argued that the U.S. government has "split portfolios into ever finer detail" and that the smallest decisions need to be "coordinated at so many levels," such that a decision is taken as a point of debate or an oppor-tunity to elevate to the next level. 22 The end result is that senior leaders are presented with either nondecisions or decisions that should have been made at a much lower level, which is a costly use of their limited time and attention.
Another coordination issue has to do with classification barriers. Several interviewees noted that information needed to coordinate action in UGS-such as space or cyber-is tightly controlled, thus limiting both the ability to develop a common understanding of the environment and the capabilities for acting within it. 23 One interviewee noted that policymakers do not have a good understanding of the tools and capabilities in the irregular warfare realm because of classification issues. 24 For example, the partner engagement activities of special operations forces (SOF) are opaque to many DoD and NSE stakeholders. Certain SOF authorities allow SOF to provide training and equipment to actors within countries where other programs are already operating, thus risking competing or contradictory efforts or duplication as a result of fragmented visibility and poor coordination.
A common theme in our interviews was the importance of multi-stakeholder engagement. One interviewee discussed the value of gathering a group of experts with diverse experiences and views in a series of meetings and iterative workshops. They remarked that there is "substantial value in talking to people with really different opinions and [e]ffecting a synthesis. Or getting people with different opinions to argue in front of me." 25 The interviewee also noted that these types of engagements can help push leaders to change their minds if the right context is provided. 26 For this method to be successful, the engagement must be repeated to "sift and sort through information and eventually find a point of convergence." 27 If an engagement is a single event, participants are more likely to stick to what they know, come to the discussion with their hobbyhorses, and not change their minds. 28  Another interviewee highlighted two issues that could be solved by multi-stakeholder engagement. The interviewee mentioned that in their area of expertise, policymakers are worried about both too much and too little security: Too much security leads to a military buildup and a security dilemma, but too little security leads to illegal activities and an inabil- 22 RAND Interview D8A8, October 2020. 23 RAND Interview A5A8, November 2020. 24 RAND Interview B6D1, September 2020. 25 RAND Interview A5E7, December 2020. 26 RAND Interview A5E7, December 2020. 27 RAND Interview A5E7, December 2020. ity to enforce international law. 29 Multi-stakeholder engagements are particularly helpful for solving the issue of too little or too much, because they bring leaders together to find the balance between encouraging stakeholders to commit resources to building and preserving governance capacity and overcommitting resources in pursuit of national interests that might trigger a security dilemma and undermine governance institutions. 30  This same interviewee also discussed how multi-stakeholder engagements at senior levels signify that leadership views UGS as important, both to other parts of the NSE and to those operating within the space. For example, the interviewee noted that active involvement by senior government officials in Arctic policy signaled the importance of international law and norms in the region. Furthermore, they noted that a successful sign of governance in UGS was visible in the Arctic, where the institutions at the foundation of regional governance rested on global laws and norms (e.g., the United Nations Convention on the Law of the Sea); this limited the extent to which actors were willing to risk actions in the Arctic that might undermine them in other domains. 31   
A recurring theme in our interviews was the need to maintain an organizational commitment to exploration and learning and the continuous search for alternative ways to deal with problems. Adaptation and innovation in long-term competition requires putting greater weight on experimentation and learning and less weight on binary success or failure criteria for technologies, operations, and organizations. How organizations manage risk and hedge is critical because it is the foundation of their ability to adapt. 32 As one interviewee noted, getting DoD to think in terms of a "campaign of learning" is critical to DoD's adaptive capability. 33 Organizations that are committed to exploring, adapting, and maintaining heterogeneity can better cope with complexity. 34  Interviewees noted that one way to measure an organization's commitment to exploration and innovation is through learning metrics. Learning metrics, as opposed to outcome metrics, track and measure changes in how organizations frame, understand, and monitor problems. Learning depends on observing and maintaining an organizational commitment to preserving heterogeneity and diversity at three levels-alternative views on the state of the world (data), alternative views on how the world works (models), and alternative views on 29 RAND Interview A3E1, November 2020. 30 RAND Interview A3E1, November 2020. 31 RAND Interview A3E1, November 2020. 32 RAND Interview D0E6, October 2020. 33 RAND Interview A0C2, January 2021. 34 RAND Interview C2C2, December 2020.  what to do about the problem (goals and actions). As one interviewee noted, "how decisionmakers invest their time and attention away from things that they are predisposed to believe is an indicator of the adaptive potential of an organization." 35 Thus, instead of using direct outcome metrics, organizations could develop metrics focused on process and adaptation. One interviewee mentioned "triple loop learning"-how organizations "learn how to learn" by reflecting on how they learn in the first place-as a way to transform organizations and increase their capacity for learning. 36  Another dimension of adaptiveness highlighted by interviewees was the importance of policy testing so that policymakers can receive feedback on what is or is not working during implementation. Interviewees noted A/B testing, which provides a way to compare two versions of interventions into UGS to discover which one performs better. Multiple interviewees argued that breaking decisions up into smaller, modular pieces could allow the use of feedback from structured testing such that policymakers can make more-informed decisions by collecting feedback on the efficacy of small, localized actions before committing to larger resource decisions. For example, one interviewee advocated for an ink drop strategy, where policymakers look at the effects of one small decision to get early insights into possible outcomes and the effects of additional interventions: "Start with a small intervention, assess its results, and grow as you need." 37 Policy engagement should be guided by the logic of experiments that involve hypothesis generation, testing, and data collection. One interviewee indicated that the U.S. government already does this kind of testing to a degree but that it has not been systematically tracked or used:
We sent out probes, for example, through bomber assurance missions. You would do this and see what reaction you got to your probe from Russia. Track 1.5 dialogues can also help test policy options as a sort of "trial balloon." 38   Innovation in Government Business Practices Must Be Considered for Long-Term Competition Several interviewees noted the importance of investing in research on organizational design and management practices that could help make the U.S. government and commercial sector more competitive. One interviewee noted that a variety of mechanisms exist to incentivize research, development, experimentation, and innovation-such mechanisms as changes in accounting practices, tax credits, and incentives that could mobilize the commercial sector to 35 RAND Interview D0E6, November 2020; RAND Interview A5E7, December 2020. 36 RAND Interview D4C1, December 2020. 37 RAND Interview E2D8, November 2020. take on challenges in UGS at scales that exceed what government-sponsored research could support. 39  Several interviewees noted that DoD's acquisition system produced tension between the desire to try out an approach and the bureaucratic pressure to commit to it for an extended period. One interviewee provided an example from the weapons acquisition community:
There was a desire to move toward life cycle costing-the total cost of a system over its full life, including the cost of planning, development, acquisition, operation, support, etc. There is a pressure to look out 30 years and determine how many billions it will cost to buy a system. There is little tolerance for experimentation, for trying things out and failing. 40   Another interviewee agreed and noted that the U.S. government's requirements process is rigid and tends to push systems toward early closure. 41 As one interviewee noted, DoD should not try to be a "technology picker," given that it has not proven to be very good at picking winners. 42  Another area of investment for shifting business practices is evaluating program effectiveness by criteria other than the sophistication of the technologies that are developed. Additional criteria are community building and signaling the value of research approaches that sponsors value. One interviewee argued that creating sustained intellectual input and a sound body of ideas, practices, and techniques requires seeding new professions and disciplines, not just producing artifacts-particularly in the social sciences and the infrastructures affecting sociotechnical interfaces and practices. 43 The interviewee noted that research patrons did not create a new approach but rather tended to use their resources to signal interest in and legitimize particular approaches. 44 Programs that were successful were oriented around building a community of researchers rather than funding specific projects. Using programs to validate and secure promising research approaches and seed the research ecosystem represents one way in which government business practices could be changed to ensure adaptability and flexibility.
analysis, the challenges of sensing and data collection in UGS, information presentation, and the evaluation of the effectiveness of U.S. engagement in UGS.
Interviewees presented varied perspectives on how policymakers consume information. First, several interviewees indicated that policymakers might not be receptive to new or contradictory information and do not always seek out diverse information that challenges their views, thus limiting their ability to adapt as circumstances change. For example, one interviewee noted the following:
Most of the leaders I worked with did not want to entertain complex intelligence that contradicted their own worldview. Many of them will create their own reality. It is difficult to explain to decisionmakers that they are wrong. 45   Another interviewee noted that in situations where policymakers have already decided to move out on a particular course of action, analysis can simply "make their lives miserable." 46  A central issue is the relationship between analysts and policymakers. Policymakers might inherently resist the idea that analysts can reveal hidden assumptions and biases in their reasoning. As one interviewee noted, "Analysis that tries to change policymakers' preferences is beyond what analysis is capable of. They already know what they think." 47  However, this sentiment was not universal; other interviewees described a variety of consumer reactions to analysis. For example, one interviewee noted that some principals they worked with at the undersecretary level were willing to engage with analysis when it showed results that were unexpected: "I could tell them that their fundamental principles were wrong and they would say show me the data." 48 Similarly, another interviewee noted that although analysis is unlikely to change policymakers' intuitions or preferences, data or field research that is directly relevant to an immediate policy that they have to make, along with "information that is not an opinion," can help policymakers find a stronger basis for their actions. 49  A second issue on information consumption is the amount of time policymakers can devote to consuming analysis. Policymakers' attention is a scarce and valuable currency. Several interviewees noted that principals are often overwhelmed by paper and have little time to sit and read because their attention is drawn to emergencies and crises that require con- 45 RAND Interview C7A1, December 2020. 46 RAND Interview D1A1, October 2020. 47 RAND Interview C7C9, January 2021. 48 RAND Interview D1A1, October 2020. sideration and decisions. 50 Moreover, there is often not enough time to consider long-term analysis. One interviewee noted that the trend over time has been that decisionmakers have less patience with analysis and want insights and actionable analysis quickly:
The idea of analysis [that] would take 12 months is seen as unacceptable. Decisionmakers are generally willing to accept analysis that is unsophisticated if it at least gives them something to go with. Too often analysis is seen as something that takes too long and comes back with fine-grain results that 
[are]
Several interviewees raised the issue of the tension between the need for greater specificity for understanding complex problems and the need for increased simplicity for presenting information. One interviewee noted that when analytical approaches lack the ability to keep pace with policymaking needs, policymakers default to relying on their instincts because they do not have access to topic-specific, high-quality analysis when it is needed, which means that analytical outputs are often misaligned with needed policymaking inputs. 52  A third and final issue involving policymakers' consumption of analysis is the extent to which policymakers trust the analysis they receive. Interviewees noted that policymakers typically act using their own knowledge or instincts or recommendations by advisers that they trust. Several interviewees identified the extent to which policymakers were comfortable with data and models as a critical issue. One interviewee noted that data literacy varies among policymakers, which might make it difficult for them to distinguish between highquality and lesser-quality analysis. They noted that "[t]here is a risk that as people start to use the sophisticated tools that are available, it will look like they did sophisticated analysis. There is a greater need to provide checks on that process." 53  Another interviewee explicitly called out such computational methods as Agent-Based Modeling (ABM) and Bayesian logic models that were a "black box" to policymakers. Using these tools without an understanding of how they will be received by policymakers could increase the risk that the tools are simply ignored or dismissed if they produce analytical results that policymakers disagree with or do not understand. 54 One interviewee likened policymakers' apprehension about models to the distrust of automation in the U.S. military; the interviewee recounted an experience when the U.S. Air Force outfitted aircraft 50 RAND Interview B6D, September 2020; RAND Interview C7A1, November 2020. 51 RAND Interview D8A8, October 2020. 52 RAND Interview D1A1, October 2020; RAND Interview E2D8, November 2020. 53 RAND Interview A5E7, December 2020.
with heads-up displays but untrusting pilots simply turned the technology off and chose to fly the airplanes without that assistance. As that interviewee summed up the issue, "The problem is that people don't trust models." 55 Finally, interviewees emphasized the weight that principals put on their closest and most-trusted advisers versus what analysis and models might tell them. 56   Data Collection in UGS Demands a Sustained Commitment It is extremely challenging for the NSE to obtain information that helps it understand dynamics on the ground. One interviewee emphasized that the NSE consistently struggles with sociocultural intelligence and the "softer, human side" of intelligence collection versus the harder military information that the Intelligence Community (IC) is primed to collect. 57  Questions of human motivation and behavior require context to interpret, which can be difficult to obtain without sustained commitment and presence. Such commitments to data collection have been difficult to sustain precisely because the United States has tended to withdraw those individuals best suited to gather necessary information when the security situation was deteriorating. Interviewees noted that a feedback loop has existed between security and intelligence-one where good security was needed to secure intelligence collectors and assets and good intelligence collection enabled effective security. 58 Declines in one can be seen as jeopardizing the other.
In an example of this dynamic, one interviewee expressed how difficult it was to vet armed nonstate groups in Syria to provide them with U.S. assistance, noting the inherent challenges of human intelligence collection in dangerous environments with few U.S. personnel: influence in Mexico, which required collecting data on previous formal and informal relationships, family ties, and historical commitments and rivalries. 60  In addition to collecting information, intelligence collectors must develop capabilities to vet information to ensure that it is truthful and accurate. Given that vetting largely rests on triangulation (comparing one data source with another), it requires multiple data sources to increase the quantity and quality of data. 61 As a result, the time and cost of data collection are raised and made more difficult given the challenge of sustaining resource commitments in UGS.
Interviewees emphasized that there are few shortcuts when it comes to gathering and processing the type of data that is needed for analyzing UGS. One interviewee noted that gathering highly qualitative, human-centric information in UGS requires high levels of trust and repeated engagement. They noted that sustained relationships diminish incentives to lie because sources realize that future gains can be more valuable than a single payday and that high-quality information is valued by policymakers and motivates a continued demand for its collection. 62 This type of collection requires resources to travel, to hire observers and listeners with the appropriate language and cultural skills, to pay collectors on the ground, and to use fixers to secure meetings and provide security. 63 As one interviewee noted, the data are challenging and time-consuming to collect and must be constantly and manually updated. This means that highly qualitative, human-centric data on UGS can be difficult to produce at scale.
Another interviewee noted the problem that knowledge in UGS is likely to be contested. Well-governed spaces might be able to produce authoritative, official data, such as a population census, tax and health records, or economic employment statistics. By contrast, the fragmented nature of UGS means that records of this type might be difficult to produce or unreliable and challenged if they are produced: "Producing numbers requires authority and legitimacy. In undergoverned spaces, which can lack both authority and legitimacy, numbers are going to be contested and you might not get relevant information." 64   Understanding Human Dynamics in UGS Is Highly Valued by Policymakers but Difficult to Achieve Understanding UGS involves synthesizing large amounts of qualitative, ethnographic, human-centric data to explain both how social systems work in these settings and the importance of specific actions in UGS. Interviewees noted the importance of nuanced, 60 RAND Interview C7A1, December 2020. 61 RAND Interview C3C1, December 2020. 62 RAND Interview C3C1, December 2020. 63 RAND Interview C3C1, December 2020. highly localized and country-and issue-specific expertise that requires experience and specialization to address. Importantly, the demand for depth and expertise to develop nuanced and case-specific assessments and engagements is challenged by senior policymakers' limited ability to commit time and attention to analysis, thus creating an inherent dilemma about analysis and its communication. Despite policymakers' desire for sophisticated analysis, many interviewees remarked that the U.S. government often struggles to develop-and then use-expert-driven, methodologically rigorous analysis for the reasons noted earlier.
Policymakers often lack depth and expertise in UGS, thus limiting their baseline understanding of problems. One interviewee referenced the Rumsfeld square-"known knowns, known unknowns, unknown knowns, and unknown unknowns"-to illustrate the complexity of developing decision-relevant information and matching analyses to needs. 65 UGS have high levels of "unknown unknowns," thus frustrating analysts' attempts to build accurate models of the world. Complex problems present many elements that might or might not be relevant, which makes it difficult to scope or abstract models that capture the most-salient features of the situation. 66 Policymakers enter into UGS unsure of the nature of the problems they confront and thus might not be able to ask the right questions to guide research and analysis or place what knowledge they have in a larger strategic context.
Interpreting nuanced and highly localized information requires deep expertise. Interviewees noted that policymakers often lacked confidence that they, their organizations, and the broader NSE had the necessary expertise for solving policy problems in UGS. There was a sense that the community's "bench" on highly specialized issues was not sufficiently deep and that the lack of depth meant that analysis and advice were potentially biased. 67 One interviewee noted that, too often, the right people and expertise were not involved in the policymaking processes. They also observed that the government could improve its ability to reach outside traditional sources of expertise to involve diverse perspectives. 68 There are several government programs working to solve this outreach problem. For example, one interviewee argued that more could be done to exploit open-source intelligence collection networks on an enduring basis to bring critical information on UGS to policymakers, which would make engagement more effective. 69 They discussed IC outreach efforts, which would better integrate open-source intelligence from sources outside the IC, such as academia, nongovernmental organizations (NGOs), and industry. 70   65 RAND Interview E7B1, November 2020. 66 RAND Interview E1D6, August 2020. 67 RAND Interview D6B0, December 2020. 68 RAND Interview A5E7, December 2020. 69 RAND Interview C3C1, December 2020.
Additionally, interviewees noted how issues and terminology within UGS are contested, leading to confusion over what is relevant for the problem at hand. For example, one interviewee noted how difficult it was to achieve agreement on a shared lexicon among policymakers: DoD uses the term irregular warfare, and most of the community thinks this just means counterterrorism, but it also includes counter-state. Gray zone is used in the academic literature. Competition is a catch-all term that has become meaningless. 71   Another analyst noted that in technical domains, such as space, the complexity of operations has proliferated as competitors have adapted to one another's capabilities and operations, introducing issues of organizational behavior, psychology, and strategic culture that have traditionally been excluded or only marginally considered in highly specialized studies. 72 Thus, even when the physics of the environment was known, the human and organizational dimensions of competition became the dominant sources of uncertainty and complexity.
Finally, because analytical findings about UGS tend to be contextual, contingent, and qualitative, they are rarely quick to consume. For example, several interviewees pointed to the issue of information overload. After a certain point, excess information becomes a source of noise for policymakers. The ability to sift through data to find the correct information and make it both useful and digestible for policymakers gets more difficult as the amount of data increases. 73 While the solution to this issue might be to limit collection to relevant data, the definition of relevance itself shifts with policy priorities that depend on time and stakeholders. One interviewee noted that it was often rare that they had definitive evidence to make long-term policy decisions. 74 In the context of long-term competition, analysts do not know what questions policymakers will have, what information will be needed to answer them, and how to efficiently communicate insights when needed. "Standing out from [the] noise" is a key challenge. 75   Analysis of the Effectiveness of Engagement in UGS Is Ambiguous, Because Outcome Metrics Are Difficult to Define Several interviewees noted that it was difficult to support policymakers when policy objectives were uncertain and shifting and when there was limited or no explicit mapping between actions and outcomes. They noted the importance of developing metrics for evaluating policy outcomes and rationalizing actions within a causal logic that could indicate whether actions 71 RAND Interview B6D1, September 2020. 72 RAND Interview A5A8, November 2020. 73 RAND Interview C4D1, October 2020. 74 RAND Interview E2D8, November 2020.
were producing expected and desired outcomes. 76 Metrics, in theory, enable policymakers to understand how well they are doing and what they can be doing better. 77 One interviewee noted that the problem of ambiguous outcomes was acute on such issues as cyber deterrence, where the objective of preventing conflict led to the belief that deterrence was failing whenever breaches and hacks were discovered, while a broader, cross-domain view suggested that the limiting of conflict to the cyber domain was a sign of successful deterrence and the management of conflict at the geopolitical level. 78  While the importance of metrics was emphasized, the fact that they rely on developing causal models of the system in question was seen a challenge for UGS. Suitable metrics require a clear understanding of a causal pathway, the development of certain measures to determine adherence to the identified causal pathway, and the data to populate the measures-all of which are difficult to identify, derive, and collect in UGS. Many interviewees expressed doubt as to the utility of these metrics in UGS. As one interviewee noted, "[p]olicymakers have no ability to understand how their investments yield strategic results-spending a dollar on Estonia is simply assumed to be a dollar spent deterring Russia." 79 Another interviewee noted how difficult it was to measure nebulous concepts in the international system. For example, despite years of experience in the space domain, it was still unclear what outcome metrics would demonstrate the policy choices that would lead to resilient space capabilities. 80  A related but distinct issue has to do with the difficulty of measuring a particular strategy's effectiveness when the outcome of the decision cannot be observed. As one interviewee noted, "How do I develop a measure that shows that I reduced strategic surprise? Outcomes that are truly hard to measure present their own set of challenges." 81 These perspectives point to a similar conclusion about using outcome metrics in UGS: Policymakers have little way of knowing whether outcomes happen as a result of actions they have taken. More fundamentally, though, policymakers do not know whether the metrics measure outcomes that are in their long-term interests or what they believe their interests are in the moment. 82   
Interviewees we spoke with repeatedly emphasized the idea that "exploration" was critical to organizational and policy adaptation. A policymaking process needs to be designed so that it can maintain the search for new models, frames, and assessment criteria to improve 76 RAND Interview D4A2, November 2020; RAND Interview D7A7, November 2020. 77 RAND Interview D0E6, November 2020. 78 RAND Interview C2C2, January 2021. 79 RAND Interview B6D1, September 2020. 80 RAND Interview A5A8, November 2020. 81 RAND Interview D4A2, November 2020. and adapt, while at the same time building in pathways for new information to enter future decisions. Interviewees noted that the biggest uncertainty for policymakers is often the definition of the problem itself. Policymakers' initial framing of the problem is almost guaranteed to be wrong, but organizational decisionmaking seeks solutions and action quickly. Thus, the challenge is to find the fastest way to get to better formulations and models. Tools and approaches that can help speed up the process through which policymakers explore and work through alternative formulations are valuable. 83 One interviewee summed up the need for exploration succinctly:
If we agree that the system is legitimately complex, the likelihood that we are framing the problem correctly is low. Anything that can be done to scale insights and the speed of those insights-from open-source development, to rapid multiple competing framings, to teaming-will be a critical area of investment. 84   Throughout our interviews, one potentially helpful method in this area that was repeatedly mentioned was gaming. Many of the interviewees argued that gaming provides a catalyst for analyzing difficult issues and understanding how various stakeholders will react to challenges in UGS. 85 Games can also highlight where and how it would be strategic to act in UGS. 86 Gaming at the senior leader level was identified as being particularly useful, because games can reveal how DoD leaders unconsciously frame problems. A challenge of gaming at senior levels is the resistance that some leaders have to letting games show them things they do not know. Many interviewees had suggestions for how to make gaming more tailored to UGS. For example, because UGS have constant changes in the political and military atmosphere, one interviewee suggested that games be used less for optimization and more for "robust alternative discovery." 87  The importance of generating a diverse array of hypotheses throughout the policymaking process was another key theme. For example, when asked how they would characterize the role of technology in understanding long-term adaptation and competition, one interviewee replied: "Rapid access to a diversity of hypotheses." 88 Another interviewee noted that generating hypotheses "in a way that is different but not random" was a key area of analytical and decision support for UGS because having a principled way to explore high- 83 RAND Interview D4C1, December 2020. 84 RAND Interview D4C1, December 2020. 85 RAND Interview D3A4, November 2020; RAND Interview D1A1, October 2020; RAND Interview G9R7, September 2020; RAND Interview B6D1, September 2020. 86 RAND Interview B6D1, September 2020. 87 RAND Interview D1A1, October 2020. dimensional spaces is needed to move in productive ways that are more likely to help policymakers improve and learn. 89  One important caveat that interviewees provided is that tools for exploration that might help organizations be more adaptive might compete with other organizational resources. As one interviewee noted, "There is no such thing as free lunch. Every dollar spent trying to implement a model is a dollar less for [the organization's mission]." 90  Many interviewees noted that analysis is valued when it helps policymakers understand uncertainties and identify risks. Analytical paradigms and tools that emphasize robustness and discovery rather than optimization are most useful. 91 Analysis, therefore, should be seen as evolutionary, or something that changes over time. For example, analysis should not invest too much in details early in the process when uncertainty is high and the features of the problem are unknown. Analytical models, data, and frames should change with time; errors result from pushing these tools beyond their capabilities. 92  The role of models in supporting analysis should be regarded as variable and dynamic. One interviewee noted that early in an engagement on a new problem, when little is known, analysis should be viewed as exploratory, broad, and unable to support detailed or longterm planning. The effort should be on getting a broad understanding of goals, information requirements, and options for actions. Together, these insights provide organizations with a capability for learning that enables models and interventions to become increasingly tailored, eventually allowing modeling and analysis to sit atop a stronger foundation of knowledge, expertise, and experience. One interviewee discussed this learning process in the context of the coronavirus disease 2019  pandemic by noting that early in the pandemic, little was known about the virus's spread and treatment. Making long-term policy was not possible, and efforts to apply models that were available at the time using the accessible data would not have provided a credible basis for long-term policy. Under those conditions, the best strategy was to start with broad policies updated frequently and then make increasingly differentiated policies based on local conditions as models and data matured. 93  Finally, as policymakers use models to develop and search for solutions to complex problems-design challenges-testing each model-derived and -generated solution becomes infeasible. Instead, interviewees noted that experimental resources should be employed to validate the models at multiple points around the design space so that policymakers have a better understanding of the reliability and valid use of model-generated solutions. 94   89 RAND Interview E7B1, December 2020; RAND Interview D7A7, November 2020. 90 RAND Interview D4A2, November 2020. 91 RAND Interview D3A8, October 2020. 92 RAND Interview E2D8, November 2020. 93 RAND Interview E2D8, November 2020.
Policymakers and technologists agreed on the importance of information presentation in moving policymakers toward collective action. Visualization and simplicity are keys to communication because they provide ways to reduce complex information so that it can be consumed and applied. 95 Interviewees emphasized that information should be made visual and should tell a story to be most effective. One interviewee noted, Visual analytics are crucial for communication. How can you put together ideas in a cognitively appealing way that would make a principal want to take credit and put them in [the] deck? 96   This interviewee noted the importance of minimizing the use of numbers: "Do not present numbers-present stories. Presenting a number outside what it really means is focusing on the wrong thing for decisionmakers." 97  Another interviewee similarly noted that visualization is a powerful tool to help policymakers gain insight into the complexity of a situation or space. This official described how the IC had made inroads into using data visualization during the campaign to counter the Islamic State of Iraq and Syria (ISIS):
We kept getting the same questions, so we tried to create a visualization tool that was available to policymakers in real time. . . . We visualized Syrian opposition groups, their location on the ideological spectrum, and their effectiveness. We also visualized in real time areas of ISIS control. This was a powerful tool to show policymakers not just what was happening, but why. 98   Priority Investments for Engaging in UGS Throughout the interview process, participants identified areas of investment that could provide high-impact capabilities for supporting engagement in UGS and facilitating increases in the adaptive capabilities of DoD and the NSE. It is important to note that there are likely some UGS that will always lie outside policymakers' control, regardless of investments in social science, models, technology, and better bureaucratic processes, because of structural factors, such as geography or lack of attention and resources. Nevertheless, these structural constraints might not be absolute. Thus, this final section highlights prescriptions for broad areas of investment in the domains of bureaucratic practice, data, and analysis. 95 RAND Interview C7C9, January 2021. 96 RAND Interview D4C1, December 2020. 97 RAND Interview D4C1, December 2020.
Explore Organizational Incentives and Practices to Increase Investments and Rigor in Research and Development DoD could be a model for both experimenting and promoting new research and development practices. As the federal government's largest spender, DoD is in a unique position to set practices for the rest of the government and incentivize different behaviors. Through adjustments in its own contracting and bureaucratic practices, DoD could play a role in driving how companies think, invest, and spend their money in ways that increase innovation and national competitiveness. 99   
Investments of this type can involve 
(1)
100
Investments that bring together stakeholders working on UGS from inside and outside the U.S. government, including academia and NGOs, could help the government better understand and adapt its policies in UGS. These engagements can consist of such settings as Track II negotiations and more-analytical settings, such as games and workshops.
Tools for fusing information across security levels might help to increase collaborative decisionmaking by mitigating barriers caused by the level of secrecy and compartmentalization associated with some UGS areas, such as space, cyber, or special operations. Algorithms could look for common entities or data values in different agencies' systems at different levels and then alert analysts when a match is found. Automating data discovery and characterization in this way, when combined with appropriate inter-agency data governance capabilities, could help create a structure where machines can share information in ways that humans 99 RAND Interview D1E1, November 2020. cannot, thus preserving organizational practices but enabling the sharing of information when inter-agency coordination is needed.
Invest in Tools for Exploring Large Decision Spaces Tools and approaches that can help to speed up the process through which policymakers explore and work through alternative formulations of policy problems are valuable. These tools should promote diversity of thinking and speed up the process of cycling through hypotheses. Tools that can scale insights and the speed of those insights-from open-source development to rapid, multiple, competing framings to teaming-are a critical area of investment.
Models that can help leaders understand the long-term implications of their decisions are another area of investment. 102 After the initial exploratory effort, empirical data collection and testing (including specialized data gathering and historical back testing) enable models and interventions to become increasingly tailored, eventually allowing modeling and analysis to sit atop a stronger foundation of knowledge, expertise, and experience; this foundation aids leaders as they consider long-term consequences. 103 To understand different potential implications and results of stakeholder choices, decision trees could be particularly useful. 104  Real options, which help policymakers keep options open to allow for future decisions by placing a quantitative value on the benefits of maintaining and keeping multiple designs, might also help. 105   
Models that accurately portray the volatility of the human dynamics of UGS are also a critical area of investment. When predicting certain groups or actors' reactions to U.S. policy decisions and actions, it is important to understand how the people that occupy UGS will engage or react. The nuanced views of human behavior, interactions, and relationships can affect how these groups of interest will react to policymaker decisions in UGS. 106 One way to develop this understanding is to leverage data from social media, which presents large sets of data that are rich for analysis. 107 102 RAND Interview D3A4, November 2020. 
Aligning models with the tools and capabilities that policymakers have is important. While it is common practice to match models to the system being represented, a critical feature of achieving policy-relevant analysis is to also ensure that simulated interventions could be mapped to real-world constraints on policymakers. 108 These constraints might involve both limits on resources (such as time, information, budgets, and expertise) and organizational factors (such as authorities and coordination processes). Absent these considerations, models might identify theoretically interesting but impractical, immoral, and even illegal strategies. 109 While research investments have been made in model validation strategies for simulating complex systems (e.g., financial markets), investments in capabilities that can search across the space of viable interventions in ways that are both computationally efficient and organizationally and operationally plausible might be worth pursuing.
Interviewees identified the importance of tools that could aid policymaking in conditions with complicated levels of uncertainty and risk. Among the specific tools discussed were Robust Decision Making tools to identify strategies for regret minimization and real options for identifying the value of preserving flexibility and delaying choices that will lock out future flexibility.
Interviewees noted that computational agents play a significant role in modeling the behavior of complex systems, most visibly in ABM. While the policymaking architectures of software agents have advanced, the overwhelming majority of those used in advanced modeling and simulation applications remain grounded in probabilistic logic and the Kolmogorov axioms of probability. 110 While these models allow for internal mathematical validity, it is difficult to align them with real-world deviations from rationality that are both experimentally observed and important to many theories of social behavior. Investments in formal, computationally efficient policymaking architectures for individual and collective behavior might enable new approaches to modeling the social behaviors of actors within UGS and assist in the discovery and assessment of alternative engagement approaches that rely on more-realistic treatments of information consumption and social interaction.
108 RAND Interview D3A4, November 2020.
109 RAND Interview B4D0, July 2020.
Discussions with interviewees summarized in this chapter reveal several important and challenging insights about decisionmaking and action within UGS. First, while there is no clear, emergent definition as to what an undergoverned space is and, therefore, no singular way to assert which national interests are put at risk by the presence of UGS, interviewees repeatedly identified approaches that they regarded as necessary for successful engagement. Clear examples of potentially beneficial investments to help guide engagement emerged from our interviews, but no singular method can be applied to all UGS.
Second, the United States needs to clearly understand its own interests and willingness to commit time, attention, and resources-both military and nonmilitary-to engage in UGS.
Third, policymakers should be realistic in their assessments about the structure of the situation and the opportunities to change or otherwise live with circumstances that might be less than ideal. Prior positions of power, status, and influence might evoke desires or reverse unwanted trends, but policymakers must be focused on future possibilities and not anchored on the past.
Fourth, analytical needs are varied and feature two opposing requirements. One requirement is deep expertise, nuance, and attention to the details of specific circumstances, often necessitating long time lines to develop. The other requirement is to produce information that nonexpert policymakers can consume with limited time and attention, often under crisis conditions. The result is a trade space with two poles: On one end is sophisticated analysis that incorporates a broad variety of qualitative and quantitative information and expertise to expose the dynamics of systems and their responses to interventions. On the other is breadth-first analysis that quickly identifies risks and opportunities, allowing policymakers to manage complex challenges by informing their choices at the speed of relevance.
Finally, within this mix, analysis of all types must consider the likelihood of the information being consumed by multiple stakeholders engaged in organizational and bureaucratic processes. Without attention to the circumstances within which policymakers reside, the most sophisticated information collection and analysis-the Sense stage of the ASDA cycle-cannot connect to the Decide stage, thus leaving the final and necessary Adapt stage beyond reach.
We interviewed the following people: In your view, how do problems of long-term competition and undergoverned spaces present decisionmakers and organizations with distinct challenges when compared with preparing for and executing kinetic military operations? 3. In your experience, in what contexts do decisionmakers primarily rely on their instincts and expertise versus more analytical processes? When do they, and when should they, seek to challenge their beliefs? 4. In your experience, how do policymakers' expectations about analysis differ under different circumstances? How do analytic products and processes assist, or hinder, the ability of organizations and stakeholders to reach a shared perspective or better understand sources of their disagreements regarding situational assessments, risks, actions, and outcomes? 5. What analytical approaches or tools might help policymakers develop adaptive policies for long-term competition? What could help policymakers be more flexible in their decisions? 6. Are there particular analytic tools and/or processes that were helpful to you in the past? What types of decisions did you use it for? Are there any that have been unhelpful? If so, how? What is the bar for being useful? What is it about the problems, or the technology, that lead you to think that? 7. What analysis and information would make it easier for decisionmakers to reconsider their choices and commitments? What increases their confidence that they are making wise decisions? 8. Oftentimes, analysis can be overlooked or difficult to sort through due to the volume of information available. What format or context would be most helpful in presenting/ communicating information to policymakers so that it can be easier and simpler to understand? 9. What are the major pros and cons of policymakers using their instincts to make decisions versus making a "data driven decision"? 10. Is there anything we should have asked you that we didn't?
Interview Protocol for Technologists 11. How might technologies assist decisionmakers and organizations to understand longterm competition and be adaptive to changing and unforeseen circumstances? 12. Is there a difference in technologies that would assist in the discovery of preferences and goals, vs. those that help optimize the allocation of resources? The concept of undergoverned spaces (UGS) and the motivations for intervening in them have been discussed in previous chapters. This chapter examines new frameworks for engaging in UGS. For many in the U.S. Department of Defense (DoD) and broader National Security Enterprise (NSE), the joint phasing construct (JPC) has provided a logical point of departure for thinking about engaging in a wide variety of UGS. However, the JPC has proven to be problematic as an approach to planning and engagement in UGS. This chapter discusses alternative approaches for conceptualizing engagements in UGS and discussing the concepts of infinite games, the Act-Sense-Decide-Adapt (ASDA) cycle of adaptive campaigning, problem-centric governance, adaptive governance, and alternative modalities of governance and exchange. Although these do not represent an exhaustive set of concepts for engaging in UGS, they illuminate features of what effective approaches might look like.
The protracted conflicts in Afghanistan and Iraq focused attention on the need to understand conflict in its many dimensions and view warfare as a long-term process. 
1
2
6
In recent years, the JPC has been scrutinized for several reasons, and alternatives for thinking about the operational environment are emerging. New frameworks that emphasize continuous, fluid, and nonuniform movement among conflict, competition, and cooperation have developed as alternatives to guide military planning, operations, and NSE activities  more broadly. This section discusses how perspectives on the JPC have evolved, while subsequent sections in this chapter present several alternatives that emphasize continuous learning through engagement. 3   Evolving Views on the JPC From its inception, the JPC faced criticism. Although it situated military planning in a longer time line of competition that involved noncombat operations and broader national engagement, critics argued that it militarized conflict between states by placing the military at the center of planning and interventions that were best performed by nonmilitary organizations. In addition, critics observed that the JPC divided engagements into discrete phases in which different members of the NSE might have more or less prominent roles. 4  As time passed, additional concerns emerged over the JPC's utility as a framework for organizing military operations because complex, real-world engagements did not move among conflict phases in a linear, stepwise fashion. 5 Moreover, although the JPC broadened temporal dimensions of military planning, it did not automatically encourage the commitment of resources, energy, and imagination to all the activities that could be performed across each phase. Detractors argued that Phase 0 shaping actions were simultaneously and contradictorily viewed as both the responsibility of non-DoD agencies (to prevent escalation) and an opportunity for the military to take steps to ensure advantages in later phases because of the belief that open, violent conflict was the inevitable, natural state of the international system. Exercises, experiments, and scenarios showed that military operators viewed the early phases as a "race to Phase III" in pursuit of the opportunity to decisively gain control over the conflict. 6  These limitations became especially acute as Russia and China each developed broad capabilities and the will to advance their interests in the conceptual space between Phase 0 and Phase I, in which aggressive actions were subtle, diffuse, and below the thresholds that would prompt a military response and meet criteria for armed attack according to international law, treaties, and plans. 7 As Antullio Echevarria has noted:
Moscow and Beijing have exploited the West's conception of, and long-standing aversion to, armed conflict to accomplish what some Pentagon observers describe as "wartimelike" objectives. Thus far, these objectives have remained outside the scope of what mili-tary strategists and campaign planners are legally authorized or perhaps professionally trained to address. 8   He argued that a gap exists between the phases that make these approaches to conflict both conceptually interesting and organizationally challenging, as illustrated in Figure 
6
Contemporary planning concepts are seeking to redress the notion of conflict progressing through ordered phases and the prospects that aggressors might seek to avoid crossing phase boundaries. Doing so requires a more nuanced view of the international system where actors, both states and nonstates alike, simultaneously engage along a continuum of cooperation, competition, and conflict. 9 Alternatives to the JPC seek to increase the sensitivity of policymakers, military planners, and operators to the strategic realities that many competitors desire to advance their interests through ways that are beneath the thresholds of open and direct conflict. Terms for these challenges are varied, nuanced, and contested because of the history and context of their use-they include gray-zone conflict, hybrid 8 Echevarria, 2016, pp. 12-13. 
Chinese coercion ? warfare, unrestricted warfare, irregular warfare, nonlinear warfare, virtual societal warfare, and next-generation warfare. 10   Finite Versus Infinite Games: Open-Ended Engagement One way of rethinking the JPC's cyclical model of conflict is to emphasize the long-term, openended nature of strategic interaction-cooperation, collaboration, coordination, contestation, competition, and conflict. Game theorists have noted that repeated interactions-iterated games-among players can create new motivations for actors to cooperate with one another. 11  James Carse elaborated on this logic by examining the differences between what he termed finite and infinite games, which explored how the idea of long-term contests and interaction required a different mode of thinking than finite games in which victory could be achieved. 12  Carse differentiated between finite and infinite games by noting that each proceeded from a different foundation that motivated players' choices and actions. Finite games are entered into voluntarily, because players cannot be compelled to play even though they might believe that they need to play. Finite games are bounded by time, space, and rules regarding what is permitted and prohibited. Finite games have agreed-on systems for scoring and allow players to be ranked and ordered in terms of their performance against one another; thus, there exist unambiguous conditions for terminating the game and accepting its outcome. As Carse noted, 10 Williamson Murray and Peter R. 
Mansoor, eds., Hybrid Warfare: Fighting Complex
'Gerasimov Doctrine' and Russian Non-Linear War," In Moscow's Shadows, blog post, July 6, 2014;
Peter Pomerantsev, "How Putin Is Reinventing Warfare," Foreign Policy, May 5, 2014;
Adam Elkus, "50
December 15, 2015;
Echevarria, 2016;
Paul Scharre, "
March 22, 2017
, Santa Monica, Calif.: RAND Corporation, CT-486, 2017;
Mark Galeotti, "I'm
March 5, 2018;
Mark Galeotti, "The Mythical 'Gerasimov Doctrine' and
Carse, Finite and Infinite Games, New York: The Free Press, 1986, pp. 1-31;
Simon Sinek, The Infinite Game, New York: Portfolio, 2019.
By contrast, infinite games are also entered into voluntarily, but they are unbounded because players are free to change the time, the space, and the rules of play as they wish. As a result, players cannot determine when the game begins, when it ends, or how it is scored. Because the game is open-participants, times, locations, rules, and ways of keeping score might change-victory conditions cannot be known, nor can the ranking of the players be made in an unambiguous fashion. As a result, infinite games are not played in the pursuit of victory; rather, they are played for the purpose of continuing to play. Or, as Carse stated, "A finite game is played for the purpose of winning, an infinite game for the purpose of continuing the play." 14  Finite games might be played within infinite games. Although infinite games might be unbounded and open, players can agree to conduct themselves according to rules that dictate interactions among them. Thus, players engaged in long-term, open-ended competition might nevertheless create limited, bounded, and ultimately managed contests among them, yet the results of finite games cannot settle the larger infinite game within which they occur. As Carse concluded, Finite games can be played within an infinite game, but an infinite game cannot be played within a finite game.
Infinite players regard their wins and losses in whatever finite games they play as but moments in continuing play. 15   The differences between finite and infinite games are profound. Surprise, death, and power-three of the most-consequential elements of gameplay-are discussed next.
Finite and infinite games each locate the sources of surprise in different places. In finite games, surprise occurs as a result of one player not being fully aware of what actions are allowable under the game's rules. 16 Thus, being surprised within a finite game reveals a lack 13 Carse, 1986, p. 3.   14 Carse, 1986, p. 3.   15 Carse, 1986, p. 7.   16 It might be argued that surprise in a finite game could result from one player cheating, violating the agreed-on rules of the game. However, such a circumstance, in which one player abides by the agreed-on rules of the game while the other does not, more closely aligns with the playing of an infinite game, in which one player has altered the rules. of mastery over the rules and the permitted actions. Players demonstrate their expertise by knowing what actions are possible, anticipating their use, and deterring or countering their opponent's moves. In doing so, players use their past knowledge to shape the future:
It is the desire of all finite players to be Master Players, to be so perfectly skilled in their play that nothing can surprise them, so perfectly trained that every move in the game is foreseen at the beginning. . . . A finite player is trained not only to anticipate every future possibility, but to control the future. 17   By comparison, because infinite games are open and malleable, surprise does not occur as a result of unfamiliarity with the rules but rather because of unfamiliarity with the other player and the variety of actions they might perform. However, because infinite games are open and subject to change, players expect to be surprised. Put another way, in finite games, strategies emerge as a result of the game's structure, while in infinite games, the games themselves arise from the strategies of the players. 18 Therefore, being surprised is not seen as a lack of skill; rather, skill is expressed in the ability to adapt and change depending on what others do:
Because infinite players prepare themselves to be surprised by the future, they play in complete openness. It is not an openness as in candor, but an openness as in vulnerability. It is not a matter of exposing one's unchanging identity, the true self that has always been, but a way of exposing one's ceaseless growth, the dynamic self that has yet to be. 19   Different perspectives on surprise shift how players should examine their understanding of the game and competition. If surprise is a matter of failing to understand the system and the legality of possible moves and their countermoves, then players might find it fruitful to commit analytic resources to exploring possibilities within the space of interactions constrained by the game's rules. Although such games as Chess, Go, and Starcraft have massive state spaces, they remain closed conceptually, even though realism limits the extent to which human and machine computing resources can exhaustively search the space and identify an optimal strategy. 20 By comparison, if the source of surprise is found in the motivated reason- 17 Carse, 1986, pp. 17-18.   18 These differences manifest in contemporary research methods. Game theory seeks to discover optimal strategies for players given a structure of allowable moves and payoffs. Agent-based models seek to discover what kinds of interactions or moves result when agents with specified strategies interact. Although contemporary models often blend these approaches, the core differences between searching for what strategies emerge given a game structure and observing what interactions emerge given a set of strategies reveal fundamentally different research motives. 19 Carse, 1986, p. 18.   20 This issue is addressed in Chapter Seventeen of this report (Justin Grana, "Difficulties in Analyzing Strategic Interaction: Quantifying Complexity," in Aaron B. Frank and Elizabeth M. 
Bartels, eds
In finite games, a player's death or removal as a competitor ends their ability to win the game. Victory is often achieved by a terminal move that renders the opposing player unable to compete any longer: "A terminal move results in the death of the opposing player as player. The winner kills the opponent. The loser is dead in the sense of being incapable of further play." 22  Under such conditions, death is synonymous with defeat.
In infinite games, a different circumstance arises in which death, or the inability to compete further, is an achievement if it enables the game to continue. Carse regarded this as "life in death," resulting when the deaths of some players allowed others to continue the game. 23  Using warfare as an example of an infinite game, he noted that soldiers achieved immortality by sacrificing themselves to allow others to continue to fight: 24   Soldiers commonly achieve a life in death. Soldiers fight not to stay alive but to save the nation. Those who do fight only to protect themselves are, in fact, considered guilty of the highest military crimes. Soldiers who die fighting the enemy, however, receive the nation's highest reward: They are declared unforgettable. Even unknown soldiers are memorialized-though their names have been lost, their titles will not be. 25   The importance of death, or the removal from the game, carries different meanings in finite and infinite games. In finite games, being killed or removed from the game is framed as a loss or lack of success. Alternatively, in infinite games, death signals not weakness but strength-a costly sacrifice demonstrating commitment and investment in competitiveness.
Another important difference between finite and infinite games concerns how power manifests. Carse argued that conceptions of power in finite games revolve around the ability to compel others to do as directed (i.e., to act in ways that they would otherwise not). This accords with traditional definitions of power employed in international relations, such as conceptions of military deterrence (to prevent others from acting), compellence or coercion (to make others take an action), and economic or institutional leverage (to adopt practices to participate in markets or governance). 26  Carse believed that in finite games, measurements of power are historical-they are based on demonstrations of what the player has already done. Therefore, power is attained or achieved. As Carse concluded, To speak meaningfully of a person's power is to speak of what that person has already completed in one or another closed field. To see power is to look backward in time.
Inasmuch as power is determined by the outcome of a game, one does not win by being powerful; one wins to be powerful. If one has sufficient power to win before the game has begun, what follows is not a game at all. 27   Because power is historical and exists in the context of an established game, it has no meaning in an infinite game played in a nonstationary environment. Instead, Carse argued that the concept of strength is more appropriate. Strength, according to Carse, is defined as what a player can allow others to do. Strength defines the boundaries within which players believe that they have the capacity and adaptability to cope. As Carse summarized, Strength is paradoxical. I am not strong because I can force others to do what I wish as a result of my play with them, but because I can allow them to do what they wish in the course of my play with them. 28   In contemporary terms, strength might be regarded as robustness or resiliencerobustness in that a player might be insensitive to a large number of actions others might take, and resilience in that a player might be able to adapt to what others have done.
The differences between power and strength might be evident in the construction and persistence of institutions of international governance. Although the development and balance of power has been a central concern of international relations theory and practice, mat-ters of strength have been evident-though not explicitly identified-in the terms put forward by Carse. For example, the development of order after great-power conflict has been seen as the moment at which victorious great powers have the opportunity to cement privileged positions in the international system through such institutions as military alliances, trading rules, and international law. Yet the international order that has endured for more than seven decades was based on decisions made by the United States in the aftermath of World War II to voluntarily bind itself to rules that constrained its use of power and created avenues for others to assert their national interests. 
29
30
The North Vietnamese colonel pondered this remark a moment. "That may be so," he replied, "but it is also irrelevant." 
31
33
One model for managing competition within UGS is the ASDA cycle. Initially developed by the Australian Army, the ASDA cycle provides a complex adaptive systems (CAS) approach to operational design and adaptive campaigning. 40 The ASDA cycle is particularly important 38 Chad Buckel, "A New Look at Operational Art: How We View War Dictates How We Fight It," Joint Forces Quarterly, No. 100, January 2021, pp. 94-95. 39 Frank G. Hoffman, "The Missing Element in Crafting National Strategy: A Theory of Success," Joint Forces Quarterly, No. 97, April 2020. 40 Head Modernisation and Strategic Planning-Army, 2009; Huba Wass de Czege, "Systemic Operational Design: Learning and Adapting in Complex Missions," Military Review, February 2009; Justin Kelly and Mike Brennan, "OODA Versus ASDA: Metaphors at War," Australian Army 
Journal, Vol. 6, No. 3, Summer 2009.
At the strategic level, the ASDA cycle was introduced as a prescriptive step toward meeting the Australian Army's needs to operate in environments that did not resemble high-intensity conflict between similarly structured military forces. Addressing debates on the appropriate orientation of military forces-and echoing the same dilemmas that have occurred within the United States-Lieutenant General D. L. Morrison, the Chief of Army, noted, Unlike some, who continue to suggest that our deployment of forces to East Timor, the Solomon Islands, Iraq and Afghanistan have been an aberration, I am convinced they are symptomatic of the changing character of war. 41   Thus, the ASDA cycle is motivated to manage decisionmaking processes in environments affected by the interplay of state and nonstate actors, all competing to influence the allegiances and behaviors of individuals, groups, and societies while operating at and below thresholds of conflict. 42 The result is to successfully influence and shape the overall environment to facilitate peaceful discourse and stabilise the situation, noting that there may be no end state to an operation but rather an enduring set of conditions conducive to Australia's national interests. 43   At the operational level, the ASDA cycle emphasizes linking organizational action and learning. Given the expectation that the environment will continuously change, the ASDA cycle emphasizes five organizational and decisionmaking tenets, each representing some version of adaptive behavior on the part of military organizations, operations, and staff. These five tenets are as follows:
• Flexibility-the ability to maintain effectiveness across a range of tasks, situations and conditions within a single line of operation. For example, the structure and capability of the force can be reconfigured in different ways, to do different tasks, under different sets of conditions. • Agility-the ability to dynamically manage the balance and weight of effort across all lines of operation in time and space. • Resilience-the capacity to sustain loss, damage, and setbacks and still maintain essential levels of capability across core functions. • Responsiveness-the ability to rapidly identify, and then appropriately respond to, new threats and opportunities within a line of operation. • Robustness-the ability to achieve and sustain a critical mass of forces in relation to both population density and adversarial group capabilities, thereby achieving 41 Head Modernisation and Strategic Planning-Army, 2009, p. i. 42 Head Modernisation and Strategic Planning- 
Army, 2009, p.
sufficient control of the environment to account for Operational Uncertainty and respond across the five lines of operation. 44   The ASDA cycle differs from the more popular Observe-Orient-Decide-Act (OODA) loop, which is based on fundamental beliefs about the character of competition. Specifically, the OODA loop principally emphasizes the attainment of competitive advantage through being able to orient and decide faster than rivals. By contrast, the ASDA cycle emphasizes learning and adaptation, building competitive advantage through the ability to rapidly reframe situations according to experience gained:
The OODA loop is a model of decision-making that emphasises the importance of orientation for making sense of the observed situation, which is the basis for decision and action. . . . The Adaptation Cycle emphasises understanding a problem through experience, knowledge and planning, enhancing that understanding through interaction and explicitly drawing out the requirements to learn and adapt, individually and organisationally. 45   The depth to which the ASDA cycle emphasizes learning and change is extensive. Its prescriptive guidance reaches beyond changes in tactics and operational concepts and extends to the highest levels of strategy. In doing so, the cycle seeks to assist decisionmakers in learning not only whether the ways of competing need to change but also whether the ends being pursued should be altered. The emphasis on learning and change follows the logic of a theory of success discussed earlier, in which the causal structure of the system over which competition occurs is discovered through the process of developing causal hypotheses and their tests: 46   Modern combat can therefore be characterised as competitive learning in which all sides are constantly in a process of creating, testing, and refining hypotheses about the nature of the reality of which they are a part. The resulting adaptations might need to be extensive, extending beyond forms of tactical action to possibly encompass previously sacrosanct areas such as the force's mission. The underlying premise [is] that the original mission, objectives, and plan were based on conjecture about the enemy system's elements and internal relationships, and subsequent action will have modified the applicability of that conjecture. 47   The proposed depth of adaptation and discovery is important. Just as the shift from finite to infinite games discussed earlier changes the decisionmaking focus from the end state or conclusion of a game to an endless process of interaction and discovery, the ingrained expectation to adapt at all levels of decisionmaking accords with models of CAS and organizational 44 Head Modernisation and Strategic Planning-Army, 2009, p. 30. 45 Head Modernisation and Strategic Planning-Army, 2009, p. 31. 46 Hoffman, 2020.  behavior, which accepts that decisionmakers might be unaware of their goals and priorities until they are challenged. The garbage can model of organizational decisionmaking posits that organizations operate under conditions where (1) preferences are not universally shared by stakeholders and might even be unknown to them; (2) technology, inclusive of artifacts and processes, for connecting actions with outcomes is uncertain and often must be discovered through trial and error; and (3) stakeholder participation in decisionmaking processes is fluid, given that decisionmakers have limited time, attention, and resources to commit to problems. 48 The garbage can model has repeatedly shown how decisionmaking does not align with the ideals set forth in explanations of rational strategic action and that well-governed strategy, particularly decisionmaking conducted at the highest levels of governance from which tactical and operational choices flow, is often made to appear more deliberative and rational than it is. 49  The empirical conditions characterized by the garbage can model are well served by the ASDA cycle's commitment to learning and adaptation. Like the popular OODA loop, the ASDA cycle, shown in Figure 
6
Each step in the ASDA cycle is part of a learning process that commits organizations and individuals to challenge their beliefs, make new discoveries, and change their thinking and behavior as a result of new information. Each phase of the ASDA cycle and its role in the adaptive process is discussed next.
The act phase of the cycle begins with actions intended to stimulate the system-whether a specific adversary, population, or environment. Actions might be taken to confirm an understanding of the target system (i.e., a form of hypothesis test, referred to as discovery actions). Alternatively, actions might be driven by the belief that a cause will have an effect, which is 48 Michael D. Cohen, James G. March, and Johan P. Olsen, "A Garbage Can Model of Organizational Choice," Administrative Science Quarterly, Vol. 17, No. 1, 1972. rooted in the belief that a suitable causal model of the system has been discovered (i.e., a decisive action). 50   
The sense phase has two interrelated purposes. First, the response to the action reveals new information about the system. Such information might be regarded as providing feedback that has both qualitative and quantitative properties. Qualitative information might be the type of response produced by the stimulus and its source. Quantitative information might be the intensity of the response. Importantly, proponents of the ASDA model note that sensing itself is a learning process, where actors might need to discover how to detect, characterize, and measure feedback produced by stimulating actions. 51 50 Head Modernisation and Strategic Planning-Army, 2009, p. 33. 51 Head Modernisation and Strategic Planning-Army, 2009, pp. 33-34. 
The decide phase consists of making two types of choices. The first type is diagnostic. Diagnostic choices are determinations about the significance or consequences of sensed feedback.
The second type is prognostic. Prognostic choices are determinations about what should be done. 52   
The final phase of the cycle is adapt. This phase consist of three types of learning. First, the phase emphasizes learning how to learn, which involves communication, the sharing of lessons learned, and incentivizing risk-taking. Second, adaptation identifies specific knowledge or lessons to pursue. This shapes future action by prioritizing what discoveries should be pursued to inform the timing and direction of additional actions. Finally, adaptation challenges the entrenched understanding of the system to ensure that organizations do not grow complacent and retain outdated or flawed beliefs about the system and themselves. 53   The ASDA Cycle and Governance
When viewed holistically, the ASDA cycle is intimately related to the challenges of governance in at least two ways. The first is instrumental. Because the ASDA cycle and the larger adaptive campaigning premise begins long before the initiation of violent conflict and lasts long after its termination, issues associated with building and maintaining capable and legitimate governance play an important role in the operational and organizational toolkit. Addressing matters of governance and building the capacity to manage resources and allocate services effectively and legitimately are seen as opportunities to forestall crises and inhibit the escalation of competition into conflict. For example, the adaptive campaigning concept noted that building indigenous capacity for governance provided an opportunity to forestall the outbreak of conflict before the commitment of military resources. Indigenous capacity building involves actions to nurture the establishment of capacity within civil communities whilst simultaneously working to establish longer term governance and socio-economic capacity that meets the needs of the people. This may include; micro financial initiatives, local and central government reform-security, police, legal, financial and administrative systems. 54   The second touchpoint between the ASDA cycle and governance is abstract yet reveals a shared set of ideals on adaptive behavior and the pursuit of processes that limit the extent 52 Head Modernisation and Strategic Planning-Army, 2009, p. 34. 53 Head Modernisation and Strategic Planning-Army, 2009, pp. 34-35.  to which momentum drives decisionmaking, planning, and organizational behavior more broadly. Two particular models of governance-problem-centric governance and adaptive governance-offer complementary perspectives on how to engage with populations on complex issues. They provide guidance about how to limit the propensity for governing organizations to allow their own internal preferences and processes to overwhelm the need to adapt to the specifics of the external environment. Likewise, shifts in the nature of governance itself, specifically the transition from authoritative and market-based allocation systems toward networks of exchange, offer additional perspectives on the strategic and adaptive benefits of cooperation as a means for achieving security in the face of unpredictable threats. Together, these perspectives offer preliminary speculations about how the ASDA cycle might be aligned with contemporary perspectives on governance and adaptation.
Problem-centric governance approaches policy with the expectation that governing organizations and processes must be adapted to the features of the problems they encounter. Much like the idea of problem-centric research that seeks to tailor and develop new research methods based on the problem being investigated as opposed to seeking problems that are well suited for specified methods, problem-centric governance emphasizes the minimization of internal constraints from within organizations to maximize the use of available information and capabilities. 55 Problem-centric governance is particularly important in cases where problems are complex and involve multiple stakeholders. It is also crucial in cases where traditional coordination processes across organizational elements produce gaps and seams that limit the effectiveness of established engagement frameworks:
Problem-oriented governance is an approach to policy design and implementation that emphasizes the need for organizations to adapt their form and functioning to the nature of the public problems they seek to address. This approach is fundamentally outwardlooking in its effort to shape both long-term strategy and day-to-day working arrangements around problems as they manifest themselves. An underlying premise is that no single organization is able by itself to take on complex problems. . . . In essence, it is radically committed to prioritizing the problem-solving challenge over the comfort and convenience of preserving existing organizational practices and institutional arrangements. Learning about problems, and how they evolve over time, is at the heart of this approach. This involves challenging assumptions, developing new hypotheses, and gathering evidence to guide thinking and action. Adaptation is the logical consequence of this 55 Rudra Sil, "Problems Chasing Methods or Methods Chasing Problems? Research Communities, Constrained Pluralism, and the Role of Eclecticism," in Ian Shapiro, Rogers M. Smith, and Tarek E. Masoud, eds., Problems and Methods in the Study of Politics, Cambridge, United Kingdom: Cambridge University Press, 2004. learning: problem-oriented organizations are committed to correcting actions that fail to address the problem and double down on remedies that work. 56   Problem-centric governance requires a commitment to information collection, assessment, problem framing, and organizational reform. It emphasizes processes and resource commitments that sustain the continual search for new organizational forms and problem frames, accepting that the organization has never reached an optimal design and that artifacts from prior decisions and forms are persistent. 57 Creating fluid governance structures that can be corrected through the use of feedback rests on (1) a reflective-improvement capability that simultaneously develops and tests alternative causal models of the problem that can guide policy action and organizational oversight that holds leaders and operators accountable for their actions and commitment to learning processes; (2) a collaborative capability that emphasizes cross-silo, cross-sector, and state-society relationships and interaction; and (3) a data-analytic capability that collects, processes, analyzes, and, ultimately, learns from both formally collected and tacitly present information available to those participating in the governance process. 58  Like the ASDA cycle, problem-centric governance imagines that governing organizations have the best opportunities to achieve their goals by maintaining flexibility and openness. Such commitments diverge from governing strategies that seek to impose uniformity and regular order on the world by seeking efficiencies through the ability to regulate systems and routinize engagements. Such efforts to "see like a state" have often produced illusions of control, order, and success in the short term, only to create long-term problems and instabilities in the very systems they seek to secure. 59 Problem-centric governance challenges organizations to be more flexible, adaptive, and, ultimately, responsive to the world.
Adaptive governance complements problem-centric governance. Whereas problem-centric governance seeks to make organizations more flexible, adaptive governance seeks to make them more open. Specifically, adaptive governance was developed to correct the practices of scientific management and classical organizational theory that emerged a century ago; it was based on the premise that science could optimize organizational performance and separate decisionmaking into science-based and judgment-based decisions-the former belonging to 56 Quinton Mayne, Jorrit deJong, and Fernando Fernandez-Monge, "State Capabilities for Problem-Oriented Governance," Perspectives on Public Management and Governance, Vol. 3, No. 1, 2019, p. 34.   57  
Daniel A. Levinthal, "Adaptation on Rugged Landscapes," Management Science, Vol. 43, No. 7, 1997
Herbert A. Simon, Organizations, 2nd ed., Cambridge, Mass.: Blackwell, 1993, pp. 31-52;
Ronald D. Brunner and Toddi A. Steelman, "Beyond Scientific Management," in
Brunner and Steelman, 2005, pp. 21-24.
Scott Turner, The Extended
The second effect, however, complicated the boundary between decisionmakers making value judgments and scientists and experts dealing with objective facts. Specifically, Thomas Kuhn's Structure of Scientific Revolutions called into question the veracity of idealistic characterizations of scientific method and practice. 67 By emphasizing the actual behavior of scientists, it became clear that science was practiced by real people with real cognitive processes living in real social and organizational circumstances. Science was not practiced in a timeless vacuum, or from "a view from nowhere"; rather, science was practiced in time and space by individuals interested in the outcomes of their research. 68  The practice of science is infused with human values, rendering observations theory-laden and framed by the mental models and processes of observers. 69 The result of investigations into the boundaries between fact and values has subsequently persisted within the scientific community, calling into question the achievability and utility of objectivity, neutrality, generalizability, and other ideals-an issue that remains unresolved. 70  Scientific management, and science more broadly, encountered practical challenges associated with the limits of reductionism and analysis (i.e., the decomposition of systems into independent parts), in addition to philosophical matters. Complexity, interdependence, feed- back, and adaptation over time all revealed the limitations of explaining, predicting, and ultimately controlling systems in which these and other properties were present. The scientific challenges posed by complexity have been well documented and do not need to be reiterated at length here. 71 It is sufficient to note that the challenges presented by complexity have exposed theoretical and methodological limitations and exacerbated the issues noted earlier-fluid or porous boundaries within systems, unavoidable participation in the system, and contingent framing of problems within which observations and assessments are made.
The core corrective action taken by adaptive governance is to open the decisionmaking process to stakeholders. In a policy context, scientific management emphasized the roles of political or organizational authorities, scientists, and other technocratic experts. Absent from this approach to governance was the governed population itself-the people that had unique knowledge about the system and were most affected by the government's decisions.
The problems posed by the exclusion of stakeholders from governance processes have been exemplified in the evolution of smart city management concepts and practices. Initial efforts to develop smart cities focused on large-scale, advanced infrastructure and master planning, neither of which provided the expected benefits to city managers or their inhabitants. 72 Instead, later generations of investments emphasized the development of open data infrastructures and processes for increasing citizen participation in governance decisions. 73  These investments include efforts to place the consumers of governance services on the 71 The extensive literature on complex systems offers perspectives on how combinations of interdependence and feedback within systems, among other properties, create limitations to what more-traditional scientific approaches can offer. See, for example, Warren Weaver, "Science and Complexity," American 
Scientist, Vol. 36, No. 4, October 1948
[I]n the face of uncertainties the burden of decision making shifts to monitoring and evaluating and to terminating policy alternatives that fail. No policy can be a permanent 74 Goldstein and Dyson, 2013; Michael Bloomberg, "City Century: Why Municipalities Are the Key to Fighting Climate Change," Foreign Affairs, Vol. 
94, No. 5, 2015
Brunner and Steelman, 2005, pp. 21-25.
MR-1626
-RPC, 2003, pp. 26-27.
6
Shifting attention from governments to governance reveals alternative modes for organizing how social systems allocate resources and coordinate the behavior of members. For some observers, such a change presents an alternative to studying the role of government in society by looking at the design and activities of other organizations, such as commercial firms and civic groups. 80 For others, the shift to governance places the entire study of social organization into the broadest possible context, putting governments, private-sector firms, civil society, religious institutions, and more on a continuum of interacting organizations that have managed the flow of information and resources within society, maintained the continuity of social life, and enabled society's transformation or collapse when challenges arise. 81  From the perspective of governance, three modes of social organization warrant attentionhierarchies, markets, and networks-each motivating action within social systems and organizations in different ways. 
Adaptive Governance
Relationships underlying observed behaviors are stable if not universal (reductionist).
Relationships evolve; the behaviors of living forms depend on the context (contextual).
Relationships are tested by experimental, quantitative, and other "hard" methods.
Multiple methods are necessary, including qualitative, interpretive, and integrative.
Verified relationships are independent of any particular context or point of view.
Verifiable explanations of behaviors differ from one particular context to the next.
Knowledge of open systems is contingent and incomplete; surprises are inevitable.
Goals are single targets to be realized efficiently; they are fixed, given, or assumed to separate science from nonscience, and progress is measurable.
Multiple goals are to be integrated if possible or traded off if necessary; they depend on judgments in the particular context and are subject to change.
Problem definition depends on scientific assessments within procedures and boundaries established by higher authority.
Problem definition depends on human interests and other contextual considerations, including law and policy.
Science-based technologies are prerequisites for solving problems and gaining support.
Local and scientific knowledge are both relevant to solving policy problems.
Policy alternatives focus on how to realize the target, discounting uncertainties.
Modest incremental steps minimize the unintended consequences of policies.
Planning is the priority in policy process; monitoring and evaluating are not.
Policy process often depends on monitoring, evaluating, and terminating failed policies.
Management proceeds from the top down under a single, central authority.
Policy integration proceeds from the bottom up under fragmented authority and control.
Only the experts are qualified to make and implement sound management plans.
Participation is open to almost any person or group with a significant interest in the issue.
Bureaucracies are necessary to enforce uniform rules and regulations.
Community-based initiatives can compensate for the limitations of bureaucracies.
Expertise and authority to enforce rules and regulations are the necessary resources.
Local knowledge, respect, and trust are a few of many resources necessary for success.
Plans and planning processes are standardized and stabilized over long periods of time.
Successful policies are diffused and adapted elsewhere, at the same and higher levels.
Science replaces politics through clear policy direction from elected officials.
Politics are unavoidable and are commendable when they advance the common interest. 
Hierarchies are among the most-common forms of organization. In governance, hierarchical arrangements are identified in two interrelated ways. The first and most common is the distribution of authorities that establish the chain of command within organizations. For example, March and Simon noted that an individual's participation in formal organizations cannot be adequately characterized by a series of independent transactions between an employee and employer. Instead, employment implies the acceptance of a role that obligates employees to accept the legitimacy of the employer's authority and prescriptions regarding what actions are permitted and prohibited:
In joining the organization, he accepts an authority relation, that is, he agrees that within some limits (defined both explicitly and implicitly by the terms of the employment contract) he will accept the premises of his behavior orders and instructions supplied to him by the organization . . . Acceptance of authority by the employee gives the organization a powerful means for influencing him-more powerful than persuasion, and comparable to the evoking processes that call forth a whole program of behavior in response to a stimulus. 82   In this formulation, hierarchical relations are characterized by power relations between actors. Within organizations, this is accomplished through employment contracts and job definitions and the titles they carry. In society, having a monopoly over the legitimate use of force to manage inhabitants within a bounded territory or domain is often taken as the basis for defining the state. 83 In either case, exchange occurs as a result of the coercive power that one actor possesses over another.
A feature of hierarchies in governance is that as the scale of the governed population grows, the performance of authoritative roles can become increasingly depersonalized and routine. The emergence of bureaucracies, which emphasize professionalization and welldefined, routinized organizational processes, further differentiates states from other forms of governance, most notably bands, tribes, and chiefdoms that rely on patronage and personal relationships between actors. 84 Importantly, anthropologists have noted that both states and complex chiefdoms can govern large swaths of territory and large populations, but they do so through different organizational mechanisms, as will be discussed later. 85   82 March and Simon, 1993, p. 110.   83 Max Weber, Economy and Society, Vol. 1, eds. Gunther Roth and Claus Wittich, Berkeley, Calif.: University of California Press, 1978, pp. 54-56.   84 Flannery, 1972. 85 Elizabeth M. Brumfiel and Timothy K. 
Earle, eds., Specialization, Exchange, and Complex Societies, New York: Cambridge University Press, 1987;
Feinman and Marcus, 1998
Stephen A. Kowalewski, Gary M. Feinman, and Laura M. Finsten, Ancient Mesoamerica: A Comparison of Change in Three Regions, New York: Cambridge University Press, 1993, pp. 13-19.
Hierarchies are thought to work best when an organization has a fairly clear purpose. Bureaucracies in the public sector are meant to pursue the public good. Firms in the pri- 86 Howard H. Pattee, ed., Hierarchy Theory: The Challenge of Complex Systems, New York: George Braziller, Inc., 1973; Robert V. O'Neill, Donald Lee DeAngelis, J. B. Waide, and Timothy F. H. Allen, A Hierarchical Concept of Ecosystems, Princeton, N.J.: Princeton University Press, 1986; Valerie Ahl and T. F. H. Allen, Hierarchy Theory, New York: Columbia University Press, 1996; Philip E. Agre, "Hierarchy and History in Simon's 'Architecture of Complexity,'" Journal of the Learning Sciences, Vol. 
12, No. 3, July 2003;
Simon, 2003, pp. 183-216
[1776]
Markets provide an alternative model for allocating resources within systems. As opposed to relying on centralized and authoritative decisionmaking, markets offer a means for distributed actors to exchange goods and services according to prices. When markets are efficient, and actors have the information they need, an optimal allocation of resources can be found such that no actor can be made better off (i.e., no actor can acquire an alternative allocation of goods that they would prefer without making other actors worse off).
Information is essential to support governance through markets. Uncertainty and asymmetries in information might limit the willingness of actors to fully use their endowments and enter into mutually beneficial exchanges. 91 The basis for forming institutions of governance is to address market failures that occur when actors lack the trust to transact with one another. 92 This can be overcome by depersonalizing trade and transferring trust from the individuals involved in the transaction to the rules for participating in the market itself (i.e., creating trust in the institutions that oversee markets provides participants with confidence that others will abide by its rules, including rules that manage disputes between actors). Depersonalization is a critical step along the path to commoditization that can be achieved through standardized weights, measures, and scales (e.g., determinations of the quality of beef or wheat) that allow exchange to occur between producers and consumers that are indifferent to each other's identity. 93  Although there are many alternative theories of how markets work, there is broad consensus that participation and exchange is motivated by self-interest and competitive pressures:
The dominant neoclassical view [of markets] emphasizes perfect competition. In this view separate firms try to maximize their profits by responding to changes in prices. . . . The alternative view of the neo-Austrian school emphasizes the competitive process. . . . Neo- 90 Bevir, 2012, p. 18.   91 Ian Molho, The Economics of Information, Malden, Mass.: Blackwell Publishers, 1997. Austrian economists think of the market as a process of selection occurring in changing and tumultuous conditions. 94   The effectiveness of markets, then, rests on two important properties that are rarely stated explicitly. The first is coercive power to limit participation to trusted parties, enforce agreements made between parties, or both. Second, although actors in markets might engage in mutually beneficial trades of goods and services, the structure of exchange is fundamentally competitive because each actor benefits from offering less of what they possess to get more of what they desire. Thus, although markets are considered distinct from hierarchical, authoritatively controlled systems, coercion and competition remain central to their function, even if violence is not required to motivate exchange.
While hierarchies and markets represent depersonalized modes for allocating resources, networks provide a personalized alternative. Rather than rely on coercive or competitive influence to motivate decisions and action, exchange in networks proceeds based on cooperation, reputation, and the expectation of reciprocity:
Networks consist of multiple actors who are formally separate but depend on one another for key resources and so build long-term relationships to exchange resources. On the one hand, networks differ from hierarchies because they do not usually contain an authoritative centre to resolve disputes among the actors. On the other, they differ from markets in that the actors engage in repeated and enduring exchanges, often relying on trust and diplomacy rather than prices and bargaining. Examples of network relationships thus can include cooperative set-ups, coalitions, relational contracting, partnerships, and joint ventures. 95   The differences among networks, hierarchies, and markets are profound. Whereas increases in power, the centralization of authorities, and the ability to provide goods and services at lower prices might increase an actor's ability to access and allocate resources in markets, these factors might fail to benefit actors in networks. Instead, in systems that rely on trust and reciprocity, access to resources and the ability to control their allocation depends on earning the trust of other actors, developing commitments, and positioning oneself to become indispensable within the system. Such objectives are often accomplished through exchanging more than the minimum of what is desired or required with the expectation of creating commitments to future interaction and exchange. Cooperative interactions have been regarded as an essential element of biological survival and the ability to cope with uncertain and novel threats. For example, Geerat J. Vermeij noted the central role that cooperative relations have played in biological evolution and the ability of species to cope with uncertainty: 94 Bevir, 2012, p. 23.  The organizational properties that enable biological entities to cope with unpredictable circumstances may likewise have originated as adaptations to everyday problems, but they more directly transform unpredictable phenomena to predictable ones. They do so by cooperation, creating multiple novel combinations of preexisting components, preventing threats from spreading, or creating larger biological units that have a longer life span and therefore the means to retain and accumulate information about rare events. Redundancy and adaptability emerge as modules multiply, cooperate, and forge larger stable evolutionary units. 96   The networked form of governance poses a significant challenge for engaging in UGS. First, the dominant experiences and expertise resident within DoD and the NSE are based on the image of organizations competing, whether by violently asserting their will or by engaging in nonviolent, market-based exchange within the shadow of competition and conflict. Viewing governance through the lens of layered obligations and commitments challenges institutions built to compete for more-abstract and diffuse pursuits, such as the national interest. Second, cooperating in networks is just as strategic as competing within them. Strategically minded cooperation is simply an alternative approach to ensuring access to resources (material, financial, ideological, etc.) through the creation of social, deontic bonds of rights, roles, permissions, and obligations.
The ASDA cycle is fundamentally agnostic to developing expertise to engage with governance structures of all types-its emphasis on learning through interaction can be directed toward discovering patterns of authority and exchange within the international system. However, networking expertise might be better suited toward competing in infinite games because of the games' unbounded characteristics. Whereas hierarchies might cease to operate if coercive power is lost, and markets-based exchanges are bounded by the honoring of contractual agreements, relations in networks reward the accumulation of reciprocal commitments that are forward-looking-exchanging goods or services now in return for unspecified future transactions. This allows a level of robust interaction under uncertainty that would otherwise require elaborate efforts to avoid or convert uncertainty to risk under alternative frameworks. 97   96 Geerat J. Vermeij, "Security, Unpredictability, and Evolution: Policy and the History of Life," in Raphael D. Sagarin and Terence Taylor, eds., Natural Security: A Darwinian Approach to a Dangerous World, Berkeley, Calif.: University of California Press, 2008, p. 36. 97 For a discussion of the challenges posed by such conversions, see Francis X. Diebold, Neil A. Doherty, and Richard J. Herring, eds., The Known, the Unknown, and the Unknowable in Financial Risk Management: Measurement and Theory Advancing Practice, Princeton, N. 
New perspectives on long-term competition might be realized by viewing decisions to engage in and manage UGS as infinite games. Pairing the ASDA cycle-with its emphasis on learning at a faster rate than competitors (as opposed to deciding at a faster rate)-with infinite games might provide a new basis for engaging in UGS without the meta-framing of a "cycle of conflict" that is codified in the JPC. In this context, problem-centric governance, adaptive governance, and alternative modes of governance based on hierarchies, markets, and networks all offer perspectives that can inform how DoD and the NSE might engage in UGS. Together, these frameworks offer insights into how to (1) reduce internal barriers to adaptation, ( 
2
Science and Technology Planning for the Future-Operating in Three Realms
The United States does not know where its nation's military may be asked to respond in the future, but how it responds is embedded in U.S. strategy, with operational and tactical components based on doctrine, training, and technology. Unlike 20th-century conflicts, this century's doctrine, training, and technology exist in three realms: physical, human, and cyberthe last an abstract realm created by the physical interconnectedness between humans and between humans and machines. 1  Developing offensive and defensive technology for warfare in the physical realm has been vital for millennia. To control a populace, subjugate it, or ultimately force its surrender still requires action in the physical realm.
Controlling a populace's will-the human realm-without direct force has also existed for millennia. Intimidation and propaganda affect the human character, not the human corpus. Therefore, sociology and psychology have always functioned as an intimate accessory to force.
The advent of internet technologies in the 1990s, which gave rise to the cyber realm, combined with more recent advances in data analytics has revealed even more insight into human behavior.
As the cyber realm has evolved since 2000, the links between the physical and social sciences have grown stronger. In the cyber realm, one can manipulate people and systems from afar. Vulnerabilities in computer code can be exploited to impede physical systemsand human susceptibility to rumors can be exploited to impede discourse. The cyber realm amplifies propaganda's ability to bypass critical thinking and elicit emotional responses.
The U.S. Department of Defense (DoD) is attempting to understand how its military can operate in the human and cyber realms with the same facility as it does in the physical realm. See Figure 
7
Creating science and technology (S&T) programs that depend on sociology and psychology is different than creating S&T programs for the physical sciences. The social sciences are less reductionist than the physical sciences; there are no immutable physical laws, such as conservation of energy. Instead, a multiplicity of factors must be examined. The essential features of the human-social and cyber realms are interaction and interconnectedness on a massive scale. How does one structure S&T research programs in these areas?
This chapter is the first of several that address questions of aligning and managing S&T research across physical and social science disciplines. It introduces the reader to DoD's S&T enterprise, which is based predominantly on the physical sciences; draws distinctions between the physical and social sciences that affect how their research is conducted; and provides guidelines for structuring social science programs to meet the needs of decisionmaking and engagement in undergoverned spaces (UGS). UGS are those spaces in which a state presence is weak and legitimate institutions fail to exist. The subsequent chapters in this part of the report-by Andrew M. Parker, 2 Elisa Jayne Bienenstock, 3 and Edward Geist 4 -address specific scientific challenges, while Chapters Eleven, Twelve, and Thirteen (by Steven W. Popper, 5 Paul K. Davis, 6 and Robert J. Lempert, Kelly Klima, and Sara Turner, 7 respectively) in the next part take on the connection between scientific knowledge and decisionmaking. These chapters address the importance of social science to national security, the importance of having a strategic posture, and how to support the development of technology based on social science in an enterprise dominated by the physical and computational sciences.
The observations in this chapter are drawn from my experience as chief scientist of the Army Research Laboratory (ARL) developing programs grounded in the physical sciences. The presentation is personal, not academic. This is meant both to illuminate the mindset of a physical scientist and to make the process of developing technology tangible to readers unfamiliar with it. I draw particularly from research efforts on autonomous agents to highlight the interplay between the physical, human, and cyber realms. I present the features of a wellstructured physical sciences program and end with comments and caveats on applying these features to programs that encompass all three realms.
The tangible tools we use in our daily lives are based on scientific principles that matured into engineering before being mass manufactured into useful implements. This progression from science to engineering to technology, often ascribed to Vannevar Bush, is reflected in the government's budgetary categorization of research (see Table 
7
Each stage is distinguished by an increase in understanding, which is obtained by posing different questions. But what is the origin of these questions?
As represented in Table 
7
In the following sections, the differences between science, engineering, and technology are defined and distinguished by the different research motivations indicated in Table 
7
Sargent, Jr., Department of Defense Research, Development, Test, and Evaluation (RDT&E)
The Oxford Dictionary defines science as "the systematic study of the structure and behavior of the physical and natural world through observation and experiment." Science is driven by observation and a desire to understand observed patterns. Stated simply, science is about understanding the physical world to answer the question "why." Why does the world function in the manner we observe it?
Engineering is defined as "the application of scientific principles to design structures, machines, apparatus, or processes." Unlike science, the operative engineering question is "how." How can an effect be reproduced, and under what conditions? How can one use a physical effect to do something useful?
Finally, technology is defined as "the application of scientific knowledge for practical purposes." Technology provides the means to do work based on the scientific and engineering understanding gained. Technology allows one to produce the desired outcome predictably, effectively, and reliably on a large scale.
Understanding the distinctions between science, engineering, and technology is important when attempting to understand different types of research. As noted earlier, Table 
7
The lower left quadrant, in which research provides no utility and no understanding, is easily dismissed as an unworthy pursuit. The lower right quadrant-applied research-is the Edisonian Quadrant. As evidenced by Thomas Edison's approach to develop a viable filament for his incandescent bulb, it is possible to provide utility without fundamental understanding. Rather than ask which properties of materials are the best indicator of their suitability as a filament, Edison chose to test thousands of materials. His inefficient but dogged approach eventually led to a carbonized bamboo filament and the infamous quote "genius is one percent inspiration and ninety-nine percent perspiration." Given limited funds, such a scattershot approach is not well suited for DoD purposes.
We can contrast Edison's approach with that of Albert Einstein's iconic Gedankenexperiments, which are representative of the upper left quadrant-basic research. Einstein is often portrayed as a lone individual pondering innumerable what-ifs. How else could someone develop a model of gravity as mass bending space or figure out that space contracts and time expands as an object's velocity approaches the speed of light? However, although Einstein's concepts expanded our understanding of the physical world to an unrivaled degree, their utility at the time was uncertain. Such research is termed curiosity driven, and the case for government support of it has always been the unknowns about the long term. No one could have imagined in the early 20th century that Einstein's theory of relativity would prompt a necessary correction to the Global Positioning System 10 or that Einstein's unease with quantum mechanics would prompt the so-called second quantum revolution based on entangled elementary particles. 11  Referring to the upper right quadrant as Pasteur's Quadrant acknowledges that Louis Pasteur's work in chemistry and microbiology was motivated by his desire both for understanding-comprehending the causes of diseases-and application-how to prevent those diseases. The next section discusses examples of how thinking in Pasteur's Quadrant leads to new research and increased understanding.
To make Pasteur's Quadrant tangible, I present two examples of autonomous agent development that I was responsible for at ARL: (1) a program to enable handheld autonomous platforms and ( 
2
ARL has been involved in developing robotic ground vehicles since the mid-1990s and even helped DARPA formulate its 2004 Robotics Grand Challenge. In 2006, I was asked to develop a research program to mature the capabilities of small (handheld) autonomous platforms. The program was called Micro-Autonomous Systems and Technology (MAST).
The fundamental problem in MAST is that solutions to autonomous locomotion and navigation for vehicle-sized platforms provide little insight to enable handheld ones. Specifically, the energy available for mobility is reduced. Computational processing power is also reduced (i.e., in 2006, the computation available in a chip-scale processor capable of fitting on a small platform was insufficient for the platform to sense, process, move, and navigate as robustly as large platforms at that time had demonstrated). Furthermore, the physics of motionwhether crawling or flying-are different for small platforms than they are for large ones. 10 Neil Ashby, "Relativity and the Global Positioning System," Physics Today, Vol. 55, No. 5, May 1, 2002.   11 Jonathan P. Dowling, and Gerard J. Milburn, "Quantum Technology: The Second Quantum Revolution," Philosophical Transactions of the Royal Society of London, 
Series A, Mathematical, Physical and
Engineering Sciences, Vol. 361, No. 1809, August 15, 2003.
One problem we recognized in this scenario was how platforms launched in an external environment move into an interior one. For ground crawlers, terrain can change from soil or sand to a hard surface. Flyers must identify points of ingress and fly through them. As they do, aerodynamics can change (e.g., from a breezy exterior to a calm interior). The challenge is for platforms to transition smoothly from one environment to the other. How platforms do this became one of MAST's several research foci. This focus eventually led to an increased understanding of terramechanics for crawling platforms, i.e., explaining why large insects walk the way they do, as well as the development of simple parametric models that MAST researchers used to replicate this locomotion on different surfaces. 13   
This MAST example highlights the mindset of scientists and engineers who work in Pasteur's Quadrant. Its specificity indicates the nature of problems this group enjoys solving. Understanding this was helpful when, as chief scientist, I was tasked with developing a long-term research vision to enable the future capabilities desired by the Army for autonomous agents. 14  The program had to be scientifically meaningful yet relevant to the Army.
Senior technical staff, both researchers and managers, and I distilled from Army documentation that effective teaming between soldiers and autonomous agents was an essential desired capability. (We chose the term agents, as opposed to robots, to underscore that not all autonomous agents are mobile. Many exist on computing platforms, such as agents that are digital assistants on smartphones and smart speakers.)
Through internal and external workshops, we identified three broad areas for investigation: (1) increasing the intelligence of autonomous agents, ( 
2
3
Army Research Laboratory," in 2008 IEEE National Aerospace and
Electronics Conference, Dayton, Ohio, 2008;
Joseph N. Mait, "The Army Research Laboratory's Program on Micro-Autonomous Systems and Technology," in Thomas George, M. Saif Islam, and Achyut K. Dutta, eds., Micro-and Nanotechnology Sensors, Systems, and Applications, Vol. 7318, Orlando, Fla.: SPIE, 2009
Stephen Childress, Anette Hosoi, William W. Schultz, and Jane Wang, eds., Natural Locomotion in Fluids and on Surfaces, New York: Springer, 2012.
This application of the Pasteur's Quadrant paradigm allowed ARL to structure its efforts objectively, identify metrics for technical performance, and, finally, develop an execution plan despite having only a vague understanding of each area. This structure satisfied the technical staff's attraction to technically deep questions while also meeting the Army's desires. Furthermore, the plan enabled the lab to focus its existing resources and to plan for future ones. Managers were able to identify the disciplines and backgrounds most needed in new hires, identify equipment purchases, and reallocate space.
The previous section describes the processes I used to craft a focused research program and a strategic vision for long-term research. More important for the purpose of this report are the lessons learned and advice I can offer to those charged with crafting research programs that link psychology, sociology, and other social sciences with physical and information sciences.
Despite the differences between the social and physical sciences, many of my recommendations are repeated in other chapters in this report. A recurrent theme is that the significance of a program is highest when the program is established within the framework of a strategic vision. Establishing a strategic vision bounds the area of investigation and allows one to identify areas where knowledge is high and (more importantly) areas in which knowledge is low and further investigation is required.
A significant difference between the social and physical sciences is that, although each seeks predictive power, the contingent nature of the social sciences places stronger demands on explanation and causal inference. Unlike problems in the physical realm, problems in the human and cyber realms resist simplification to behavior about some equilibrium point. They are nonlocal (entities do not need to be near one another to influence each other), nonstationary (entities' behavior can change temporally in unpredictable ways), and nonlinear (the response of an entity to a change in an input stimulus is not proportional to the change in the stimulus-"the straw that broke the camel's back"). Predicting the behavior of entities in such an environment is less deterministic than doing so for engineered physical systems.
This does not negate the importance of social science research. Rather, it dictates a different mindset toward the research goals, objectives, implications, and applications. As Elisa Jayne Bienenstock emphasizes in Chapter Nine, the lack of immutable physical laws does not relegate social sciences to a lesser field of study. The social sciences still adhere to the scientific method and are just as rigorous as the physical sciences. They have simply adapted science to the character of their discipline. 15  
Successful research and development programs reflect the following factors:
• an understanding of the capabilities desired through meaningful objective metrics • a balanced portfolio of approaches to achieve the desired capabilities • the use of transparent and auditable processes in decisionmaking, such as periodic review (especially by knowledgeable outsiders) • experimentation • maintaining cognizance of activities in the technical community at large. Most importantly, program leadership must have the integrity to change direction if periodic review indicates that one approach is not meeting expectations or if community cognizance points to an alternate approach that improves performance. The program must be structured from its inception to allow this flexibility. Research and development programs do not fail because their assumptions were not 100 percent correct at the beginning but because they do not pivot in new directions when required.
Technical managers need to set research directions now based on their best estimates of what will be needed in the future. Careful examination of the desired capabilities is essential and leads to a firm foundation on which to build. This is the essence of questions 1-3 in the Heilmeier Catechism (HC), which is used extensively at DARPA to establish new programs (see the text box). 16  Dialogue between technologists and operators is a good first step to enabling researchers to grasp the general capabilities desired. Early in the MAST program, researchers participated in a three-day exchange with the Army's Maneuver Center of Excellence at Fort Ben- 16 DARPA, "The Heilmeier Catechism," webpage, undated.
1. What are you trying to do? Articulate your objectives using absolutely no jargon. 2. How is it done today, and what are the limits of current practice? 3. What is new in your approach and why do you think it will be successful? 4. Who cares? If you are successful, what difference will it make? 5. What are the risks? 6. How much will it cost? 7. How long will it take? 8. What are the midterm and final "exams" to check for success? SOURCE: DARPA, undated.
ning. 17 Researchers received training on small-unit building assault (see Figure 
7
Army documentation and workshops with academics and uniformed personnel shaped our Human-Agent Teaming endeavor. In Chapter Eight of this report, Andrew M. Parker 18  acknowledges the need for collaboration across disciplines and proposes elements of a social science infrastructure to achieve this, while Paul K. Davis 19 also notes in Chapter Twelve the need to overcome disciplinary fragmentation to aggregate knowledge in the social sciences in service of policy applications.
As indicated in the Human-Agent Teaming example, notions about what exactly is needed are sometimes vague. Our identifying the information exchange across the human-agent boundary was a key insight. The next step, again consonant with the arguments by Parker 17 Albert Sciaretta, Joseph N. Mait, Richard Chait, Elizabeth Redden, and Jordan Wilcox, Assessing Military Benefits of S&T Investments in Micro Autonomous Systems Utilizing a Gedanken Experiment, Washington, D.C.: National Defense University, Defense Technology Paper, January 1, 2011. 18 See Chapter Eight 
(Parker, 2022)
Building on the Human-Agent Teaming example, collaboration-whether between humans or between humans and agents-requires that all participants share a common understanding of their mission, its execution, and the environment and circumstances in which the mission will be executed. How does one know objectively when this has been achieved? What does one measure, and what value or condition indicates that common understanding has occurred? In an operational setting, the speed with which common understanding is achieved is critical. For a tactical mission, a research goal might be to achieve a 70-percent level of common understanding within seconds. Although how one does this remains unknown, the problem has been distilled from a notional capability to an objective measure of performance. (Recall that my perspective is grounded in the physical sciences.)
Because the technology or combination of technologies that lead to success are unknown at the outset of a research project, a balanced portfolio of approaches is important in the beginning. Not all approaches will pan out. This uncertainty is reflected in HC questions 5 (understanding risks) and 8 (checking for success through periodic review). The review process should be formal, transparent, and auditable. It is the process by which decisions are made as the program proceeds and involves both peer review of technical matter by the science and engineering community and review of the program by stakeholders and technical managers. Employing external reviewers disinterested in the outcome is critical.
Second to the external reviewer is the internal Curmudgeon, who always tells researchers why something will not work or cannot be done. Technical managers need Curmudgeons to explain in detail why they believe what they believe. Sometimes, the Curmudgeons are wrong. However, even if this is so, Curmudgeons force researchers to reexamine their assumptions and to be rigorous in their analyses.
Graybeards are the Curmudgeon's cousins. 21 They are also internal colleagues who bring their expertise and experience to a program. What distinguishes a Graybeard from a Curmudgeon is the diplomacy with which they tell researchers their baby is ugly. A Graybeard will offer solutions, not just the Curmudgeon's critique.
When technology integration is involved, experimentation is essential. Engineers need to put different pieces together to see how they function. Not a single vehicle completed DARPA's first Grand Challenge in 2004. The farthest any vehicle traveled was seven miles. Although the experience was objectively a failure, the development teams learned from it and 20 See Chapters Eight and Nine 
(Parker, 2022;
Bienenstock, 2022)
five vehicles successfully completed the 132-mile course in the 2005 Grand Challenge. The chapters that follow recognize the need for experimentation even in the social sciences.
The Army's Future Combat System (FCS) is an ignoble example of the need for experimentation. 22 The development of FCS was motivated by the desire to exploit nascent network capabilities. 
23
The need for experimentation was recognized in the organization of Army Futures Command in 2018. Army Futures Command consists of three major subcommands, one of which is Combat Systems. 24 Combat Systems is responsible for developing experiments, demonstrations, and prototypes. I am cautiously optimistic about this development. It bears noting that technologies developed from the FCS impetus have found their way into ground platforms. The capabilities were not far-fetched; they needed time to mature through test and failure.
Returning to the theme of testing, systems built on integrating technologies are weakest at their seams. Consequently, Red Teams are an essential element in experimentation and its simulation cousin, wargaming. Red Teams consist of Curmudgeons intent on breaking things. Because they serve as surrogates for a real adversary, Red Teams are not bound by the rules of fair play. Consequently, they keep developers on their toes.
A program's primary focus is internal-specifically, how to achieve an objective using an approach that is agreed upon through common understanding and well suited to the personnel and facilities available. However, it is important not to lose sight of developments outside one's purview-cognizance of the community or, colloquially, tech watch-which is an important adjunct.
Human-Agent Teaming provides an example of the importance of tech watch and, particularly, advancements in artificial neural networks for computing. Before 2010, neural networks had a checkered history. These networks, inspired by human brain activity, are meant 22 Joseph N. Mait and Jon G. Grossman, The Return to Relevancy: The US Army and the Future Combat Systems, Adelphi, Md.: National Defense University, April 1, 2002; Joseph N. Mait and Jon G. Grossman, "Is Technology Mature Enough for the Future Combat System?" National Defense Magazine, September 2002; Joseph N. Mait, "Balancing Technology and Risk in the Future Combat Systems," Transformational Science and Technology for the 
Current and Future Force, Vol. 42, 2006.
Tech watch is one of several hedges against missteps in initial assumptions. It helps mitigate risk. As a hedge to conventional thinking, online tools based on gaming and crowdsourcing provide a way to generate innovative solutions to solve a specific problem. They are less likely to help answer fundamental questions in science. Furthermore, proffered solutions need to be evaluated and curated to separate science fact from science fiction.
The depth of an organization's bench provides an additional hedge for development programs. Tech reachback is the entirety of an organization's staff, beyond just the Curmudgeons and Graybeards, whose broad experience and expertise managers can access when confronted with insurmountable problems that demand immediate attention.
The value of a deep bench is evident in the impact that long-term ceramics research at ARL had on delivering transparent armor to the U.S. Army after the 2003 Iraq invasion. While working at the Army's Material Technology Laboratory in Watertown, Massachusetts, in the 1970s, James W. McCauley developed a transparent ceramic, essentially a bulletproof window using ceramic armor technologies. 26 McCauley continued this work after the Material Technology Laboratory was integrated into ARL in 1992, but it remained primarily a research program. 27 This changed after the U.S. incursion into Iraq. Plagued by improvised explosive device attacks, DoD published an urgent universal needs statement for improved vehicle protection. Within a year, more than 4,000 High Mobility Multipurpose Wheeled 25 Dan Claudiu Cireşan, Ueli Meier, Luca Maria Gambardella, and Jürgen Schmidhuber, "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition," Neural Computation, Vol. 22, No. 12, December 2010. 26 James W. McCauley, "A Simple Model for Aluminum Oxynitride Spinels," Journal of the American Ceramic Society, Vol. 61, Nos. 7-8, 1978.   27 Parimal J. Patel, 
Gary A. Gilde, Peter G. Dehmer, and James W. McCauley, "Transparent Armor," AMPTIAC Newsletter, Vol. 4, No. 3, Fall 2000.
28
To close this section, I reiterate the program characteristics that increase the likelihood of a successful research program: meaningful objective metrics that reflect an understanding of the capabilities desired, a balanced portfolio to achieve the desired capabilities, transparent and auditable processes in decisionmaking, experimentation, tech watch, tech reachback, and enlightened leadership.
For programs that span the physical, human, and cyber realms, experimentation is perhaps the most valuable of the listed characteristics. Developing theories, performing analysis, and making predictions when physical absolutes are muddled by human foibles is difficult. Therefore, insight and understanding are best gained through experiments and wargames. I continue this speculative posture in the next section, where I comment on efforts to make strategic and operational planning more adaptive and more competitive.
Given the intent of this report, one can reasonably question this chapter's role. My presentation has been a personal one based on lessons learned structuring physical science research programs. Furthermore, my experience has been solely in developing operational capabilities for the future Army. This report is about improving strategic and operational security planning to be more adaptive and competitive.
The authors of the following chapters underscore that the aforementioned lessons learned remain valid even when applied to social science research. An important caveat is that one must understand the nature of social science research. Thus, my decision to highlight programs on humans interacting with technology was deliberate. I have an appreciation for the social scientists' perspectives and an understanding of the work they do, which lends credence to my observations in this final section.
To expand the nation's capabilities to engage in so-called infinite contests, DoD is investing in new technologies to compete in UGS. 
29
As stated in U.S. Joint Doctrine Note 2-19, the role of the strategist is "[to] exercise influence over the volatility, manage the uncertainty, simplify the complexity, and resolve the ambiguity, all in terms favorable to the interests of the state and in compliance with policy guidance." 32 Such a formulation matches the objectives of a finite game, in which one side wins, the other loses, and ambiguity is eliminated. 33 In contrast, an infinite contest, where influence is in constant flux, requires a different approach to vulnerability, uncertainty, complexity, and ambiguity. 
34
35
36
7
I comment on the second and third elements of the ASDA mode: Sense and Decide. If the goal is sustaining long-term influence, what does one measure as part of the sensing process to know that applying an ASDA decision loop, as opposed to an alternate approach, has improved one's long-term influence? This is critical because building technology is easy only when one knows what the technology is supposed to achieve.
It is also important to recognize the practical constraints of sensing. One needs to understand the measurements that sensors provide over an area, as well as the measurements they cannot provide. In information science, the characteristics of this so-called null space are critical to understanding the limitations of information derived from sensor measurements.
Recognizing the existence of the null space is just as critical in the social sciences as it is in the physical sciences. When sensing is sufficiently dense, even when no sensor is capable of measuring some variable in time and space, e.g., energy or pressure, one can interpolate measurements from multiple sensors to obtain an acceptable and reasonable estimate. How 31 Simon Sinek, The Infinite Game, New York: Portfolio, 2019. 32 Joint Doctrine Note 2-19, Strategy, Washington, D.C.: U.S. Joint Chiefs of Staff, December 10, 2019. 33 Carse, 1986.  often one makes measurements and the length of time it takes to process them also affect the fidelity of derived information.
However, all sensors have limitations. Sensing faster or more densely does not overcome the fundamental limitation that there always exist data that cannot be measured. In the physical realm, filling these gaps is called extrapolation. In the social realm, filling these gaps is called speculation. Both are unreliable and noisy, especially the farther one is from confirmed  measurements. In a nonlocal, nonstationary, and nonlinear system, one does not have to be too far away before noise overwhelms any signal.
The period between measurements impacts the effectiveness of decisions based on those measurements. If the period is too long, one can miss important events. However, if it is too short, it is difficult to distinguish a significant event from a random one. Because information is contained in deviations from a norm, it is important to establish a baseline by observing over a long period or, as Elisa Jayne Bienenstock refers to it, measuring the mundane. 37  One impetus for increased interest in the social sciences is the recognition that many of the problems posed by UGS must ultimately be understood and shaped through the lens of human interaction. A second impetus for increased interest in the social sciences is that advances in computation enable new tools for discovering the inner workings of complex social systems. 38 For example, data analytics have allowed us to discern previously undetectable patterns within a population over time and space and, thus, identify precursors to conflict or crisis. Thus, much effort is focused on the application of these tools to improve decisionmaking.
This is both a blessing and a curse. As alluded to by Bienenstock, the tantalizing potential of such tools creates considerable churn in program executive offices as empirical approaches are generated without the foundational sciences to back them up. 39 The guidance offered in the succeeding chapters, if heeded, provides a hedge against this continual churn.
The programs Bienenstock discusses, however, are not without merit. Their Edisonian approach enables the development of a social science infrastructure, including personnel with the requisite technical skills and a technology base of information.
Structuring research programs for the future is complicated by the increased melding of elements from the physical, human, and cyber realms. Sociology and psychology have become as important to the nation's safety, security, and defense as the physical and information sciences, largely because of increased people-to-people and people-to-things connectivity. Unlike fields of science with physical laws, a reductionist approach-focusing on a single factor-to multidisciplinary social sciences research is limiting and nearsighted. 37 See Chapter Nine 
(Bienenstock, 2022)
Several factors can help structure research in the social sciences. Experimentation and wargaming are especially useful for testing theories and for measuring the performance of different elements for technologies based on assumptions of human behavior. It is also important for researchers to remain cognizant of developments outside the main technology thrusts of their programs. Without question, integrity and flexibility in program leadership are essential to increasing the likelihood of success of any research program in any field.
Nonetheless, the goal of research remains the same: to gain understanding through scientific study and to use that understanding to engineer systems and ultimately solve problems. Research and development demand objective measures to show an improvement or an advantage over current solutions. The value of an approach derives from the objective outcomes that result from its application and from the conclusions drawn therefrom. The conclusions must stand up to rigorous interrogation and review.
The Need to Invest in Social Science Infrastructure to Address Emerging Crises
Emerging crises, such as civil unrest, natural disasters, economic crashes, and major terrorist attacks, can cause societal disruption by upsetting norms and breaking down traditional governance and social services. These issues have substantial and lasting impacts on societies, economies, and nations. Such emerging crises, which create undergoverned spaces (UGS), raise safety, security, social, and economic challenges. They also raise time-sensitive research questions about how we as a society respond to crisis, how vulnerabilities are disparately distributed among different groups, and how we can extract lessons learned from crises and improve long-term planning for such crises. More generally, enabling policy for emerging crises-or those yet to emerge-and promoting security and resilience for the United States and its communities requires nimble and adaptive scientific capacity. Such capacity would provide a means to react, recover, or correct course after a surprise, which will inevitably happen. However, such capacity cannot be built on the fly; rather, it must be established and maintained in advance as existing social science infrastructure.
Take the example of the coronavirus disease 2019 (COVID-19) pandemic. COVID-19 emerged and spread rapidly, creating a global shock to a variety of interlocked social, political, economic, and health systems. These interdependencies mean that the impacts of major events are often nonlinear and multilevel. 1 The pandemic has motivated a flurry of rapidly conceived, proposed, funded, and fielded studies on elements of these systems. Notable studies include research on the epidemiology of the virus and disease, biomedical countermeasures, economic impacts, and our collective behavioral and social lives. This work promises significant scientific advances, a variety of pharmaceutical and nonpharmaceutical interventions, and sweeping policy changes, similar to those made in response to the last major pandemic (the 2009-2010 H1N1 influenza). The pandemic also demonstrates the limitations of responsiveness. All too often, scientists, funders, and policymakers can only react to a crisis, marshaling funds and capabilities as quickly as possible. This is partially unavoidable, given that such events are by their very nature unexpected. However, if the disaster research field tells us anything, it is that something will happen with regularity, even if we do not know what that specific crisis will bereadiness is necessary, not just response.
Operational readiness for crises is often discussed, but it is less common to discuss scientific readiness for crises. 2 Operationally, we plan for the unexpected. However, without investments in scientific readiness in the form of standing infrastructure that is poised to adapt, 3 responses are slower, costlier, and less coordinated.
COVID-19 is hardly the only example or application for such infrastructure. Disasters and crises of many sorts are on the rise, and, like COVID-19, such crises represent unexpected, systemic shocks that lead to a feverous if reactive scientific response. Table 
8
The past 80 years have seen outbreaks of over 300 previously unknown diseases, with other disease outbreaks becoming worse and more widespread. 5 In addition, recent unrest against systemic racism in the United States and abroad, sparked by instances of police brutality; the 2008 financial crisis; and major terrorist attacks (such as those on September 11, 2 The term scientific readiness is used in engineering to denote the readiness for a given mission, a quite different idea. Here, I use readiness in the same sense as used by first responders and in public health to denote capacity and capability to respond to emerging (and as-yet unknown) events. Similar, if more narrowly focused, concepts are offered in Avi Loeb and Dario Gil, "Let's Create the Science Readiness Reserves to Advise on Catastrophes," IBM Research Blog, May 12, 2020; and Elisabeth Jeffries, "Governments Detail Gaps in Their Scientific Readiness for a Pandemic," Nature Index, June 9, 2020. 3   
) have had substantial and lasting impacts on societies, economies, and nations. Recent rapid technological changes (e.g., the surge in video conferencing precipitated by telecommuting), political shifts (e.g., increased polarization around the world), and even scientific events (e.g., the replicability crisis) can also act as unexpected systemic shocks. More generally, these shocks fit within a larger set of instances in which governance is disrupted or degraded. 6 These events, disastrous or otherwise, illustrate the ongoing need to understand the dynamic nature of public behavior and social systems at the core of UGS along with their influences on global and domestic security and resilience. This lack of scientific readiness is not uniform; readiness levels vary depending on the features of the crisis, the societal context, and research silos. For example, the exact timing and size of major tropical storms are unknown, but the tropical storm season is relatively well anticipated every year, whereas major terrorist attacks typically are conducted by an intelligent adversary who must be unpredictable to succeed. Societal context will include the availability of on-the-ground partners and capabilities needed for data generation, model formation, validation, and other common scientific tasks. To the extent that conditions do not allow this (whether through lack of basic capacity or through degradation of normal capacity), readiness will suffer.
Academically, scientific readiness is bolstered within the physical and computational sciences through investment in major infrastructure, such as observatories, sensing networks, laboratories, vessels, analytic resources, knowledge bases, and scientific networks. However, crises and other events the scale of COVID-19 are rarely restricted to physical systems. This can be seen in the public response to changing COVID-19 guidelines (e.g., regarding social distancing), political battles contrasting public health and civil liberties, changes in fertility and mortality, job loss, decreased consumer spending (among those with more discretion to do so), and dramatic reduction in geographic mobility. Human and disease dynamics influence each other, again highlighting the importance of complex interdependencies.
Unfortunately, scientific infrastructure is far more limited for addressing these social, economic, and behavioral scientific questions. However, if we take lessons from the physical and computational sciences, we can start to anticipate what sorts of infrastructure would be the most valuable investments for capturing the often-ephemeral data surrounding crises. These infrastructure priorities include sensing capacity, particularly that which provides early warning of unusual but potentially significant events. Such systems provide the triggers for mobilizing assets, whether research or operational, and they also can provide valuable data for predicting trouble spots and tracing trajectories over time. Priority infrastructure Kaiser Family Foundation, "The U.S. Government & Global Emerging Infectious Disease Preparedness and Response," fact sheet, December 2014; David M. Morens, Gregory K. Folkers, and Anthony S. Fauci, "The Challenge of Emerging and Re-Emerging Infectious Diseases," Nature, Vol. 430, No. 6996, July 2004. 6 For examples, see This chapter provides an argument for investing in social science infrastructure as a way of increasing scientific readiness. 
7
What Is Social Science Infrastructure?
The scientific community has long called for enhanced social science infrastructure. 
8
9
Emerging events provide unique opportunities for conducting research, but they also create unique challenges. Researchers will typically need to stand up new endeavors quicklyrapidly recognizing the research opportunity, designing an approach to address that opportunity, possibly building a team, and applying for funding.
The release of funding opportunities is often the gunshot that starts the research sprint. For example, my collaborators and I responded to a COVID-19 Rapid Response Research (RAPID) grant opportunity cutting across the divisions and programs at the National Science Foundation (NSF). Such grant programs are designed to capture ephemeral data and address time-sensitive problems and are critical for disaster research in the social sciences. Proposals and budgets are small and quickly reviewed to get researchers into the field as soon as possible.
Importantly, however, such calls for research follow the emergence of an event, and it takes time to write a proposal for a grant, be reviewed, and pivot to the field. Because of this process, our first data were collected in late March 2020, weeks after the pandemic reached the United States. Such delays or longer are typical. There was little opportunity for capturing earlier dynamics, and gathering pre-event data was nearly impossible, although such data are critical for understanding short-and long-term impacts of the pandemic. 11 As stated by Elisa Jayne Bienenstock in Chapter Nine, 12 without understanding the mundane-in this case, the pre-event status quo, collected through baseline assessments, such as existing disparities, expectations, and behaviors-it is nearly impossible to fully recognize and understand disruptions.
Beyond these data limitations, a critical feature of most funding is that it typically goes to independent teams, each of which is standing up its own research machinery. This provides an important diversity of perspective, but it also creates inefficiencies when commonly needed capabilities are multiply created. For example, we were fielding COVID-19-related surveys, but so were many other groups. In many cases, these groups reinvented similar processes.
Funding streams often reinforce this, because there typically is no easy way to leverage capabilities across multiple proposals. For example, these teams (including ours) each needed to gather large and diverse (even nationally representative) samples. Each needed to collect many of the same variables (e.g., risk perception, protective behaviors, demographics), and each needed to write and refine survey instruments. One solution for avoiding this redundancy in time, effort, and monetary costs is to have access to centralized, preexisting resources (in this case, survey capabilities). Such standing capabilities could efficiently provide core, common needs while still maintaining flexibility and adaptivity to specific needs (e.g., for customized surveys whose data could be merged with core data sets).
As a whole, science struggles with an inherent tension. Independent inquiry promotes innovation and competition of ideas, which has the potential to accelerate research progress and increase quality. However, coordination adds efficiency, enables the pursuit of broader strategic goals, and helps organize fields of inquiry.
Funding agencies are often tasked with maintaining this balance. In the physical sciences and engineering, it is more common to fund large scientific infrastructure projects. These projects provide a public good, centralizing large, fixed costs within a single public resource that can provide a foundation for smaller, more nimble inquiries unburdened with the need to spin up common capabilities for each project. Examples of natural science infrastructure are truly massive: NSF's National Radio Astronomy Observatory; the Integrated Ocean Observing System, which is funded by the National Oceanic and Atmospheric Administration, and 16 other federal agencies; NSF's Academic Research Fleet; the U.S. Department of Energy National Laboratories; and NSF's Big Data Regional Innovation Hubs. The data and analytic capacity produced by these facilities is beyond the scope of any specific research project, the resulting capabilities can be leveraged for diverse uses, and the scientific production facilitated by these resources is truly impressive.
Obviously, these resources come at substantial cost and (given limited resources) are funded in lieu of other, typically smaller, research opportunities. Large particle colliders, such as the Large Hadron Collider run by CERN (European Organization for Nuclear Research), are an example of such resources. CERN is proposing a new, even bigger collider whose budget would dwarf the almost $4.9 billion price tag of the Large Hadron Collider. 13  Such massive international investment provides unique capabilities to the physics research community, but this funding prioritization comes at a scientific opportunity cost. This cost applies to not only questions in physics but also many of our greatest "human" problems, such as our response to climate change and massive economic and health disparities. 14 The challenge to funders is sensibly trading off the value of the public good that can be accomplished through these large scientific projects versus the opportunity cost of other research that could be supported with those funds, such as social sciences research.
Another aspect of this challenge is to avoid being captured by pressures toward honoring past expenditures. Rational decisions should be made based solely on expected future costs and benefits. That said, past expenditures (so-called sunk costs) are notoriously difficult to ignore cognitively or politically, 15 and large projects tend to have substantial momentum. This makes decisions to abandon scientific infrastructure particularly agonizing, as evidenced by recent deliberations regarding the Arecibo Observatory in Puerto Rico. 16 Mechanisms need to be in place to maintain, update, and decommission large infrastructure projects, as necessary, along with clear criteria for when to do so.
I propose ideas for potential social science infrastructure that are analogous to physical science infrastructure and discuss the advantages they offer.
Population surveys remain a key tool in understanding public response to emerging crises and other similarly disruptive events. Surveys are often the best (or only) source of key information about the public, such as risk perception, intention to engage in protective behavior, or social learning. In the social sciences, surveys play a role similar to that of sensing networks or observatories within the physical sciences, providing observations of conditions as they occur in the world. Although "social observatories" may or may not be designed for specific events, they have the potential to detect systemic change and adapt on the fly to emerging events. For example, major industrial accidents happen with some regularity, but the United States and Caribbean nations still were not prepared for the specifics of the 2010 14 Sabine Hossenfelder, "The World Doesn't Need a New Gigantic Particle Collider," Scientific American, June 19, 2020. 15 Hal R. Arkes and Catherine Blumer, "The Psychology of Sunk Cost," Organizational Behavior and Human Decision Processes, Vol. 
35, No. 1, February 1, 1985
27, 2020.
8
20
Our experience with the NSF RAPID grants program, as implemented for COVID-19, also illustrates this problem. We are aware of at least four groups funded by RAPID grants, each to field surveys assessing public response to COVID-19 (in various ways, with various research goals). Each group took a different survey approach; however, the surveys were designed around available (and often redundant) resources and were often designed in redundant ways. Although the studies are all fielded by well-qualified teams and are producing valuable insights, as a whole, the collection illustrates the need for new research capabilities that, in a planned fashion, bring together social, behavioral, and economic data, at multiple scales, in diverse contexts, over periods that span disaster trajectories.
The solution to this problem likely takes the form of a large-scale panel study. Panels are standing collections of individuals, sampled through systematic and well-documented means, who regularly respond to surveys (typically for pay). Several large panels do exist in the United States for specific purposes. The Health and Retirement Study, funded by the National Institute on Aging and the Social Security Administration, and the Panel Study on Income Dynamics, funded by a variety of U.S. federal agencies, are good examples. 
21
• Data and data-collection capabilities would be pre-positioned, which minimizes time between event onset and the beginning of event-related data collection.
• A well-designed infrastructure would adapt to events (e.g., allowing outside groups to field novel surveys to the panel; incorporating surge capacity through the ability to quickly add samples in key demographic or geographic groups). • Because baseline data collection instruments are designed in advance to be relevant to multiple contingencies, they can explicitly track more dimensions than instruments designed post-event, providing increased opportunity for discovery. • A stable infrastructure facilitates merging data across projects and events, allowing comparisons that are not possible in typical studies. In turn, this permits development and testing of more-sophisticated theories, such as feedback loops among socioeconomic factors.
In addition to the examples already provided, infrastructure capabilities could include data storage, data visualization, institutional review board review and survey capacity (the approval process can be a major source of delays), instrument designs, robust data collection capabilities (e.g., through apps that would be beyond the resources for smaller grants or contracts), and even interfaces with modeling and simulation capabilities. Such rigorously designed data collection platforms (i.e., designed data) can also prove critical for understanding biases in more-naturalistic data flows (i.e., "found" data, such as interactions on social media). As an example, we recently used a survey on another such panel, the RAND American Life Panel, to better understand the representativeness of beliefs expressed on Twitter regarding vaccine conspiracy theories and other beliefs. 22  With the advent and increased prevalence of online survey panels, the feasibility of such an infrastructure within UGS is increasing. Online surveys rely on network connectivity, but surveys increasingly can be administered using low bandwidth or when bandwidth is unavailable (e.g., through software that downloads surveys onto a smartphone and automatically uploads answers when connectivity is restored). Online surveys can even be taken by individuals who have been displaced or have migrated because of conditions at home, with ancillary benefits of not exposing survey staff to unnecessary risk. In more substantially ungoverned spaces, other survey modes (phone, mail, in person) may be required, which would produce additional challenges.
This approach works best in locations (i.e., trouble spots) where there is some expectation that a degradation of governance will likely occur (e.g., the U.S. Gulf Coast during major hurricanes). In more-unforeseen events, adaptive infrastructure (e.g., for quickly recruiting and fielding surveys to geolocated Twitter users) may still allow for quick-response capabilities, but without baseline data. 22 Sarah A. Nowak, Christine Chen, Andrew M. Parker, Courtney A. Gidengil, and Luke J. Matthews, "Comparing Covariation Among Vaccine Hesitancy and Broader Beliefs Within Twitter and Survey Data," PLOS One, Vol. 15, No. 10, October 8, 2020.
Empirical and simulation testbeds offer another set of capabilities that could be supported by social science infrastructure. These testbeds would facilitate systematic comparison of alternate interventions through either randomized control or simulation of counterfactuals. Creating such testing situations would involve a variety of techniques from different fields: randomized clinical trials, A/B testing, gaming, and testing for robustness. Such infrastructures would be akin to laboratories in the physical sciences, allowing detailed observation under tightly controlled situations.
Social and behavioral experimental research suffers from many of the same problems already outlined for surveys but has continued to be a critical tool for understanding human behavior. 
23
24
A dedicated freestanding resource for recruiting participants and running experiments could ameliorate these validity problems. NSF supports this on a modest scale through a grant supporting Time-Sharing Experiments for the Social Sciences (TESS). 25 TESS provides a means to field online experiments using the National Opinion Research Center's AmeriSpeak panel-a probability-sampled survey platform that accurately represents the U.S. adult population. Opportunities to field experiments on TESS are competitive, but they are free to selected research teams. However, the system is not designed to scale; bandwidth is limited by funding (at the moment, $1.7 million over four years), which is far less funding than for typical physical science infrastructure programs. However, with additional investment, the existing online platform could be moved into UGS. Alternately, data collection could be established locally, as has been done successfully by Christopher Blattman and colleagues within UGS, 26 but the lack of portability of those resources suggests that such approaches are more study-specific than infrastructure.
One intriguing possibility, which could be addressed through social science infrastructure, is whether policy can be selected and managed through randomized controlled experiments (as with clinical interventions). For example, researchers could study the effects of different food aid program designs on social and political instability.
As noted, human behavioral experiments often address internal validity, but they often have questionable external validity. Games might exist as a sort of middle ground between the two types of validity by bringing interacting people together in a synthetic environment; at best, they provide more external validity than a traditional lab environment by replicating more of the real-world decision environment while still allowing a degree of control for internal validity. 27 However, many games are not designed with formal concerns about validity in mind. 28 Furthermore, many games rely on elite samples and a great deal of customization, which makes it unclear how such an approach would scale.
Expanding the TESS approach to provide behavioral experimental infrastructure would have the following advantages:
• Centralized and subsidized testbeds could reduce cost and barriers to entry for diverse research teams. • In cases with a clear value proposition, standardized infrastructure could be designed to specifically support multiple, specialized subject populations (e.g., elite professionals, geographically specified) for more-realistic tests.
A related concern is the inability to observe counterfactuals within naturalistic environments. In isolation, the occurrence of an undesirable outcome after a choice (e.g., getting the flu after either getting or not getting vaccinated) argues for switching behavior, regardless of the behavior. In contrast, the occurrence of a desirable outcome (e.g., not getting the flu) provides evidence for staying the course.
In life, we rarely get to experience the counterfactual-and, unlike with the flu, we often experience a given risk just once, which prevents learning how outcomes follow probabilistically from actions. Such decision contexts include risk-related choices regarding health (e.g., choice of cancer treatment), economics (e.g., federal revisions to monetary policy), and security (e.g., responses to sensor alarms). In the real world, we are stuck in our own time line.
Modeling and simulation (M&S) provide an avenue for observing distributions of outcomes, contingent on behaviors, to help grasp this counterfactual problem while taking into account many real-world complexities. Adaptive behavior, collective group dynamics, and social interactions can be modeled at both population and individual levels. However, M&S suffers from several challenges. Just as the survey and experimental research community tends to be siloed, so is the M&S research community, resulting in redundancy and lack of integration. Unlike behavioral experiments, simulation dynamics can be hard to observe. Even well-documented code can be unapproachable to all but the most sophisticated and determined of audiences. Finally, simulations are often hampered by ad hoc synthetic worlds and assumptions that limit both internal and external validity. Big investments are needed in micro-level data sets to provide external validation of model states and dynamics and provide confidence that existing models and data can be used when needs are dictated by current events.
M&S testbeds require a suite of computing, modeling, and data resources, but these resources are rapidly growing in exciting directions. 29 However, individual researchers and teams have differing levels of access to these resources, which limits scientific progress. Social science infrastructure could account for common fixed costs and provide standard and reusable building blocks (e.g., computing, software, tools) to make M&S faster and cheaper. Leveraging these building blocks against many projects would reduce the cost per project while pro- 29 See Chapter Sixteen (Robert L. Axtell, "Short-Term Opportunities, Medium-Run Bottlenecks, and Long-Time Barriers to Progress in the Evolution of an Agent-Based Social Science," in Aaron B. Frank and Elizabeth M. Bartels, eds., Adaptive Engagement for Undergoverned Spaces: Concepts, Challenges, and Prospects for New Approaches, Santa Monica, Calif.: RAND Corporation, RR-A1275-1, 2022). viding more-robust and better resources (akin to TESS's ability to leverage a large probabilitybased sample rather than convenience samples).
Such resources are becoming more common, but they need to be expanded and become more widely available. For example, the National Dynamics and Simulation Science Laboratory at Virginia Tech generated a realistic synthetic population of a social network representing the full population of Portland, Oregon. For an agent-based model representing a population of people, this provides a much more realistic basis for modeling social interaction (e.g., compared with common simpler and stylized network structures). Such a data set could provide one resource for producing a simulation testbed that provided programming modeling tools, computing resources, and visualization and analysis capabilities to a broad variety of users.
An emphasis on broad application could also push such tools to be more flexible. Such broad applications could involve taking the model of Portland and translating it to a model of, for example, Lagos, Nigeria. Such infrastructure would also increase incentives to invest in transparency and documentation-addressing concerns, particularly among nonmodelers, about interpretability of results.
An M&S testbed would have the following advantages:
• Researchers could plan for model needs ahead of time or tap into the resources on an as-needed basis, without standing up basic building blocks each time. • Hypotheses could be refined using formal models, with efficient consideration of alternate structures and parameters. • Methodological stress testing could be done by simulating research designs for data collection and inferential problems, as could be done within the Defense Advanced Research Projects Agency's Ground Truth program. • Models could be aligned with empirical approaches by first identifying promising interventions (either because they work or because they provide counterintuitive results in simulations), increasing the expected benefits of real-world experiments.
As noted by Robert L. Axtell in Chapter Sixteen, 30 the feasibility of more-sophisticated arrays of models and simulations (for example, through parallel processing) is increasing, with many exciting prospects on the horizon. This has great potential, if models of human dynamics within UGS can be brought to bear, potentially in tandem with real-world experimental studies (either in person or online).
Independent of the social science infrastructures chosen, infrastructure in general adds value, as I discuss next.
Perhaps the clearest case for investing in social science infrastructure is to centralize and establish common, core capabilities for implementing social science in quickly evolving situations. By foreseeing the need and putting in place resources for empirical data collection (e.g., instruments; data sources; tools for data integration, analysis, and visualization; mechanisms for quick institutional review board review) and M&S (e.g., data storage; computing capacity; basic, modifiable agent-based models; synthetic populations), the social science community can make the research process quicker, more efficient, and more cost-effective. Within empirical research on disruptive events, there is a critical need for planned pre-event baseline data that can be used as the basis for pre-post comparisons (which dramatically improves the ability to draw causal inferences) and for appropriate contextual data to account for situational factors that could moderate change. As argued earlier, research teams must typically build these capabilities de novo, rely on less-than-ideal comparison data (or simply do without), and create unnecessary, redundant efforts.
Such infrastructure also has the potential for training and technical support. It reduces cost of entry for less-resourced researchers and practitioners, who have been systematically excluded from many high-stakes research opportunities. Finally, resulting data and models could be made publicly available for additional users at no or limited cost.
Such infrastructure can be the basis for broader insight on many of the most vexing problems revealed through disasters and other crises, such as maladaptive risk behavior, population displacement, social and economic disparities in crisis impacts, and the societal disruptions that result from these impacts. To have the most benefit and involve the broadest set of users and stakeholders, such infrastructure should incorporate the concepts of interdisciplinarity, multidisciplinarity, and convergence. 
31
Such infrastructure should be designed to test links across traditional disciplinary concepts-linking psychological (e.g., mental models for disaster risk), economic (e.g., incentives and constraints), and anthropological phenomena (e.g., how shared beliefs organize cul-tures) with physical and built environment systems (e.g., epidemiological dynamics, climate models). The results of such inquiry can motivate and test more-sophisticated theories, such as how risk materializes at different levels, with dynamic feedback in complex environments. The collaboration across disciplines also can increase measurement quality-especially in UGS where sensing might be harder-by incorporating multiple perspectives and methods.
For maximum benefit, social science infrastructure should facilitate bringing together many types of data, on multiple scales and across diverse contexts, and do so in a planned fashion that creates both access and utility for many types of users. Figure 
8
Direct empirical data are collected for a specific research purpose from real-world sources. They offer focused measures, experimental control, and targeted samples, but they do so typically at a high cost per observation. Indirect empirical data, such as secondary and passively collected data, are often naturally occurring. Such data contain many available observations and wide intertemporal or geospatial coverage, but researchers have limited control over the timing and form of measurement. Simulated data are purposely generated through models presumed to reflect the real world or to illuminate specific real-world processes (e.g., an agent-based model could shed light on diffusion processes, even if the networks are clearly artificial). They use explicit causal structures and allow for fluid policy experimentation and observable macro dynamics, but they are at their heart synthetic and often difficult to visualize in the micro sense. 
Micro dynamics
Generated purposefully from real-world phenomena for a defined scientific use
Generated by other processes and accessed for current scientific purpose
Generated purposefully via theoretical or statistical models presumed to reflect the real world
As shown in Figure 
8
Several projects conducted at RAND exemplify this approach, informing agent-based simulation models (of breast cancer and mammography, influenza and vaccination, and taxation and tax evasion) using targeted national surveys along with existing secondary data sets. The goal is to use empirical data sources where they are the best (or only) sources for defining key model features. This results in models with unusually informed parameters, allowing key policy insights. 
32
33
34
Finally, robust stakeholder engagement should ensure robustness to diverse research questions, questioners, and motivating circumstances. The infrastructure should be capable of contributing to basic science and translational research. Infrastructure also needs to have the capacity to adapt to emerging events, taking advantage of surprises rather than itself being disrupted by them. Accordingly, stakeholders should assess ongoing and unmet stakeholder needs (such as scientific, practitioner, and policy needs) and develop best practices and opportunities to build robustness in approach.
A strong evaluation component, built in from the start, can keep this use-inspired and adaptive focus. By incorporating an action logic model for effective evaluation and adaptation, infrastructure management can specify key decisions, contextual factors, and desired outcomes and use these to design activities to address user needs. 40   
Realizing the advantages of social science infrastructure requires overcoming the following three challenges.
Infrastructure funding has typically been focused on the physical sciences and engineering rather than the social sciences. For example, of NSF's ten awards for midscale science infra- 36 Natural Hazards Center, University of Colorado Boulder, "CONVERGE," webpage, undated-a. 37 DesignSafe, homepage, undated. 38 Cybersecurity and Infrastructure Security Agency, "National Infrastructure Simulation and Analysis Center," webpage, undated. 39 MIDAS, homepage, undated. structure in its last round of funding (which totaled just more than $116 million over five years), nine were awarded within the physical sciences and one within computer science. 41  No awards were within the social, behavioral, or economic sciences (SBE)-a lack of prioritization that also is reflected across all research support at NSF (Figure 
8
An advantage that many physical sciences have over social sciences is the existence of unifying theories for which there is general agreement within and across fields. This makes the motivation and implementation of large-scale research infrastructure more straightforward. In contrast, social and behavioral sciences have a remarkable lack of consensus in theory, data, and interpretation-a challenge that exists both within and across social science fields. In this sense, it is harder for a given infrastructure project to serve a variety of constituencies equally well. Such projects as the long-standing Health and Retirement Study have addressed this by explicitly engaging a variety of stakeholders, such as scientists from economics, sociology, psychology, and anthropology. But typically, even these efforts are dominated by specific disciplines and funding priorities. That said, such examples speak to an opportunity here for the infrastructure itself to force discussions across disciplines and perspectives. This could 44 Gulf of Mexico Research Initiative, homepage, undated. 45 Cutter, Boruff, and Shirley, 2003; Norris et al., 2008.   46 Melissa L. Finucane, Aaron Clark-Ginsberg, Andrew M. Parker, Alejandro U. Becerra-Ornelas, Noreen Clancy, Rajeev Ramchand, Tim Slack, Vanessa Parks, Lynsay Ayer, Amanda F. Edelman, Elizabeth L. Petrun Sayers, Shanthi Nataraj, Craig A. Bond, Amy E. Lesen, Regardt J. Ferreira, Leah Drakeford, Jacqueline Fiore, Margaret M. Weden, K. Brent Venable, and A. Barrie Black, Building Community Resilience to Large Oil Spills: Findings and Recommendations from a Synthesis of Research on the Mental Health, Economic, and Community Distress Associated with the Deepwater Horizon Oil Spill, Santa Monica, Calif.: RAND Corporation, RR-A409-1, July 6, 2020. 47 Hunter Heyck, Age of System: Understanding the Development of Modern Social Science, Baltimore, Md.: Johns Hopkins University Press, 2015. involve discussion of scope, research design, instrumentation, analytic capabilities, and oversight mechanisms. In this sense, such infrastructure may itself be a mechanism for promoting overarching theoretical and methodological commonality. It is also an opportunity for fields to capitalize on advances and capabilities from other fields, such as psychology's emphasis on measurement (through psychometrics) and economics' emphasis on analysis causality through quasi-experimental and observational designs (through econometrics).
Promoting Effective Management, Adaptation to Emerging Trends, and Sustainability
One potential criticism of large infrastructure investment is that the substantial cost can create psychological and political escalation of commitment, which can lead to entrenchment and resistance to change. Flexibility and ability to adapt to new and emerging trends is critical for the value of long-term infrastructure, and, although standardization (e.g., of variables, of models) can facilitate comparison, regular review should attend to the risk of stagnation, especially when the focus is on the wrong things. Robust stakeholder engagement and active evaluation are both safeguards in ensuring adaptiveness and alignment with evolving needs. A robust and user-facing evaluation component is equally critical, which should use welldefined evaluation criteria.
Sustainable funding is another possible stumbling block. Importantly, some funders might be willing to fund implementation of new infrastructure but might balk at the prospect of bearing the cost to maintain that infrastructure over time. Therefore, proposers of infrastructure funding should come to the table with concrete plans for sustained funding. A broad coalition of stakeholders can create a more robust funding environment. This could involve researchers supporting infrastructure in competitive grant applications designed to use that infrastructure, universities and research societies investing in strategic collaborations, and federal agencies addressing their core missions. For example, the Federal Emergency Management Agency, Centers for Disease Control and Prevention, and National Oceanic and Atmospheric Administration could collaborate for disaster research. It could also involve strategic partnerships with other facilities (e.g., supercomputer centers, big data hubs). The U.S. Department of Defense has existing models for developing and transitioning basic research (e.g., Collaborative Technology Alliances within the Army), which could leverage such infrastructures. These infrastructures could also be aligned with the National Defense Strategy; for example, regional or domain components could be built through a Central Command country survey panel. Proposers and funds should also consider novel business models, involving academia, business, nonprofits, and government. Subscription services could provide the media with fluid access to data summaries. Specific products could be designed for local, state, and federal partners. Derivative products could be commercialized through partnerships with businesses.
Proposers and funders should both be aware that the focus inspired by a specific crisis will wane with time. We saw this following the H1N1 pandemic: Research and prepared-ness investments initially surged during the pandemic but declined as time progressed. As a result, readiness for COVID-19 was severely limited; maintaining this readiness for future crises will be a key challenge.
Research, as a whole, is typically slow and deliberative. This deliberative pace is particularly striking when researchers are trying to marshal resources and funding in response to disasters and other emerging events. Existing funding opportunities, such as rapid grant mechanisms, offer only a limited solution. Understanding the impact of such events requires suitable comparison data, such as pre-event baselines, but rapid grants are by their very nature reactive to events (rather than prospective). This lack of existing resources and nimbleness severely limits scientists' ability to learn from surprises.
The physical sciences and engineering have partly addressed these challenges through investments in observatories, sensing networks, and other standing research infrastructure, which provide them with resources to identify impacts more effectively. There is a need within SBE for similar infrastructure investments to tackle some of the most vexing scientific and societal problems that result from complex and disruptive events.
In 2005, Defense Advanced Research Projects Agency (DARPA) Director Tony Tether approved a novel program: Pre-Conflict Anticipation and Shaping (PCAS). What made this program novel was that it was among very few explicitly social science-focused DARPA programs since the premature termination of Project Camelot-an ambitious Cold War social science program to study social processes associated with social and political upheaval or destabilization-in the early 1960s. 
1
In this chapter, I offer answers to these questions. First, after briefly discussing PCAS in the next section, I make explicit some fundamental, but not insurmountable, obstacles to the serious study of the social world. I focus on the value of a positivist social science approach, not because it is all that is needed to advance social science, but because this social science tradition has largely been overlooked by and excluded from recent investments in advanced research programs. The need for social science is recognized by funding organizations, as is the need to formalize or quantify social science to scale to meet the needs of the U.S. government. However, what seems to go unrecognized is that the social science traditions include validated approaches for the quantitative, formal, empirically based study of the social world and that there are hundreds of social scientists trained explicitly in these methods. These researchers, just like their qualitative social scientist colleagues, are trained in theory and substantive areas, as well as in the mathematical and computational skills used today by scientists in other disciplines. The difference is that these social scientists are trained specifically to capture and analyze social phenomena using these formal tools and techniques. They are also well trained to engage with their qualitative counterparts and with researchers from other fields. However, this type of specially honed expertise is underutilized, and this underutilization is detrimental to advancing social science innovation and application. As a final thought pertaining to this discussion, quantitative social scientists should be sought after as leads on advanced research programs that specifically seek to advance theoretical and practical applications of social science.
The next section lists some of these challenges. I argue that progress and precision in social science is lagging because social science requires that people study themselves; doing so presents specific challenges and requires coordination and tools that have not yet been invented. Making major advances in social science would require a way to observe, record, coalesce, organize, and process the right data. Determining how to make these advances will require serious consideration about what the study of people requires.
After discussing these challenges of social science research, in the next section I present a very high-level summary of the goals and processes of the social science research process as it stands today. This discussion makes explicit how the challenges of studying social phenomena prompted social science researchers to invent a host of novel approaches for observing and recording the social world from many different perspectives. Some of these approaches do not conform to the norms of scientific inquiry; however, operating within the familiar structure of the scientific method, these approaches nevertheless produce validated general theories.
Finally, I conclude with recommendations for structuring research and programs to advance methods for studying the social world and identifying the most-essential and mostfeasible as-yet unanswered social science questions. These preliminary questions will need to be addressed to enable the eventual answering of more-vexing and more-ambitious questions and better integrate knowledge of the social world into the operations of the National Security Enterprise (NSE).
The specifics of the science advanced by PCAS are published elsewhere. 
3
At the start of PCAS, research teams were selected to represent the broad scope of social science disciplines; the teams consisted of political scientists, anthropologists, psychologists, economists, and sociologists along with mathematicians, computer scientists, and engineers. Ten interdisciplinary social science teams were given six months to develop causal models to characterize the stability of two Pacific Rim countries.
Each team brought a different approach toward the study of social phenomena to the program. During the first six months, teams shared data and insights in a competitivecooperative environment. They then proposed theoretical causal "predictions" about the stability of these two countries six months later. The PCAS modelers submitted predicted outcomes and their causal models, which showed how increases in specific variables would precede increases or decreases in the ultimate variables of interest-state stability and political unrest. At the end of the year, the program manager briefed out the effort, comparing the teams' predictions with the state of the real world.
One of the countries had experienced upheaval, which had been predicted by the social science models but otherwise unforeseen by most regional experts. The other country, which many political experts had worried would become more unstable, did not change much-as the models had predicted. Demonstrating that social variables could be observed and measured and that an association between a change in one and a change in a second was possible convinced Tether that social phenomena could be studied using the scientific method and that an investment in a social science program should move forward.
PCAS's objective was not to advance science or technology but to answer a fundamental epistemological question: Is there science in social science? In 2005, social science was so alien to DoD that research and development (R&D) leadership had to be convinced that it is possible for people (e.g., social scientists) to observe and measure relationships among social variables in much the same way that physicists can observe and measure relationships among physical variables. I was involved in PCAS from its inception through its conclusion, and I was in the room when the program manager pitched PCAS and briefed the findings. The outcome of PCAS was not a surprise to me: As a mathematical sociologist trained to formalize social science theories and quantify complex and often qualitative constructs, I took for granted that the social world could be measured and modeled. It was only once I became involved in the world of scientific research funding that I discovered two contradictory and dominant opinions about social science that pervaded the research-funding community. It seemed that one half of the program managers I spoke with believed that the social world was beyond scientific exploration-that is, that social phenomena were so special, unique, diverse, and random that any attempt to make sense of them was folly. The other half seemed to think that the phenomena were not only amenable to study but also simple, or soft, and that all that was needed to understand the social world was to put really smart scientists-not social scientists-on the task.
To the few aware of the program, PCAS showed that neither opinion was true. Rather, PCAS revealed conclusively that social phenomena can be studied scientifically, but it does not mean that it is easy to do. Studying people offers many challenges that, although not unique to social science, are exacerbated by the nature of the phenomena. Therefore, it seems as though the social sciences have advanced at a slower pace than the physical or biological sciences. Even so, the main conclusion drawn from PCAS was that social scientists have identified and can somewhat measure some important variables that indicate or foretell the future state (condition) of a human group (state, region, community, interest group). Social and political outcomes are neither completely random nor so sensitive to initial conditions that any prediction is impossible.
PCAS showed that well-scoped social science questions are answerable using the framework of positivism familiar to physical and life scientists. There are patterns to observe and measure in the social world, similar to patterns in the physical world and biology, and social scientists, using tools available today, can glimpse and describe these patterns. PCAS also revealed that the theoretical relationships among key variables and the tools to measure the variables were far from precise. Science requires precision, and social science has a long way to go to perfect its tools; the laws of human behavior are more variable than the laws of physics. The reason for this lack of precision is not that there are no regularities, patterns, or functional relationships in the phenomena, nor is it that those who study social science, unlike those who study physical or biological science, are bad at modeling and measurement. Yet the comparative lack of precision of social science has somehow been interpreted to mean that hard-science methods are better than social science methods for observing and measuring social science phenomena. This is an absurd conclusion because what requires measurement is so fundamentally different. There is no basis to assert that what works to measure the physical would be appropriate for measuring the social.
Social scientists have made progress in developing some very dependable general models. For instance, social scientists know that economic decline very often precedes civil unrest, even though they cannot provide a single general equation that relates the two with the preci-sion of a chemist's gas laws. A social scientist can say which factors or variables are proportional or inversely proportional, but what is known is not sufficient to formulate trustworthy equations. To discover these laws of social science would require the ability to study the relationship among selected elements of the system in a closed or controlled system, such as the laboratory or a petri dish. This level of experimental control was required to make essential breakthroughs in discovering the laws of physical and biological phenomena. However, it is hard to keep exogenous factors from creeping into studies of the social world. Social scientists have not yet determined how to bound their models to ensure that all relevant factors are present in any particular model. Consequently, the existence or value of social science constants, if they exist, is still undiscovered; therefore, the precise formalization of social science principles is not yet possible.
A good first step to tackling challenges in social science research is to enumerate them. What follows is a first attempt at classifying the main challenges to studying social phenomena. Not all of these classes of challenges are unique to the study of people; several have analogues in other disciplines. When this is the case, I highlight the similarity to illustrate examples of how the challenge could be addressed.
One challenge unique to the study of humans is that humans think they know and understand themselves; as a result, the systematic study of humans is undervalued. The most important thing to remember when studying people is that being a person does not provide any insight or benefit toward understanding phenomena involving humans. Quite the contrary-people observe the world only from their own perspectives, and, although they have opinions about how the social world works, those opinions are neither science nor knowledge.
For example, a citizen of India (or of another country) who is a social or political scientist who studies Indian politics is an expert on Indian politics; a person who is solely a citizen of India, however, is not. The Indian citizen is a data point. The social or political scientist gained their expertise by reading the research of others, collecting data, and doing analyses. Whether that expert is an Indian citizen does not matter. The citizen part of the scientist is also just a data point. People do not know any more about people, what they believe, or how they work because they are also people than they understand quantum physics because they are made up of matter. Expertise comes from study.
Yet the atomic physicist has credibility because he or she studies physics, whereas the social scientist often does not have credibility because lay people or hard scientists who attempt to study social phenomena think they have expertise because of their lived human experience. Physical or biological scientists would never generalize from an n of 1; similarly, we should not generalize from ourselves.
The most unique and glaring challenge of studying people, or of people studying people, is objectivity. We care differently about the outcome or finding when we study people than when we study atoms or plants. Even when we study people, groups, or cultures we do not identify with, it is hard to remain objective and it is hard to recognize our biases. Social scientists must consciously remind themselves of their biases; therefore, social scientists have incorporated elements in their methodological approaches that control for the effect that their presence might have on their ability to objectively observe and record.
In most studies, people are not directly studying themselves; most of the time, they are studying an "other." This introduces a challenge related to perspective and prioritization. When studying this other, one's focus tends toward what is new or interesting about the other. Usually, that moves the focus on differences from one's self as a point of reference. What lay people find interesting and report is novelty, which is influenced by their own experience. This is unavoidable-it is human nature-but only by being explicitly aware of this tendency is it possible to create methods and tools to broaden the aperture.
The mundane, the commonalities, the regularities, and the unexceptional are what really hold together societies, but they go unobserved. Consider the story of a Western visitor to China who witnessed a wedding. Upon the visitor's return, they report with astonishment to their friends that the bride wore a red, not a white, wedding gown. What is really astonishing is that halfway around the globe two young people were matched and united in a ceremony. The ceremony was public. Friends and family attended. It was full of ritual. There was a feast and dancing. The bride wore a special outfit, and it had a special color. All of these instances are identical to customs back home-and all of these instances go unstated. What is reported are the differences: The groom does not smash cake into the face of the bride, and the bride wears red instead of white. The conclusion drawn is that cultures are so different, but the story reveals striking similarity.
To measure and quantify regularities across context requires recognizing and formalizing social regularities, but those are not interesting. The consequence is that a large portion of reported observation, and a large portion of funding for social science research, is focused on the exceptions. If the goal is to reveal the hidden laws of social science, then it is important to notice the rules and not focus only on the exceptions. However, for people who are studying other people, the rules are often overlooked and not reported.
Will the grand American experiment ultimately be successful? No single person can answer this question because the experiment is not over and will outlast anyone alive today. This example underscores another challenge for social scientists: They live in the same space and time as their objects of study. However, awareness of bias is not always enough to mediate its effects. This is especially problematic for studying people at a geopolitical scale. The causeand-effect cycle predates and can outlive the scientist. Another example is our 21st-century perspective on the two world wars. In the 1920s, World War I was considered over. Social scientists studying that war in 1925 would consider it a historical event, but most historians today consider the two world wars as a single protracted conflict.
This problem is not unique to social science. Climate science offers similar challenges about time and scale, and, much like with social science, advances in climate science have suffered because humans are invested in the interpretations of the scientific results. Even so, whatever challenges that climate scientists face are even tougher in the social sciences. Data on human activity are more ephemeral and often do not leave clear physical traces. The archeological record is more prone to interpretation than the geological record (which also does not provide clear-cut answers). Prehistory is a mystery, and history (i.e., "his"-the winner's-"story") is biased in that, at best, it is incomplete. This timescale challenge relates to the perspective challenge. What people think is interesting or salient is often temporally proximal. The result is that a great deal of study is focused on occurrences within a very short time span that may or may not be representative of the phenomena more generally. Attention is focused on small differences between specific examples rather than on the structural similarities that events through the ages share.
This timescale issue is especially true for research enterprises focused on meeting shortterm, applied needs of government or defense agencies. Within the NSE, the lines between social science, policy, and intelligence analysis are blurry at best, even though these fields have different objectives. One possible driver of this confusion might be that many intelligence and policy analysts have degrees in social science subjects and use data collection methods and analytic techniques developed for social science purposes. The result of this is the misconception that the product of social science should reveal short-term, specific actionable insights. That is not the case-again, social science is a collection of theories and methods, not factoids. The social scientist seeks to understand the underlying structures and pervasive patterns of behavior, especially differences that stem from location, timing, age, and "culture." 
4
Social change may be too slow (i.e., months, years, decades) to study in the lifetime of one social scientist, but it does occur faster than evolutionary change. Designing an experiment to understand social change is difficult, and perhaps impossible, because the study (or the presence of the observer) can alter the behavior of those being studied. Biological systems often adapt quickly to changes and interventions, and these adaptations are measurable in a condensed time frame. A scientist who is analyzing the adaptation of bacterial colonies, even on the genetic level, can easily acquire data and make a clear comparison with an untreated bacterial colony. However, studying people is more complicated: It is not always possible to find a control group for comparison, and introducing change into social systems requires special care in protecting subjects to ensure that no harm is done. This is especially true for macrolevel studies, in which an intervention can generate second-and third-order unintended consequences. In many cases, introducing interventions or treatments is impossible because even small interventions can have huge impacts on large segments of populations, which leads to restrictions on studies that affect people or can cause harm.
The challenges highlighted here come together when attempting to determine the criteria for evaluating the success of a social science project. It is tempting to observe a situation and attribute an outcome to an intervention or a change in the status quo, but social scientists cannot draw that conclusion. Research is most often inconclusive if it is focused on the results of a geopolitical event because researchers only have a single example to study. Any confirmatory findings might be coincidence-and a negative result may be a false negative. A large sample of examples is necessary for the systematic testing necessary to demonstrate that a certain social science model is valid. However, this sort of testing is difficult to do when a phenomenon rarely occurs.
Finally, a challenge that is unique to social science that actively hinders progress is the ease of critiquing social sciences. The underdevelopment of social sciences presents challenges that scientists in other fields in a similar state of development did not have to deal with. Much of the early progress in the physical sciences was made using methods that today would be considered crude or poorly developed; today, all sorts of restrictions exist thanks to lessons learned from the mistakes of this early work. In addition, early physicists and astronomers made measurements, formulated theories, and tested them in their laboratories without worrying that their theories might not generalize. Boyle, Faraday, Kepler, and Newton were not worried about limitations of their work from later developments by Heidegger, Einstein, or Feigenbaum.
However, at this point, "everyone is a critic." Social science tends to be less brave in its assertions and in reporting findings. Social scientists know that a study done on one population might not generalize or that a model might have only appeared to have worked because a key variable was not present in the cases studied. Social scientists are well aware that whatever they find is likely to eventually be improved, so they are very careful to couch their claims conservatively and focus only on their fields of study. 5 This tendency toward modest and bounded claims makes the discovery of general rules less likely.
Although the awareness of people's limitations in studying other people can be a liability that slows progress in social science, that awareness is also valuable. Because social science practitioners know about these liabilities, they are trained in methods to mediate their effects. In the next section, we review some of the methods that social scientists have developed to limit bias and make progress given these challenges.
Misconceptions about how social scientists go about learning about the social world have led to people untrained in social science research methods to question whether social science is science. Many aspects of social science differ from the way most physical and biological scientists conceive of scientific inquiry. Specifically, from the outside, it appears as if social scientists do not often use the main tool for scientific inquiry-the scientific method. This is not the case. The scientific method is an integral part of the social science research enterprise; it is just that social scientists have adapted the method to meet challenges specific to their field. Engagement in the study of social phenomena has taught social scientists that adopting the physical or biological methodological paradigm to address the complexities of social science is too limiting. Instead, they have adapted and extended the paradigm, seemingly in ways that some scientists from other fields do not recognize as science.
Most concepts fundamental to social science inquiry are not amenable to direct measurement or to laboratory studies, so social scientists have spent over a century developing techniques, rules, and conventions to describe and understand the social world from observation done in settings less controlled than laboratories. But the scientific method is still at the core of the investigation. For social scientists, like physical or life scientists, the scientific method consists of the coupled processes of induction-building theory from systematic observation-and deduction-testing the theory. For social science, not unlike astronomy and ethology (the study of animal behavior), data used for theory building most often are collected from observations that occur in natural settings outside the controlled environment of a laboratory. A great deal of information that inspired prevalent theories about the social world has been collected in a manner that appears less systematic than methods of collection for other sciences.
Qualitative research is a necessary and important element of the social science endeavor, but it is not all there is. If qualitative or descriptive characterizations of social phenomena were all that social science provided, then criticism that social science is not science would be reasonable because the work would only be inductive, and science requires both induction and deduction.
However, induction is not all that social scientists do. Social scientists devise ways to test and retest their theories, just as astronomers and ethologists do. In social science, as with these other sciences, only theories that can withstand repeated empirical scrutiny persist. To be clear, rich qualitative research is necessary to build theory and should not be devalued as an important part of the social science process. Still, testing these theories also is a necessary part of the overall process.
Testing social theory requires carefully measuring social variables, but this is difficult because social science phenomena differ from those of other sciences in two important ways:
(1) variables are usually not directly measurable and ( 
2
Gaining meaningful insights about the social world and finding generalizable rules about human behavior require the development of innovative techniques to observe and record behavior; some of these techniques are unique to social science (i.e., interviews, survey research, content analysis), others have analogues in other fields (i.e., observational methods, network analysis), and some were developed for social science uses but have been adapted widely for use in other fields of science (i.e., sampling, statistics).
Many social scientists are trained in methods that scarcely resemble the positivist tradition that dominates other branches of natural science. For instance, historians and ethnographers can conduct and conclude their study and neither articulate nor test a theory, leading some to think that the work is not scientific. These researchers obtain their knowledge by reading and synthesizing the studies done by others and by spending a great deal of time ensconced in the subject matter, observing and learning about the one subject. There is no expectation that these approaches require a positivist frame.
Moving beyond the constraints of positivism expands the potential for high-quality understanding and deep insights. Not only is the accumulation of this type of knowledge useful, sought after, and appreciated in the social sciences, a deep dive into the nuance and detail of a subject may be necessary to recognize new insights and to generate new and useful theories for many subject areas. Here, the issue is that this one study alone is out of context. However, from the perspective of social science writ large, the contribution of one study-a precise description and deep understanding of a specific case-is essential to the larger endeavor. These studies generate insights that inform and refine theory and data that can be used, perhaps by others, to test theory. That the one method used alone does not seem scientific misses the point: Each study is just one part of the collective enterprise that is social science inquiry, which at its core relies on the scientific method, a structure for supporting, rejecting, and refining theoretical assertions.
In contrast, other social scientists fullheartedly embrace the scientific method within their work. These researchers see their objective explicitly as formulating and testing theory, although most recognize that their contribution may not span formulating and testing theory. These social science researchers see the inductive-deductive loop-a process of stating, testing, and then refining and retesting theory-as central to the social science research process. 
6
The reliance of the social sciences on the scientific method is not obvious just from looking at a single study, and many social scientists will insist that they do not subscribe to the positivist approach. Regardless of intent, however, these researchers contribute to a cumulative process that does conform to the basic tenets of science. The data they collect feed the beast. The qualitative findings help refine theory and broaden the pool of examples to use in analyses. As long as there are also some researchers who test theory, social science as a science can advance. Fortunately, each social science discipline involves, and perhaps is dominated by, researchers specifically trained to measure social science concepts and formalize social theory. These experts in computational and formal modeling and statistics are also experts in social science theory and in knowing how best to study and model the social world.
As mentioned, a great deal of what is interesting to humans about humans is not amenable to traditional scientific approaches, but it would be tragic if the only way to learn anything about humans were through controlled laboratory studies. Some types of research must be done in situ. That is the value of rich, inductive qualitative case studies. Just as deductive studies can validate what a qualitative observer reports, inductive studies are needed to validate deductive work that is too distilled and sanitized to be realistic. In addition to benefiting from the collective nature of social science, researchers began developing techniques that allow hypothesis-testing in natural settings. These techniques, which largely fall under the label of quasi-experiments, allow researchers to hypothesis-test their assumptions about causal social science relationships while minimizing interference with the natural behavior of the population under investigation.
The deductive process in social science is reductionist, which has positive and negative aspects. Distilling complex abstract concepts into specific measurable examples (indicators) to devise a test is done at a cost: losing the depth or nuance of the original concept. However, it is important to remember that deductive studies do not really test theory; they only test hypotheses (specific instantiations of the theory). It is only after repeated tests of a theory, in different ways and contexts, that the findings might generalize. This is why it is important to find the right specifics to test or to properly specify the model. It is also why it is important to conceive of social research as a cumulative endeavor and not the findings of a single study.
As in the hard sciences, the experiment is considered the gold standard for demonstrating cause and effect. If, however, the only legitimate means to learn about the world is by using laboratory experiments, very little would be known, or knowable, about the social world. The experimental model is so strongly associated with deductive research that there is a tendency for people to think that studies that are not structured as experiments do not lend themselves to theory testing. Unfortunately, as mentioned before, social science phenomena are hard to capture and study in a laboratory. Attempting to study complex societal interactions under experimental conditions would force what is observed into a state of artificiality unlikely to resemble what occurs in the real world. A study of this type would be thought to lack external validity. This is a general criticism of almost all laboratory work in social science. On the other hand, any move to relax any feature of an experiment introduces threats to internal validity, confidence that the observed or deliberate change in one variable is the cause of an associated change in a second variable. The challenge to social scientists is to design studies that balance internal and external validity.
In their seminal treatment of this topic, 
Campbell and Stanley (1963)
For example, this framework made explicit the threat to internal validity of conducting research to study the effect of a change in status quo in the absence of a control group when it is not possible to find a control group. Rather than not run a study because the conditions are not perfect, the study could proceed-with the researchers cognizant of the limitations. Another instance might present the opportunity for a control group, but with the caveat that 7 Donald T. Campbell and Julian C. 
Stanley, Experimental and Quasi-Experimental Designs for Research, Boston, Mass.: Houghton Mifflin, 1963.
Campbell and Stanley did not intend to exalt the RCT above other methods. On the contrary, the point of their treatise is to reveal the similarities and complementarities among very different designs by deconstructing social science research into a set of shared elements. 8 This decomposition reveals how different designs relate to one another and offers researchers a scheme within which to classify any particular study. The objective was not to discourage the use of the broad variety of designs but to encourage the informed use of quasi-experimental methods.
In recent years, explicitly multi-or mixed-method approaches have gained favor, but this is a relatively new development. In the past, and for most research today, social science relies on a division of labor. Each researcher carves out both a substantive and methodological niche within which to focus and contribute. As for substance, most researchers only focus on a very narrow topic that interests them, but, even in the investigation of a very narrow topic area, the phenomenon under investigation is so complex and humans are so limited and biased that the only way to begin to understand the topic is by using many methods, in many contexts, over time. Despite this recognition that social science is a collective endeavor, for the most part, the primary mechanism for integration of different studies is ad hoc. One researcher reads the work of another, is inspired, and fills in a gap. Until recently, literature reviews and discussion sections in journal articles were the most common means to sew together a body of research. It is only recently that formal meta-analytical approaches have provided the means to formally compare and aggregate studies. Therefore, it is not surprising that for people outside the social science research community it appears that each study stands alone; the prominence of deduction and hypothesis-testing in social science research is not obvious because most of the social science studies best known to the general public are only inductive.
To discover regularities and general principles shared across human engagements is a mammoth task. Social research explicitly recommends repeated grinding through the inductive-deductive loop. No single inductive, deductive, or mixed-method study is sufficient for generating sustainable generalizable insights because the social world is full of variance. In social science research, to draw conclusions requires more than the type of experi-mentation and replication used in science. Just as clinical trials with human subjects, not just laboratory experiments, are needed to prescribe a drug, multiple complementary methods are needed to understand how the cause-and-effect relationship observed in a laboratory translates to the more complex system.
For the social sciences, even more repetition and reproduction are required to draw a generalizable conclusion. It is not enough to rerun the same study under identical conditions; for social science phenomena, this is usually impossible, but, even if it were possible, the findings would be too idiosyncratic. Unlike gas law experiments, which can be replicated anywhere at any time to give fairly identical results with each experimental run, social science research requires replicating the main effect regardless of context; if that is not possible, it requires identifying what factors are the differentiators to make more precise the scope conditions (boundaries on the studies, caveats, and controls), which, if met, do produce the expected result. The goal is to replicate, as much as possible, what the scientists can control and, rather than expecting identical results, noting which factors differed. In this way, the approach is similar to the gas law example insofar as the results of an experiment performed in Nepal versus Death Valley might differ and noting what factors differed was informative to theory development. To find general principles in social science that transcend a single case replication implies looking for similar findings using different approaches to measurement of the same concepts in a multitude of contexts.
Social science-like other fields of science-is not a set of facts about the way things work. Instead, it is a method for finding the facts. The method requires the feedback loop between induction and deduction. The method also requires replication under many conditions. It is only through that iterative process that social science regularities will be revealed.
It is important to remember not to overgeneralize or overemphasize the findings of a single study about an interesting topic or case. The social science research that least resembles science-interesting writeups about historical events or exotic people-is most likely to capture the imagination. 
9
Much that falls under the rubric of social science research scarcely resembles what scientists and the general public think of as science, but that is only because of the qualities of social phenomena. Social science appears different from science, especially in those aspects most nonsocial scientists are exposed to. However, laypeople are not exposed to those less familiar, "more scientific" aspects of social science: the division of labor and the development of methods for measuring ephemeral complex constructs that cannot be observed using more conventional scientific approaches. One key point is to emphasize how essential it is to consider social science inquiry as an ongoing, iterative collective endeavor. Complementarity of studies is essential if social science is to advance. Complementarity implies that the strengths of one study can compensate for the weaknesses of a study on a similar subject. Replication and even seeming redundancy is necessary for both inductive and deductive work, as well as the necessary complementarity of inductive and deductive efforts. It is only by addressing social science questions from multiple perspectives and from multiple angles using multiple methods that the fields of social science can accumulate a collective understanding of a phenomenon. Given the challenges of studying social science, this cumulative and collective approach is required.
Opportunities exist for research sponsors to create programs to accelerate the social science enterprise. This will require changing how social scientists, the sponsors of social sci- 10 Stephen Marrin and Jonathan D. Clemente, "Improving Intelligence Analysis by Looking to the Medical Profession," International Journal of 
Intelligence and Counterintelligence, Vol. 18, No. 4, December 1, 2005.
For example, within DoD, there is a hope for social science products that are based on the creation of new sensors and computational models. The warfighter would use these sensors and models to predict the direct effects and anticipate the second-and third-order effects of their actions or inactions on the people in the environments in which they operate. To realize this vision will require programs centered around basic nomothetic and deductive social science research. To develop this social science capability will require revolutionary advances in data collection and measurement capabilities, so there are reliable and valid data available to test and validate formal social science causal models. Investment in the studies that use those data to test and refine foundational and generalizable social theories also is needed.
The obvious limitation of social science is that it seems immature or underdeveloped when compared with the hard sciences. Specifically, theory formulations seem to be too immature to be the basis for reliable tools. There are no validated formal models, such as F = ma and PV = nRT in physics, that can be confidently applied in real-world settings. It is not that social science researchers using conventional social science research methods do not regularly learn a great deal about the phenomena they are studying; they do regularly achieve this, even to the point of developing usable formal models (even if these models are not generalizable). However, although some general principles about behavior and the social world are known, what is known is not precise, and what is precise is not generalizable. Relational patterns are directional, but, unlike formal gas laws, the plug-and-play functional forms of equations have not yet been discovered.
In that sense, social science today is a bit like planetary physics in 1600. There were models in 1600 that worked well enough, but these models needed to be updated occasionally because they got out of sync. Luckily, thanks to Tycho Brahe's meticulously collected and recorded celestial data and the serendipity that brought Johannes Kepler to Prague to work with these data, old geocentric models were retired and more-accurate heliocentric elliptical models replaced them. These corrected theoretical models-Kepler's three laws of motion-were necessary for accurate predictions. Once discovered, these three laws could be inserted with confidence into a computational model to produce reliable and believable predictions about where a planet will end up at just about any point in the future.
The Brahe-Kepler metaphor perfectly illustrates the three major objectives that should be at the core of programs seeking to advance social science. The first objective is represented by Brahe's work: creating tools needed to obtain precise measurements of social science variables. The second is creating standards and tools for coalescing data and making these data available and usable to the theorists. The third is finding those theorists with the skills and creativity required to properly use and to draw the proper conclusions from the data. I discuss each one of these objectives here.
The precision with which Brahe measured celestial objects required the invention of special tools and careful recording. From the perspective of the social scientist, it was easy for him to identify what he wanted to track: The phenomenon under study consisted of discrete physical objects that most would agree would exist in the absence of human consciousness. Social scientists have the unique challenges of measuring objects, or subjects, that are not as discrete and that in some ways exist only because of human development. Tracking democracy is more nuanced than tracking a comet. That is not to say that all key social variables are purely social constructs. Some variables present only the same types of challenges that Brahe faced. Most demographic variables (i.e., fertility, mortality, morbidity, migration) require that social scientists make the same types of decisions for measurement as the astronomer: how we define units of distance and time for measurement. For example, counting or tracking the number of births in a given geospatial segment for the time it takes for the earth to rotate the sun one time defines fertility according to the tracking of a celestial object for a year. Measurement of variables of this type are amenable to error, but there are few discrepancies between studies. Measurement of variables that are purely social constructs, such as happiness, group cohesion, identity, and democracy, are more challenging to define; therefore, the measurement of these constructs is less consistent and precise. There is no way to measure these variables directly, and so social science hypotheses are tested using latent, not direct, measures of most key variables. What is measured, how, and what it means are closely tied to theory, but different scholars invent unique ways to measure identical concepts.
Another feature of social science research is the sensitivity of results to sampling bias. If two studies have identical approaches for measuring a concept but their approaches for collecting samples differ, then it is not possible to be certain that the difference in the relationship between variables that they observe is the result of differences in the population. It is worth noting that other sciences also rely on latent variables. Astronomers today construct latent variables when studying distant objects: What they observe is not a direct measurement of a phenomenon, such as a black hole or a quasar, but a construction that combines observables and theory.
That said, astronomers mostly build consensus around the meanings of these measurements, because at least their objects of study are agreed upon. That social science studies are conducted by so many different researchers, from different disciplines and training, in different contexts and on different populations, makes calibrating measurement and forming consensus impossible. As a phenomenon becomes harder to directly measure, it becomes less likely that a unified approach to measurement will emerge, and as multiple measures of the same phenomenon compete, it becomes impossible to formulate precise models of how that variable behaves relative to others.
The second related objective is to coordinate efforts to consolidate results. Kepler needed Brahe's measurements. Without those numbers, Kepler would not have made his critical breakthroughs. That the two were colocated was an amazing accident. That the collaboration teams needed to integrate and advance social science research will occur by chance is unlikely. Today, each independent researcher with a question uses whatever tools are available or invents new idiosyncratic tools to answer the specific question that interests them. Each study is a separate capsule linked to others only through citation networks and literature reviews. Disciplinary or topical separations partition the collection of findings so that researchers doing theoretically similar work-often with similar findings but in different domains-are typically not aware of the advances of others and the potential significance of their own results if applied to other contexts. A great deal of important and excellent work has been and is being done, but there is little effort to coalesce and make sense of the body of work.
The goal of today's social science enterprise is to "put the findings out there," but where is "there"? And once the information is "out there," what happens to it? Unfortunately, there is no magic method or incentive to organize all that is out there to make sense of the body of social science produced. Without intentional coordination, the discovery of "general theories of social science" will remain out of reach. It will also require the coordination and consolidation of data and of research efforts. This is where investment is needed. A deliberate integrative research program is required to move toward that goal. A useful frame for prioritizing investment in social science would be to focus on two activities: integration and standardization of data and theoretic synthesis.
The third objective is to find the right people to do the science. Among the obstacles to advancing social science is that, except for a few initiatives and programs, there is a government-wide institutional resistance to investing in social scientists. Expenditures on social research are miniscule compared with other sciences. 11 Tether's recognition that social science insights can benefit the warfighter did spur additional investments, but the progress made has been minimal. Some of the reasons for the limited success align with the general challenges to progress in social science research discussed in the section "Separating the I from the Me: Challenges for Social Science Research." The focus of most R&D efforts has not been on building programs to discover the "rules of social science" and on developing the needed 11 Funding for social science research at NSF has consistently been dwarfed by research funding for other scientific fields, such as the biological sciences, computer and information science and engineering, engineering, geosciences, and mathematical and physical sciences. From the 1960s to the 2000s, on average, funding for social science research as a percentage of total NSF research funding was approximately 5 percent. 
See Larsen, 1992;
and National Science Foundation, 2020.
12
This ad hoc and short-term focus has produced some useful insights and tools, but it cannot advance the science, because the focus is almost entirely inductive. To create a social science capability that can provide useful benefits will require serious attention to refining social theory so that there are valid and reliable formal models (i.e., equations to drive computational models). This in turn will require serious investment into new tools specifically designed to gather and organize data for social science analysis. Finally, it will require investing in social scientists to manage and lead the scientific development.
DoD has led the drive to invest in social science with several investment initiatives, but three main institutional obstacles have deflected social science's research focus from basic to applied research and distracted attention from validating theory and developing useful social science tools.
The first obstacle is that DoD R&D program managers generally do not consider it their job to advance tools to improve the productivity of social science researchers: Their customer is not the social science community but the warfighter. 
13
The second obstacle is that many in the NSE research community who are focused on meeting the needs of policymakers and "boots on the ground" still confuse social science with intelligence and policy analysis. The misconception that social science should reveal actionable insights in real time is a liability. Making a short-term prediction about a specific case is not an appropriate ask of a social scientist and should not be the goal of a social science research effort. Again, the benefit of investing in social science research is to refine social science theories, not to produce intelligence or factoids. By providing valid, reliable, and generalizable models of underlying structures and pervasive patterns of behavior that include parameters to adapt the general model to accommodate exceptions to the rule based on location, timing, age, or "cultural" factors, social scientists can create a capability that will allow policy and intelligence analysts to produce reliable actionable insights when needed. 15  The relationship between social science and policy or intelligence analysis is like the relationship between theoretical physics and engineering or between biology and medicine. The focus of social science is on defining and refining the model or characterizing the "signal" or the pervasive pattern so that it can be easily recognized. The focus of the social sciencetrained analyst is on detecting or preventing noise or an anomaly. Ironically, if the social scientists do their job well and develop good models of the signal, it will be possible to create tools to make it easier for analysts to detect, characterize, and recommend actions to recog- 14 Even NSF funding for social science favors non-social scientists. The NSF direct social science budget is small, but NSF invests in research conducted on social science-related outcomes proposed by non-social scientists. For instance, research on social media data mining or natural language processing focused on understanding trends and behavior. 15 For example, Roger Hilsman noted that social science theories and models are distinct from intelligence analysis but that, often, developments in the social sciences can be applied to meet the needs of intelligence analysts, as in the case of the bureaucratic model of political decisionmaking (Roger Hilsman, "International Environment, the State, and Intelligence," in Alfred C. Maurer, Marion D. Tunstall, and James M. Keagle, eds., Intelligence: 
Policy and Process, Boulder, Colo.: Westview Press, 1985)
nize and eliminate noise so that social scientists can detect and help mitigate crises in real time. If investments continue to focus on the imminent problem, then the tools needed to mitigate imminent crises will still not exist five, ten, or 20 years from now. The consequence of confusing social science with policy or intelligence analysis is that most of the research focus has been on the crisis du jour instead of on the less interesting, status quo behaviors that dominate the planet.
The third obstacle is the idea that social science research should be carried out and tested in a war zone to prove that social science is a "real" science. R&D investments that seek to advance other sciences do not require this. Social science is intrinsically difficult. It is unnecessary to create artificial challenges to make real advances less likely. Just as other research in physics, chemistry, and biology is conducted in laboratories and early prototypes are tested in controlled environments before they are battle tested, social science must be conducted in calm, data-rich environments. It is hard to imagine a physics-based research project that would require that basic research be conducted only in a hurricane. Basic research is conducted in a laboratory. Prototypes are tested in wind tunnels.
The motivation that leads some to think it is important that social science research applicable to a specific environment of interest must be conducted in that environment is likely inspired by work showing that some findings from psychology and social psychology-largely conducted using exclusively WEIRD (Western, educated, industrialized, rich, and democratic) subjects-do not generalize to some non-WEIRD populations. 16 These are important findings, but they mean that social research must be replicated in multiple contexts before a claim can be made about generalization; they do not mean that social research methods can be developed and calibrated on the fly in non-WEIRD settings. The opposite is true. The reason social scientists were able to discover these differences in WEIRD and non-WEIRD populations are because of the existence of reliable and valid measurement tools and instruments developed and tested in WEIRD laboratories. It was these instruments, when implemented in new environments, that revealed differences. It is not necessary to develop different thermometer technology to measure temperatures in different locations. Indeed, it is only because we use a common measurement tool that it is possible to conclude that temperatures differ in different locations. The development of new instruments exclusive to specific environments will not advance and may hinder progress in social science because the findings from dissimilar instruments cannot be interpreted or compared. It is also important to realize that just because some research conducted in WEIRD countries does not generalize does not imply that none generalizes or that research done in one non-WEIRD country would 16 Mostafa Salari Rad, Alison Jane Martingano, and Jeremy Ginges, "Toward a Psychology of Homo Sapiens: Making Psychological Science More Representative of the Human Population," Proceedings of the National Academy of 
Sciences, Vol. 115, No. 45, November 6, 2018.
If a new social science concept or measurement tool is to be adopted, then testing would eventually need to move to more-realistic and more-extreme conditions using subjects other than college freshmen, but the initial development and testing should be done in the mostcontrolled conditions possible. If the science and theory is sound, then the results found in these basic research studies will generalize to a broad variety of contexts; therefore, the models will allow recognition of the conditions that extend beyond the model and the prescription of modifications. Social science phenomena are no different from physical or biological phenomena in that most behavior lies within normal parameters. Basic social science research should not be treated differently than other types of basic research and be expected to find structure when restricted to studying only outliers and anomalies.
A long view for social science and investment in research is necessary-one that is not directly focused on the most pressing issues of the day. The objective of social science research should not be to provide actionable information about a particular idiosyncratic event; instead, it should be creating trustworthy tools to allow decisionmakers to make sense of chaos in real time. Science should mature in advance of a crisis.
The downsides of shortsighted science planning are not unique to social science. Myriad examples emerged of the neglect of unfashionable science slowing the world's response to the coronavirus disease 2019 (COVID-19) pandemic, alongside stories of forgotten and undervalued science allowing a quick ramp-up to the development of vaccines. 17 Social science (and all science) is slow. For it to be useful when needed, the investment must come well in advance of the need. Social science is not emergency room diagnostics; it is theoretical knowledge production that allows practitioners to know the effects of their actions to mitigate a crisis with confidence. To do this requires the study of many cases in many situations over time, eventually leading to trusted practices. This is not to say that past social science programs have not advanced the field and produced tools that can be employed to advance it further. There are many examples of social science-funded research that have produced data collection, analysis, and modeling tools that could, if used properly, accelerate the accumulation of scientific knowledge. But 17 Fedor Kossakovski, "Why Some People Are Superspreaders and How the Body Emits Coronavirus," Science, October 27, 2020; Leah Asmelash and A. J. Willingham, "She Was Demoted, Doubted and Rejected. Now, Her Work Is the Basis of the Covid-19 Vaccine," CNN, 
December 16, 2020.
Tether's vision for advancing social science was to discover and validate social theory so that it could be used in computational models. As discussed at the beginning, PCAS was conceived to demonstrate the potential to create tools based on social science formulas. PCAS was not expected to discover or validate these formulations; the goal was only to provide evidence that there were observable and measurable regularities inherent in the phenomenon of interest that could be formalized. In retrospect, PCAS revealed-both in what it did well and where it fell short-features important in a program to advance social science theory. Specifically, the strength of the program was in the right framing of the objectives of the program and research question and in the diversity of performers. The weaknesses were the result of a lack of access to good data and the short time line for completion (six months).
To move social science forward expediently will require programmatic coordination. Advances in social science will be slow if each study is independent in design and scope. Social scientists are largely excited by incremental advances-small problems and simple designs. Thoughts of grand programs linking together or coordinating their work with the simultaneous work of others do not occur to them. Aggregating efforts into bigger projects is rare. One exception is the coalescence of survey research questions into large, regularly administered and publicly available questionnaires. Otherwise, most research agendas are independent or involve small collections of frequent collaborators. What is most exciting about the prospect of new, large, sponsored investments in social science is that, with an ambitious programmatic perspective, many different studies using different methods or focused on different populations can be launched simultaneously to address the same question. Changing the culture of social science research so that the community begins thinking explicitly in terms of active integrative collaboration would create an environment that reduces redundancy and increases the recognition of similarity among diverse research areas.
The first step toward building large and productive basic research programs would be attracting the right set of researchers to participate in each program. The goal would be to create a diverse set of social scientists from different fields engaged in different types of methods and focused on different populations but with a common theoretical focus. Over time, working together, this group can identify common features and determine the sources of the differences.
To do this for social science today would require a concerted effort to seek out and attract social science researchers. As mentioned before, most social science funding, except for NSF funding, is awarded to non-social scientists. If this continues to be the case, the right people will not in the room to advance social science. Institutional inertia is powerful, and the moratorium on social research imposed with the cancellation of Project Camelot still imposes barriers to productive social research. This has changed: Today, there are several social scientists working to fund social science across DoD, but their senior management and most of their peers that review proposals are non-social scientists. Therefore, social science programs are often structured to be counterproductive to producing good social science findings. The programs inherit the lessons learned and conventions about research from other fields even if they are not relevant.
Because there is no demand for social science researchers to engage with funding agencies, the relationships between these agencies and social scientists have dissolved. Except for a small number of regulars, social scientists are not recruited to or informed about upcoming funding opportunities. Even if they do become aware of opportunities, their proposals are often framed differently than expected by review panels made up mainly of non-social scientists. The result is the most innovative, methodologically sound, and theoretically compelling proposals are rejected in preference to what appeals to, and can be understood by, reviewers with no training in social science. The converse never occurs: Nobody ever puts a panel of social scientists in charge of the committee to select proposals about material science or polymer physics.
A second vital feature for a social science program is correct framing of the research question. The most-productive projects should be narrowly focused on a general phenomenon. The specificity is in the type of behavior or interaction that is of interest, not the context or the specifics within the context. As an example, if the interest is in understanding the recruitment of terrorists or insurgents to a specific cause, the objective of the science is to focus not on the group of interest but on the recruitment processes more generally. Data about this group are appropriate for intelligence analysis but insufficient to inform science. The science question is more generally about recruitment, and the study of all sorts of other groups (i.e., religious, political, recreational, and other groups that recruit new members) is equally relevant.
Social scientists, if they are doing their job properly, can only draw conclusions long after the fact. Data collection, cleaning, and analysis take years. This may be not a useful timescale for getting ahead of a crisis, but it is what is required to ensure studies that are representative, that can account for biases, and that can measure indicators in a reasonably valid and reliable manner to generalize the results appropriately. Until long-term science is complete, no shortterm answer can be relied on.
PCAS was unique in that it was a coalescence of many different types of quantitative, formal, or computational researchers all focused on a unifying theoretical question. Their challenge was to create models that would predict or anticipate state failure in two specified countries, but the objective was to come up with a generalizable model of state failure. All the social science teams selected had experience modeling complex social systems. Each team brought a different approach to PCAS. Some were focused on state-level variables and used classic political science models of state failure. Others looked at the question from a social-movements perspective. Still others took a more anthropological perspective and focused on substate social organizations. Methods used also varied: Some of the teams instantiated formal theory into models directly. Other teams used empirical data to drive models.
Normally, each team would work in a silo, study the problem, generate results, and publish its papers in journals that none of the people on the other teams would ever read. What was innovative about PCAS was that the program provided a unifying umbrella, and this feature was critical. The benefit and key advantage of a programmatic approach is in gathering diverse groups of researchers that represent multiple complementary positions in the field of social science production. At a minimum, representation should span the continuum from inductive through deductive, micro to macro, observational to experimental. Once teams are created, the tasking focus on a common problem should set each on its mission to find answers, but the program will ensure that no performer goes it alone.
The point of this diversity is to generate communication and collaboration about both theory and design. This element is necessary to generate a transformative advance. It will work only if at every stage of the program each study informs and is informed by all others about their progress and challenges, so that, when the studies are completed, the results can be easily compared, contrasted, integrated, and meaningfully interpreted by all others. The compulsory communication is what facilitates the recognition and identification of what model elements are shared. The task provides a common focus for each approach to meaningfully demonstrate its value and reveal the similarities and differences between approaches. In an ideal world, this sharing and convergence would occur naturally, but it does not; if revolutionary advances are to manifest, the connections among research groups must be manufactured.
Gathering research groups to work on a common problem is important but not sufficient. Structuring the program and the evaluation of the program so that performers do not feel like competitors is also important. This goes beyond a common task framework. The objective is to produce a new model that more precisely and parsimoniously describes the general phenomena than could be achieved by any one team. To do this, teams must be willing to work together and be willing to cooperate-performers need incentives to work as a team toward a common goal. A standard model that frames initial phases as competition forces performers to try to outdo one another and does not motivate sharing. To encourage learning and growth, the program must reward collaboration. Milestones and deliverables should be structured with collaboration in mind.
There Is a Need for Well-Scoped Questions and Innovative Solutions Once engaged, performers should not be limited in what data they use to inform theory. All methods, all context, and all previously studied examples should be encouraged. Programs should be focused on answering a specific theoretical question. The social scientist selected to study this question is the best person to decide how to design a study. Any restrictions placed on the case or data used, beyond obvious fiscal and ethical restrictions, are counterproductive to the goal of synthesizing theory.
This point dovetails nicely with recommendations for evaluation. The goal is not to pick the best approach but to assemble a new approach that brings together the best of the collective. For example, programs structured to down-select performers who do not come "closest" to predicting an outcome constrain their performers and restrict the potential for revolutionary science. Furthermore, this type of evaluation criterion is not sufficient to truly evaluate the models. Future predictive model evaluation, especially when the topic is macro-level social science, is not appropriate. To evaluate causal models-to test hypotheses-requires many cases because the outcomes are not expected to be probabilistic. By definition, any "point prediction" (a single value at a single moment) is inconclusive.
There are two main messages. The first is that the effort to turbocharge social science should be led by trained social scientists. The second is that although social science in many ways is similar to other sciences, in some ways it is not. Thus, some of the structures that have proven beneficial for advancing other sciences should be implemented in social science research programs and some should not. What should be replicated is program management of the funding of researchers with expertise in the field and support for basic research conducted in data-rich and controlled environments. What should not be replicated, because the main objective of the investment is synthesis, are evaluation criteria designed to eliminate perspectives prematurely.
Ultimately, the objective of advancing social science research is to have valid social science algorithms at the ready. To do this requires advances in data collection methods and innovative designs to test or validate the theory. One of the biggest challenges for social researchers is to find good data. The second challenge is to develop approaches to test the types of hypotheses that will matter to sponsors and users, such as DoD decisionmakers. These issues are not independent. For example, Kepler needed good data to test his theory, but Kepler's design was a one-shot design for the most part. In addition, the data collected were unchanging, at least at the time. Furthermore, the theory that Kepler was testing was descriptive, not causal. Kepler was not required to introduce or wait for an intervention to remeasure the environment. The social science phenomena that are most compelling to sponsors are primarily, but not exclusively, macro and definitely causal. The objective is not just to predict outcomes but to understand the mechanisms of change so that decisionmakers can take an action to bring about desired changes, or at least avoid detrimental repercussions.
Realistically, it will be a while before social science models are as dependable as physical models, but commencing research now is essential; even if we do not discover the universal laws of social science by the end of a research program, progress toward that goal is bound to produce many useful by-products. The recommendation here is to do the science right, with the goal of perfecting theory and generating useful by-products along the way.
Social science is slow because practitioners are passionate about sampling, data integrity, and methodical and repeated theory testing. To make social science useful, it is important to make social science better, as well as cheaper and faster. This does not mean that technology invented for other purposes cannot be repurposed and used for social science; rather, it means that technology will likely have to be adjusted to fit into the social science process. As an example, from the social science perspective, data mining is a form of convenience sampling, which is generally problematic given that the data have known and unknown biases that make the data difficult to generalize. Considerable investments have been made into developing tools to capture, code, and use information available online to study social science. However, most of this development is done without the input of social scientists. Many articles are published where the subject is social science-related, but the research team is entirely trained in computer science and engineering. Some of these papers provide interesting insights, but, because of design flaws, such as reliance on nonrepresentative samples and a completely inductive nature, few make lasting contributions to our understanding of people or behavior. That should not be surprising. To computer scientists, data are data, and what the data represent is secondary. The explosion of data generated by people as they use the internet should be exploited by computer scientists to demonstrate their prowess in churning through and organizing data. What is important to remember is that their results concern computer science metrics, not social science outcomes. From the perspective of social scientists, the exploitation of data because they are available is likely insufficient to represent social constructs and to produce valid results.
Figure 
9
It is not that the social scientist cannot benefit from these technological advances; it is that additional technology is needed to make this technology useful. Social scientists need tools to identify biases and perhaps to leverage different biases in data collection to introduce correction factors so that the data would be representative. Moving beyond just the data, these approaches are mostly entirely inductive, drawing conclusions from mined data based on correlations. Few studies are structured to support theory testing. For social scientists to advance technology, they will have to also support theoretically motivated data collection and the standardization of structure data, so that quantitative deductive analysis is feasible.
Social scientists are not enthusiastic about many computational models popular today because most of the work, especially black-box machine-learning and predictive tools, is at best inductive. However, many of these models are not even inductive, because they fall short of revealing the causal theory to support their conclusions. With this in mind, and given the vast amount of data and studies that need analysis, some of the technology developed for prediction may be useful, but not if investment is made in place of developing computational models focused on revealing causal theory and theory testing. Social scientists have the deepest understanding of what is required to formulate, test, and refine social theory and are required to be at the heart and at the head of the effort to advance social science.
The social science frameworks discussed are useful heuristics for thinking about which short-term by-products to focus on and, more importantly, how to produce short-term ben- 
Available data efits that also move us toward a long-term goal. Social science insights can benefit decisionmakers, and social science research can provide real-time evaluation of the social and political changes in an area of operation during an engagement. What is important to keep in mind is that if research being done is informative for one subject, then the research should be done, but the data should be collected in a manner that makes them more generally useful.
Data collected to research specific problems or questions are rarely designed to be combined with or inform larger research programs and therefore cannot contribute to broader research needs and applications. Careful research designs can allow case-specific research to proceed, but in a context that allows data to be reused to contribute to future, possibly larger, studies, allowing knowledge to accumulate. This is not standard practice today.
The emphasis here is that studying real-world events can be done from an idiographic perspective and focused on only one case but must be seen as an opportunity to contribute to the more general nomothetic, or scientific, approach and must be done so that it can facilitate theory testing. If each engagement, mission, or incident is treated as a collection opportunity and if there are some established criteria for data collection that allow consistent comparison across cases, then each becomes a case and a source of data that feeds deductive studies to validate the underlying science. These studies, along with the development of the commensurate tools, could allow the understanding and interpretation of social dynamics in real time.
Ongoing military, diplomatic, and humanitarian engagements can be opportunities to feed the social science data collection and experimentation machine to enable the development of a reliable set of social science laws and principles, as can studies of the activities of other U.S. agencies or nongovernmental organizations. The engagements can be decomposed into thousands of data points and blended with similar data from other cases to feed nomothetic work. Considering a bigger event as the accumulation or aggregation of many events at a lower level expands the analytical possibilities and value of scientific research. Focusing at too high a level reduces an assessment to dichotomous categorization, while measuring at lower levels allows us to measure variance and see the distribution or impacts. It also provides opportunities for experimentation.
Clearly, a one-shop macrolevel intervention can be risky and problematic. Thinking of an engagement as a collection of many micro-interactions and capturing the implementation and then measuring the effect is a means of transforming policy actions into running quasi-experiments. This breaking up of the major event into components, rather than one case, changes the one-shot design to one with repeated trials. A creative researcher may even be able to introduce random assignment in some cases. It also provides decisionmakers with a means to pretest the impact and repercussions of interventions under consideration in real time.
Take, for instance, any humanitarian engagement. Studying the impact on a national scale is ideographic and generates an n of 1 for study; in contrast, collecting data on the impact at the individual or village level generates data that can be placed in a collective database and used to inform multiple analyses with a sufficient number of cases to test a variety of hypoth-eses. The data at the national scale that are used to evaluate whether the policy had an impact might be gross domestic product or infant mortality before and after humanitarian aid is distributed. A more nomothetic design-an experiment or a quasi-experiment-could focus on data at the village level. If some villages receive aid and others do not, the comparison is a quasi-experiment. If decisionmakers were able to determine which villages received or did not receive aid using random assignment, then the humanitarian activity is also an RCT. If this type of data eventually were collected in many places over time, researchers would be provided with rich data to test hypotheses about the impacts of humanitarian relief efforts and explain why some interventions produce the desired outcomes while others do not (controlling for nationality, region, religion, cultural factors, and a variety of other factors). However, doing so will require coordination and standardization, especially if data from many events are to eventually be used in the same analysis. The benefit of this type of coordinated data effort is huge, as would be the effort required to build the infrastructure to organize, store, and make available the data so that they could be used.
Similarly, baselining before an engagement is essential. Waiting until a crisis to begin measurement makes measurement less useful because if the crisis is in progress, the system is not in equilibrium. From Campbell and Stanley's perspective, 18 if posttreatment measurements are all that is available, the opportunity is a one-shot case study, rife with threats to internal validity. Getting data to baseline in advance allows for pretest and posttest comparison, turning a one-shot design into what is referred to as a static group design experiment and allowing the researcher to at least make the claim that the event had an impact on the group. If measurements or baselining of another comparable group is also available, then the situation presents a common quasi-experimental design opportunity: a nonequivalent group (difference-in-difference) design. With each step, threats to internal validity disappear. The only feature of an RCT missing from the difference-in-difference design is the random assignment. Of course, if only two comparable instances are available to observe, there are only two trials and not a great deal of statistical power or ability to generalize. Baselining broadly across the globe extends this farther, introducing the potential to run natural quasiexperiments comparing the changes that occur on measured variables before and after they experience similar crises. Over time, if there are standards for data collection and measures across locations, the design can become closer and closer to an experimental design.
Baselining widely across the globe and at the lowest level of analysis possible is a prerequisite for being able to apply models in real social situations when they are needed. It is not just that beginning study at the start of a crisis or even at the first signs of an imminent crisis is too late. One needs to take multiple measurements of multiple variables at many time points. Most social systems remain in equilibrium for a long time, but this does not mean that they are static. What is needed is surveillance of the impacts that multiple variables have on each other over time and space. Surveillance before a crisis and careful recording of dynamics are necessary to support the development of models that can reliably reveal a system's tolerance to shocks of different types.
Finally, and most importantly, if we are ever to have valid and useful formulas and algorithms, there will have to be a concerted effort to work toward theory testing and standardizing the way data are stored, organized, described, and saved. Convergence and integration will require work. Study design, including data collection, is only one aspect. Data analysis, including meta-analysis to find general patterns from among many studies and synthesis, is needed. Automating some aspects of these analyses will be important because the scale of accumulation of studies will exceed the capabilities of any researcher or research team. What is important is that automated processes, including data collection and analysis approaches, are inspired by social science. Methods and models used today may become obsolete as technology expands what is possible. That said, the inspiration for all aspects of the social science endeavor should come from the tried-and-true theoretical and methodological foundations from the social sciences.
The main theme of this chapter is that social science research is not easy, nor is creating programs to accelerate the production of valid social science. However, the problems posed by UGS, whether similar to those studied in the PCAS program or other problems related to emerging forms and domains of competition, all require insights from the social sciences and the contributions of social scientists to meet the nation's needs. This chapter is meant to highlight my point of view of the past 20 years-from the social science research reboot with PCAS until now. Revolutionizing social science research will require cultural and institutional changes from all parties. Social scientists will have to start thinking more programmatically, and government agencies will need to become more comfortable with social science and social scientists.
Beyond that, I recommend focusing on using and gathering existing data as much as on creating new science. That is not to say that new data collection and new tools are not needed. However, as in other fields of scientific research, what is needed are legacy research projects that lead to definitive and substantial progress and tools that can be universally used by all who engage in social science research, such as tools to integrate what are now different data and data types into a standard form available and usable by social scientists of all types. Other efforts might involve investment into theory development, such as computational models to test theoretical assumptions or make formal theoretical models more accessible and useful. There is a huge need for new technology to capture, code, coalesce, and analyze data. However, the most critical ingredient needed to advance social science is the active engagement of the leaders driving the research and the research agenda.
Why Reasoning Under Uncertainty Is Hard for Both Machines and People-and an Approach to Address the Problem
Reasoning under uncertainty has always been central to decisionmaking. For example, undergoverned spaces (UGS) involve uncertainty by definition. In conducting decisionmaking for UGS, actors lack complete knowledge of what is going on, who is doing what, and the effects of their own and others' actions. This is also true of all four parts of the Act-Sense-Decide-Adapt (ASDA) cycle, which require reasoning with uncertain knowledge. When we try to sense something, we have to account for the possibility of erroneous and misleading stimuli, and we hedge against uncertainty about the true state of the world. As Descartes fretted four centuries ago, a powerful, malevolent foe may have constructed an entire world out of illusions to deceive us. When we decide what action to take, we have to account for uncertainty about the likely effects of the available actions and even about what actions are available to us. When weighing the merits of these actions, we must also grapple with uncertainty about both our own and others' current and future preferences. It is very common to have no more than an educated guess about what a rival wants, but much of the time we also have difficulty specifying our own desires. When we adapt, we have to reason about the uncertain possible futures we are choosing between. An unnerving aspect of this is the possibility of being tripped up by what Donald Rumsfeld dubbed "unknown unknowns." And when we act, we have to juggle all these types of uncertainty at the same time. However, time is not a luxury that we have when acting, sensing, deciding, or adapting. When facing an intelligent adversary, we may have to act quickly-even if we have not been able to think through everything fully.
Because reasoning under uncertainty is central to creating machines that exhibit intelligent behavior, reasoning under uncertainty is one of the most-studied problems in artificial intelligence (AI). Unfortunately, despite all this effort, no entirely satisfactory way to reason under uncertainty has yet emerged; however, efforts to find one have yielded considerable theoretical insights into the problem, as well as a wide variety of experimental systems. These programs use a variety of alternative techniques with associated strengths and weaknesses. Those that excel in some respect, such as expressiveness-the ability to describe a large number of different situations-come with a weakness, such as prohibitive computational demands. Because AI researchers have tried to translate all these different approaches for reasoning about uncertainty into engineering, they arguably gleaned deeper insights into the challenges involved compared with less empirical investigators. Perhaps their most essential finding is that there is no single "best" or "right" way to reason under uncertainty; this is both because of trade-offs between characteristics, such as computational complexity and accuracy, and because there is more than one way to be uncertain. For example, sometimes one is uncertain about whether a proposition is true, but other times one is uncertain about the degree to which a proposition is true. To reason comprehensively about uncertainty, it is necessary to be able to account for many qualitatively different species of uncertainty simultaneously.
In this chapter, I first examine the challenges that AI researchers have encountered by using the approaches they have used historically. Then, I discuss the fundamental ontological challenges that these approaches face. This is followed by the implications these challenges have for national security decisionmaking. Given all these obstacles, I suggest a proposed research agenda going forward. The chapter ends with some concluding thoughts.
Broadly speaking, AI researchers have developed two approaches to reason under uncertainty that can be classified into two paradigms: Bayesian and non-Bayesian. We discuss each in turn, along with challenges that have been encountered, followed by a discussion of ontological challenges common to both.
Bayesian approaches represent knowledge about a set of state variables as probabilities. These state variables can be either discrete (for instance, 50 percent confidence that a proposition is true) or continuous (such as a probability density function representing the likelihood that a variable takes a particular value). These variables are initialized to a prior (starting estimate) and then updated using Bayes' rule when new evidence is received. The basic version of Bayesian reasoning uses the full joint probability, accounting for possible correlations between all the variables. This fundamental approach is almost never used for nontrivial problems because combinatorial explosion rapidly inflates the size of the joint probability table to an unmanageable size. 
1
10
Slippery Season
A limitation of Bayesian methods is that their underlying knowledge representation is inherently propositional-a particular variable can be true or false, or, if it is continuous, it is assumed to have one and only one true value. To reason about complex dynamic processes-for example, the surveillance of a region containing an unknown number of targets of interest-one must extend the Bayesian paradigm to consider multiple worlds. 
5
Proponents argue that Bayesian models are more well specified, comprehensible, and correct than available alternatives, but for decades AI researchers hesitated to embrace Bayesian approaches. They had two reasons-one practical and the other theoretical. Until Judea Pearl introduced the belief net, there was no efficient way to conduct Bayesian inference and updating for a nontrivial problem. However, in the 1960s and 1970s, AI researchers mostly worked outside Bayesian models because of their observation that human reasoning seemed not to be based on probability. Instead, humans appeared to rely on simple heuristic methods, such as "default reasoning," which basically consists of assuming that a proposition is true until a seemingly more plausible one comes along, at which point it becomes the new default hypothesis. Observational psychology offered support for this idea, so AI researchers prototyped experimental systems based on it. 
6
tional Conference on Computer Vision, Vol. 1, IEEE, 1999.
Perhaps we can see the person and know for certain what proportion of their body is on each side of the border.
Dempster-Shafer theory is another non-Bayesian approach for reasoning under uncertainty that has found substantial use for defense applications. Also known as the theory of belief functions, Dempster-Shafer theory originated as an attempt to introduce an intervalvalued alternative to Bayes' rule. In an influential 1976 book, Glenn Shafer reinterpreted Dempster's original mathematics to represent what he dubbed "belief" and "plausibility" instead of bounds on an interval of Bayesian-like probability values. 8 Shafer argued that his approach transcended Bayesianism by providing a natural mechanism for representing the concept of "ignorance." The semantics of Dempster-Shafer theory and its relationship to the Bayesian paradigm are both controversial. Some critics argue that Shafer's characterization of these two values as "belief" and "plausibility" is misleading, irrespective of the soundness of the underlying mathematics. And while proponents tend to emphasize that Dempster-Shafer theory is equivalent to Bayes' rule when "belief" is equal to "plausibility," skeptics contend that the theory is qualitatively distinct from-and inferior to-the older Bayesian paradigm it sought to extend. 9 They are bolstered in this conclusion by the sometimes counterintuitive behavior of Dempster's rule, which can violate common sense when combining evidence from two sources that consider different possibilities, because the rule deletes any proposition that a source gives a probability of zero. 10 Proponents argue that these criticisms are overstated and that the use of Dempster-Shafer theory in practical systems suggests that it is still useful even if imperfect.
Despite the theoretical distinction between Bayesian and non-Bayesian approaches for reasoning under uncertainty, real implemented systems tend to incorporate elements of both. A dominant technique for tracking multiple targets, the Multiple Hypothesis Tracker (MHT), exemplifies this. The MHT aims to associate every possible detection of a target with one and only one source. It does this by maintaining a collection of single-target recursive Bayesian filters (Kalman filters) and computing a value for each track. Each valid assignment of all detections to possible tracks constitutes a hypothesis, which explains the name of MHT. 11   8 Glenn Shafer, A Mathematical Theory of Evidence, Princeton, N.J.: Princeton University Press, 1976. 9 Albena Tchamova and Jean Dezert, "On the Behavior of Dempster's Rule of Combination and the Foundations of Dempster-Shafer Theory," in 2012 6th IEEE International Conference Intelligent Systems, IEEE, 2012. 10 Lotfi A. Zadeh, "Review of A Mathematical Theory of Evidence," AI Magazine, Vol. 5, No. 3, 1984.   11 Stefano Coraluppi, "Fundamentals and Advances in Multiple-Hypothesis Tracking," in NATO STO IST-134 Lecture Series on Advanced Algorithms for Effectively Fusing Hard and Soft Information, Paris, France: NATO Collaboration and Support Office, 2015. Although the introduction of the Kalman filter predated belief nets by over two decades, Kalman filters were later shown to be a variety of DBNs, and, today, a sizable literature exists analyzing them. 
See Pavlovic et al., 1999, and Kevin P. Murphy, Switch-
The computational complexity of belief nets, by contrast, has been extensively studied by computer scientists-but their findings are very sobering. Theoretical studies have shown that exact inference in arbitrary belief nets is NP-hard relative to the size of the network. 13  This means that we cannot reasonably expect to be able to find the exact answer to a query of a Bayesian network of nontrivial size, even if we assume an arbitrarily powerful computer. This is not necessarily a showstopper in and of itself, because very effective algorithms exist to find approximate solutions to some NP-hard problems. The more damning finding is that accurate and efficient approximation algorithms for inference in arbitrary Bayesian networks of the classes of interest apparently cannot exist either. Approximate inference in Bayesian networks turns out to be NP-hard as well; furthermore, there is no guarantee that the resulting approximations will be close enough to the true values to be informative. 14 Perhaps this is to be expected, because in a cosmic sense, such an approximation would be too good to be true. Given the ubiquity of phenomena that can be stated as belief nets, that approximation would be applicable to a mind-boggling array of diverse problems and could make a revolutionary impact on both science and engineering. In the 1990s, theoretical computer scientists ing Kalman Filters, Berkeley, Calif.: Department of Computer Science, University of California, Berkeley, August 1998. 12 Samuel Blackman and Robert Popoli, Design and Analysis of Modern Tracking Systems, Norwood, Mass.: Artech House, 1999, pp. 1072-1075. 13 The NP in NP-hard stands for nondeterministic polynomial and refers to a kind of hypothetical computer that computer scientists use to theorize about computational complexity. This nondeterministic computer could explore multiple branches of a search tree at the same time, which is of interest because it could find the answer to any query in the same amount of time it would take to check whether that answer was correct. NP-hard is a broad category of problems that are at least as hard as the hardest problems in NP and would be hard in the sense that even the physically unrealizable nondeterministic computer would take a long time to solve them (Gregory F. Cooper, "The Computational Complexity of Probabilistic Inference Using Bayesian Belief Networks," Artificial Intelligence, Vol. 
42, Nos. 2-3, 1990
60, No. 1, 1993.
Beyond the challenges already listed, there are also ontological ones. To reason about uncertainty, we need to represent it somehow-but how are we supposed to represent something we are uncertain about? The problem of choosing appropriate systems of knowledge representation (or ontologies, as they are termed by AI researchers) remains an unsolved one. A complete ontology encompasses both a knowledge representation language (symbolic in most historical systems, but often incorporating learned vector embeddings in modern ones) and a semantic interpretation mapping that language to external entities. The reason that researchers have advocated so many alternative approaches is that these have meaningful differences that make them more or less fit for specific use cases. One important distinction between some major approaches is that they make different ontological commitments-that is, "truth" actually does not mean the same thing in them. For example, in a Bayesian context, probabilities are maintained about whether a proposition is true, but that proposition is assumed to be either true or false, with no possibility that the proposition is simultaneously 15 Uri N. Lerner and Ron Parr, Inference in Hybrid Networks: Theoretical Limits and Practical Algorithms, arXiv, 2013. The analysis in Lerner's 2002 dissertation is based on a kind of DBN called a Continuous Linear Gaussian (CLG), which uses both discrete variables and continuous variables, with restrictions, such as that the continuous variables must be Gaussian and that a discrete node cannot have a continuous parent. Lerner proved that unless P = NP, even approximate inference in CLGs is intractable, and, more surprisingly, no polynomial approximate inference algorithm could have an absolute error smaller than 0.5. CLGs can be embedded into the more general DBNs of interest for most defense applications, so these pessimistic complexity and approximability results should be expected to apply to them as well; see Chapter Four of Uri N. Lerner, Hybrid Bayesian Networks for Reasoning About Complex Systems, dissertation, Stanford 
University, 2002.
Another, perhaps more important, reason that no single scheme for knowledge representation has found universal acceptance is that none has proved decisively superior in practical applications. Knowledge representation was one of the hottest areas of AI research during the expert systems boom of the 1970s and 1980s, resulting in some significant theoretical insights into the relevant problems. Unfortunately, one of the outcomes of this research was the discovery that it is not possible to find one ideal system of knowledge representation that is both representative enough to account for everything we would like while simultaneously being practical. Studies of simple knowledge representation languages showed that in any language capable of nontrivial representativeness, inference for some queries was computationally intractable. 17 The consequences of this are profound: Knowledge representation languages need to be well adapted for their particular use cases to ensure that these intractable inferences do not need to be made in practice. This has particularly tricky implications for reasoning about uncertainty because it suggests that it may be necessary to modify or replace the knowledge representation language dynamically to keep it performant as knowledge is updated.
A fruitful way to think about the challenge of choosing an appropriate ontology for reasoning about uncertainty is using the "many worlds" metaphor used in the Bayesian paradigm. When we reason about uncertainty, we are considering a set of possible worlds in which the evidence available to us has different implications. But even for some relatively mundane problems, this set of possible worlds is noncountably infinite. In the general case, we need to reason about countless possible worlds-but obviously this is impractical, because it would demand infinite computational resources. Somehow, we must select an efficacious subset of possible worlds and discard those that can be disregarded without too much risk. But how are we to do this when we do not know what it is that we do not need to worry about?
To revisit the target tracking example mentioned earlier, say we are tracking an unknown number of targets in a defined area. Obviously, we need to define variables about whether each potential detection is from an actual target of interest, which of those detections should 16 In a continuous case, a probability density function describes the probability that the variable takes on a particular value-but the variable is assumed to have one and only one "true" value. Russell and Norvig, 2005, p. 524. 17 Hector J. Levesque and Ronald J. Brachman, "A Fundamental Tradeoff in Knowledge Representation and Reasoning," in Ronald J. Brachman and Hector J. 
Levesque, eds., Readings in Knowledge Representation, Los Altos, Calif.: Morgan Kauffmann, 1985.
To overcome these challenges, a robust system for reasoning under uncertainty will need to be equipped with a dynamic ontology that can be modified and extended on the fly to learn new concepts and remain computationally efficient. This seems to be how humans grapple with uncertainty. As we gain familiarity with a novel situation, we often come to conceptualize it in very different terms than we did initially. Moreover, we often invent new concepts and heuristics to navigate this uncertain situation. Contrast this with the classic Bayesian formalism, in which all propositions must be known at the outset and nothing outside the support of the prior can ever be learned. 18 Prominent AI researchers, such as Douglas Hofstadter, suggested decades ago that creating machines with "general" intelligence will probably require endowing them with a similar ability to be introspective and self-modify their own ontologies. 19 Despite this, to date only a handful of AI systems have evinced even a token ability to do this, and it does not appear to be a target of much active research. 20   
These challenges suggest some far-reaching, and sometimes counterintuitive, implications for information fusion and other forms of reasoning under uncertainty in defense applications. For instance, they indicate that more sensors are not necessarily better; they may very well turn out to be worse. In the abstract, it seems intuitive that, all else being equal, more sensors should increase the likelihood of reconstructing the state of the environment accurately. This intuition is true in a cosmic sense but does not apply when we must account for 18 Andrew Gelman and Cosma Rohilla Shalizi, "Philosophy and the Practice of Bayesian Statistics," British Journal of 
Mathematical and Statistical Psychology, Vol. 66, No. 1, 2013
Work," Artificial Intelligence, Vol. 23, No. 3, 1984.
These obstacles grow even more imposing when we consider the knowledge quality problems associated with the available evidence. In practice, we often do not know what, if any, of the evidence available to us is actually good quality. This prevents us from simply starting with the best evidence and incorporating more as time and resources permit. We must also consider the possibility that some of the evidence is not merely of poor quality but actively pernicious. While in contrived scenarios we can make convenient assumptions that evidence will have known or zero bias and regular noise, reality tends to be less felicitous. Adversary disinformation obviously falls into this category, but sometimes very misleading evidence results from natural processes. Even in the absence of adversary action, many organizations exhibit a tendency to process random noise as signal given biases in data collection and analysis.
Therefore, computational complexity and knowledge quality problems present imposing obstacles to quality reasoning under uncertainty. Because of the properties of the underlying computational problem, we cannot solve the problem by simply buying a bigger computer. Brute-force solutions would demand astronomical computational resources, and Moore's 21 Paolo Braca, Stefano Marano, Vincenzo Matta, and Peter Willett, "Asymptotic Efficiency of the PHD in Multitarget/Multisensor Estimation," IEEE Journal of Selected Topics in Signal 
Processing, Vol. 7, No. 3, 2013;
Florian Meyer, Paolo Braca, Peter Willett, and Franz Hlawatsch, "
[1]
[2]
1996)
Law cannot be counted on to save us. Instead, we must seek shortcuts of various kinds: approximation algorithms that might sometimes give inaccurate answers and/or solutions that assume a simpler, more tractable underlying problem. If we cut the right corners, we may attain the results we seek with the informational and computational resources available to us. But to pull off this feat, we need to know which corners to cut-and we cannot be sure that we have the knowledge necessary to do this. The adversary gets a vote, and these simplifying assumptions may prove a highly vulnerable attack surface. If we are tricked into making the wrong assumptions, we may play right into the enemy's designs. As Edward Feigenbaum put it, "in the knowledge lies the power." 23   
Given the challenges discussed and their implications for national security, what should research focus on going forward? In science fiction, as well as in many visions of the future role of AI in defense, computers conquer uncertainty once and for all. 24 However, there are compelling reasons to believe that computers will not be dramatically better at reasoning under uncertainty than humans. Theoretical analysis shows that rigorous thinking about the unknown would require effectively infinite computational resources. The difficulty of reasoning under uncertainty is a key reason that we may not be able to get AI to do what we want-but what should we do about this?
First, we need to temper our expectations. Progress in computer technology cannot be expected to automatically bring about "dominant battlespace knowledge"; given the relative potential of automation for enhancing deception, our situational awareness of future battlespaces might be worse than we have experienced in recent conflicts, not better. 25 However, the difficulty of reasoning under uncertainty also presents opportunities that the United States and its allies could exploit to their advantage. If reasoning under uncertainty is a wicked problem, can we force or trick the adversary into trying to solve that problem? If we find the right approaches, perhaps we can make uncertainty work for us, not against us. 23  To attain our defense objectives, we need to set realistic expectations for systems that reason under uncertainty. This goal requires comprehensive research. We have considerable empirical experience with various experimental systems and some relevant theoretical findings, but we have yet to integrate these into a method for predicting the real-world performance of operationally useful systems. The risks of failing to develop this capability could prove grave. Inflated expectations could have pernicious consequences that might ultimately culminate in defeat on the battlefield. During the past three decades, many analysts envisioned concepts of operations based on the assumption that information fusion and reasoning under uncertainty already were solved or would be solved within the foreseeable future. Research and development funds were allocated to systems that would exploit the possibilities of perfect situational awareness, not to attaining better situational awareness per se. These misconceptions also heavily distorted long-range planning. Most predictions of how AI and other emerging technology will eliminate uncertainty continue to be based on hope, not technical analysis, despite critiques of past debacles associated with this error (such as the Millennium Challenge '02 exercise). 
26
27
28
The first thrust of research for setting realistic expectations for systems that reason under uncertainty is theoretical. Computer science has produced some tools to begin tackling this problem, but they must be further cultivated to bring them closer to practical systems. For instance, computational-complexity results for DBNs focus on the worst-case complexity of specific subclasses, but those subclasses may not be those used in practice. 29 It might be useful, for instance, to analyze theoretical analogues to those systems that combine Bayesian and non-Bayesian elements, such as MHTs. Non-Bayesian components, such as the pruning and gating mechanisms in tracking systems, might be treated as oracles in such analyses, somewhat bridging the gap between our practical systems and our theoretical models.
A particularly significant insight that better theory might provide would be a means to predict the turnover point (where additional information or sensors would cease to be useful). As noted, the location of this point will depend on the quality of the evidence being considered, and a theoretical treatment of knowledge quality would be a major enabler of such analysis. The usual information-theoretic treatment of informational flaws as noise may be inadequate when confronting an intelligent adversary who can introduce carefully designed disinformation into the environment. It seems useful to draw a distinction between such disinformation, which is signal from the standpoint of information theory but is of negative value as knowledge, and generic, naturally occurring noise. Introducing a "knowledge value" component to analysis could overcome this issue and help define the turnover point for various situations by serving as the basis of cost-benefit analysis.
The other line of research needed to set realistic expectations for reasoning under uncertainty is practical. Empirical tests are needed to see whether real systems adhere to theoretical limits, as real-world implementations might outperform worst-case assumptions. To do this, we need to have a sense of what both the average case and the "planning case"-the most extreme case we expect to encounter in an adversarial environment-will be like. Such definitions are essential for informing theoretical research and understanding the insights of that research. Much empirical research can be accomplished with toy systems in contrived environments, such as simulations. There also may be opportunities for large-scale, cost-effective empirical analysis by piggybacking on existing practical systems, such as multi-target trackers. Such piggybacking may make it possible to carry out the necessary empirical studies with a minimum of additional expenditure.
If we can set expectations about how reasoning under uncertainty will work in practice, we might be able to instrumentalize uncertainty to work for our interests. The risks of failing to explore these possibilities could be great. If we neglect the possibilities of instrumentalizing the hardness of reasoning under uncertainty, a rival might beat us to this capability and weaponize it against us. Even if no adversaries do this, we may still be depriving ourselves of a potent new capability. If we can make adversaries reason about uncertainty in circumstances of our choosing, we may be able to attain our objectives at much less cost in blood and treasure. However, we are obligated to study this space to develop defenses even if we decide not to exploit it ourselves. These defenses could involve taking active countermeasures and reducing and disguising our vulnerabilities. However, to identify these vulnerabilities and shrink our attack surface, we have to be able to perceive that attack surface.
For instance, imagine a scenario in which we were competing in an undergoverned space with a sophisticated, near-peer adversary. We would need to be confident that we were allocating our computational and other resources efficiently to best understand how and when the space is undergoverned. We can anticipate that the adversary will be trying to complicate this task for us. They might be doing this through classic ambiguity-increasing or ambiguitydecreasing deception tactics. They could also be trying to impose computational costs by introducing uncertainty that is optimized not to disguise the truth but to increase the amount of processing needed to ascertain it (making "known knowns" more expensive) or by sowing doubt about how well we even know how to describe what is going on in the undergoverned space (aggravating "unknown unknowns"). To recognize these tactics, we need to know what to look for-and without research, an adversary might subject us to such tactics without our being able to tell.
As with a research program to set expectations, understanding how uncertainty might be instrumentalized can be divided into a theoretical part and an empirical part.
Fortunately, formalisms such as DBNs provide us with a rich foundation to conduct theoretical research. The DBN formalism suggests some ways to categorize different kinds of uncertainty that an adversary might attempt to exploit. For the sake of discussion, assume that the defender has excellent self-awareness and knows all the values associated with the belief net describing itself-that is, its internal state variables and the probable evidence variables that an external observer might detect. The defender aims to complicate the rival's ability to reconstruct these values, but its actions could take very different forms depending on its goals. Perhaps the defender cares little if the rival learns the true values, but merely aims to impose costs by making the rival work harder to learn them.
An obvious way to do this is simply by increasing noise, but there might be subtler or more focused strategies, such as adding targeted spurious variables and focusing noise around selected true variables in a manner theoretical analysis suggests will increase the difficulty of reasoning about the problem, even if it is not guaranteed to mislead the observer in the end. Another obvious case would be when the defender hopes to mislead the rival about the state of a few selected true variables. Such deceptions could have a variety of characteristics: An observer might experience a certain kind of uncertainty-"The value is between 3.5 and 4.2 and probably on the high side of that"-instead of making a specific wrong conclusion-"I'm sure the value is 7.1."
A particularly important goal could be to prevent the rival from correctly inferring the graph structure describing the relationship of the defender's state variables, as opposed to the variables per se. In many cases, this structure is much more important than the momentary state of its constituent variables, because it can be exploited to reconstruct other parts of the state under previously unobserved conditions. Once again, the formalism suggests efficient ways to accomplish this: One can add new variables, but there is also the possibility of convincing the rival of the existence of spurious edges between real variables. Such deceptions could be designed to reduce the accuracy of inference or increase the computational cost of inference. Finally, instrumentalizing uncertainty is not just about making rivals uncertain. Sometimes, we want to make sure that a potential adversary is absolutely certain about something that is true. For instance, to avoid undesired escalation, it is imperative that the rival not perceive a possibility of an imminent attack that does not exist.
With the definition of an appropriate metric, we can design algorithms to optimize gambits such as these. A concept like the knowledge value suggested earlier could serve as the basis for metrics to measure the efficacy of uncertainty-manipulation methods. For analytical purposes, this might be defined as "the value to the defender of the action that a boundedly rational rival will take given how they perceive a particular state," with boundedly rational defined as "instrumentally rational subject to finite computational resources for approximate Bayesian reasoning." 30 This would mean that an agent attempts to use the most accurate approximations that it can compute for the probable state of the world and that it acts rationally to pursue its goals given those perceptions. As noted, such a metric requires assumptions about the rival's ontology and preferences to predict their computational complexity. However, this is an unavoidable aspect of formalisms of this type (consider algorithmic game theory). 31   
The empirical aspect of the research program to instrumentalize uncertainty would use simulations and practical experiments to test both uncertainty-manipulation techniques and the applicability of proposed metrics. Simulated sandboxes could be used for both simulated agents and human test subjects. Given the unresolved mystery of human reasoning about uncertainty, a critical consideration is whether humans are better at overcoming or detecting uncertainty manipulation than predictions based on such theoretical abstractions as boundedly rational Bayesian agents. Alternatively, observational tests might find that humans have specific cognitive vulnerabilities that theory fails to predict. It is well known that humans employ various cognitive shortcuts, and some researchers have long sought to formalize these heuristic mechanisms to simulate them on a computer. Methods from cognitive science and existing cognitive architectures might be adapted to assist these inquiries. 32 The resulting findings could, in turn, inform updated theories and metrics, as well as the design of experiments to test them. 30 This is basically the same as Kenneth J. Arrow's definition of bounded rationality (Kenneth J. Arrow, "Is Bounded Rationality Unboundedly Rational? Some Ruminations," in Mie Augier and James G. March, eds., Models of a Man: Essays in Memory of 
Herbert A. Simon, Cambridge, Mass.: MIT Press, 2004, p. 48
Machines capable of efficient, rapid, and accurate reasoning under uncertainty could revolutionize both civilian and military affairs. The allure of these possibilities has compelled generations of AI researchers to attempt to create such systems. Over decades, they have pioneered a succession of different approaches to this goal. However, despite some real successes, they have yet to reach it. As AI researchers' theoretical understanding grew, it became apparent that this disappointment stems from the nature of reasoning under uncertainty. This problem turns out to be computationally complex and epistemologically fraught. There is no single correct or optimal way to reason about uncertainty, because there is more than one way to be uncertain. AI researchers have translated some of these alternative modes of uncertainty into algorithms. Notable examples of these are Pearl's Bayesian belief networks, Zadeh's possibility theory, and Dempster-Shafer theory.
Although we can envision ideal systems for reasoning under uncertainty, these require unobtainable computational resources. As a consequence, actual systems must make tradeoffs between speed, accuracy, and expressiveness. In essence, to reason about uncertainty using machines, it is necessary to weigh between a set of possible worlds consistent with the available evidence. However, for a nontrivial problem, these possible worlds are too numerous for a physical computer to represent and reason with. A real-world system must instead work with a smaller subset of possible worlds; in some cases, this shortcut can enable good performance, but, to make it work, one must possess accurate knowledge about which subset will be encountered in practice. As a consequence, computers and AI cannot be expected to eliminate uncertainty.
We must learn to live with uncertainty, but we can mitigate its hazards and perhaps even make it work for us. AI research on the problem of reasoning under uncertainty can serve as the foundation for investigations of these possibilities. First, we must set realistic expectations for reasoning about uncertainty. If we cling to ill-founded hopes that computers will slay the dragon of uncertainty for us, we are likely to misallocate resources and might even suffer battlefield defeat because we placed too much faith in flawed systems. A dual-pronged research program with both theoretical and empirical components could help demystify these issues for us. In particular, theoretical considerations suggest that there is probably a point of diminishing returns beyond which the computational costs of reasoning with more information are greater than the additional value that the information ends up providing. Second, we must confront the possibility that uncertainty might itself be turned into a wieldable instrument of state power. If reasoning under uncertainty is a hard problem, perhaps others can be compelled or fooled to try to solve those problems. Even if we decide not to solve the problem of reasoning under uncertainty, we need to study the problem for defensive purposes. The same sort of theoretical and empirical research needed to set expectations for reasoning under uncertainty could suggest not only possible ways that uncertainty could be instrumentalized but also prospective defenses against those possibilities. The Pentagon. Whitehall. The Kremlin. Foggy Bottom. Horse Guards. Quai d'Orsay-these are how people once referred to the foreign policy and national security establishments of major nations. In the past, one could easily envision men (and it was only men who traveled in this world) arriving with briefcases to an office, sitting at desks, joining deliberative processes in meeting rooms, taking lunch in clubs or restaurants and then working often to a late hour illuminated by candle, gas, or incandescent lamps. During the days, officials would sift information, sort it in accordance with standard rules of thumb and established protocols, consider the information gained through their intelligence services' observation of their opposite numbers, debate policies and, over time, frame actions and responses that would then be commended as courses of action (COAs) to respective governments in minutes, memoranda, and white papers.
That was then, this is now. There are new players (some undetected for long periods); new arenas of competition; new stakes; an ever accelerating pace of communication and hence a decreasing time cycle of decision; widening variation of intentions, objectives, and strategies; and a vastly more voluminous information flow paradoxically accompanied by an alarming rise in fundamental uncertainties. The nature of international interaction has changed and, along with it, the processes and protocols of decisionmaking.
But have the processes of decisionmaking changed enough? In particular, has analytical support for decisionmaking made possible the type of transition that the new era calls for? In the face of fluid conditions, deep uncertainties, and changing relationships, what capabilities would be the most desirable in an apparatus for conducting deliberation and analysis of policy alternatives on national security, relations with allies or potential adversaries, and dealing with today's uncertainties and their challenges to U.S. long-term goals (as expressed in governing policy documents, such as the U.S national security strategy)? 
1
2
Several aspects of contemporary NSE planning processes pose challenges for achieving the objective of becoming more flexible and adaptive in the face of greater uncertainty. We identify six obstacles and what we might do to overcome them in NSE planning.
The United States has lost few battles in more than half a century. But there are few wars in which it has achieved the political outcomes it stated at the onset. This might partly stem from the strong distinction drawn in the United States between civilian policy and military operations and, thus, a certain bifurcation of focus. This exacerbates the fundamental difficulty of thinking along multiple timescales in parallel, especially in complicated or illdefined arenas of conflict, such as undergoverned spaces. Knowing the next steps after securing battlefield victory is as important as achieving the victory itself. In the absence of the first, the second becomes hollow. The United States paid the price for that bifurcation in focus in the war in Iraq during which it easily won the military battle but did not consider sufficiently the consequences of victory. This led to the political vacuum in postwar Iraq. Being able to plan operations, evaluate intelligence, and trace operational pathways from means to ends within a consistent decisionmaking framework would support the difficult task of relating short-term actions to prospective long-term consequences.
The concept of uncertainty absorption formulated by Nobel laureate Herbert A. Simon captures the phenomenon of the lower levels within an organization being more cognizant of uncertainties pertaining to the sources, character, and quality of intelligence than are leaders in the higher levels. 3 What moves upward through organization channels is not raw intelligence gathered at the lower levels; rather, it is syntheses and interpretations based on such information. Nuanced understanding of the variation among sources is necessarily stripped away so as not to clog channels going upward and possibly compromise organizational function. Unfortunately, the ability to drill down and examine the foundations for an interpretation is also often stripped away or lost in transmission. What this loss leads to is less communication of subjective risk perceptions than might be purposeful within organizations and points to a need for processes better suited to enhancing the quantity and quality of information exchange. The quantification of intelligence, particularly the characterization of uncertainty by probabilities, might lead to similar effects. The capacity to convey messaging on COAs, means, and goals while retaining tools for interrogating an interpretation's underlying determinative factors would restore some of the nuance lost in the process of synthesis.
Establishing mission-oriented offices and agencies necessarily gives rise to inter-agencyand intra-agency-stovepipes of information and responsibility. The ideal would be to carry forward integrated discussions on policy objectives, intelligence, strategic concepts, operational requirements, mission requirements, existing and prospective capabilities, and mission-agency organizational goals. Although this is difficult enough to do within one mission agency, the difficulties of crafting comprehensive processes across those agencies are daunting and yet vital. The more complex the problem, the less reliable should be the confidence placed in any one organization to be uniquely authoritative and sufficiently expert.
Victory disease-a term coined by Japanese officials to describe the state of mind of war staffs after early victories in World War II-equally applies to U.S. experience, such as the Iraq example, the shared situational understanding prior to the Battle of the Bulge, or the march to the Yalu in 1950. The phrase rolls up aspects of the confirmation bias and communal reinforcement (groupthink) noted by psychologists. Any planning team may perceive in the evidence before them, particularly when questionable and less than clear, encouraging signs for the COA being advocated or pursued. The capacity to reformulate such evidence into configurations that might be equally plausible or depend on different assumptions that cannot be rejected based on information at hand is not only absent but consciously or unconsciously resisted. It is challenging to risk the ire of (and perhaps ostracism from) a planning group already under pressure for delivery and pleased with its own performance by raising latestage doubts. Less disruptive embrace of red teaming within the flow of the planning process itself could shore up weak points of potential failure.
As a related matter, there is value in also constraining proof by loud shouting which is the ability of the most prevalent or well-articulated views to dominate. A decision process can often be gamed by adding redundant data and modeling runs that will, in effect, limit other voices from being heard and perhaps even lock them out entirely. There is value in an analytical approach that will limit domination by repetition and instead reward diversity in a systematic approach that is also purposeful and operationally meaningful.
DoD has embraced scenario planning of future contingencies and the resulting potential demands. Integrated security constructs (ISCs) help enhance the joint understanding of potential operational and mission demands and so, working backward, the materiel, skills, and readiness posture required to prepare the forces needed to carry out missions. The process of generating ISCs generally enfolded into the Quadrennial Defense Review (QDR) process is complicated, costly, and subject to negotiation and approval by all the Services and offices of the defense establishment. Once the approved set of ISCs is compiled, all involved are both relieved and loathe to fiddle with them further. Worse, no one wishes an interservice contretemps to flare up by having new conditions supposed that would be viewed as invidious to policies and programs of one or another service once underway.
In short, the system manages to again remove uncertainties not explicitly captured during ISC development from further consideration. This is the antithesis of value that the ISCs were intended to provide. As with some of the dysfunctions already noted, the organizational psychology of dissent in planning is even more fraught than it previously was. There is a need to recapture the value intended to be gained by making possible scenario thinking within the national security planning apparatus.
A precarious value within an organization or process is one that has not been sufficiently defined, is seen not to have received sufficient legitimacy through leadership support, or appears to be inimical to widely held understanding of vital missions. 
4
Foresight offices or functions are created with much fanfare and then wither or disappear entirely in one organizational shuffle or another. Those that do not perish have mastered one simple task: They have caused internal demand for their output to grow. The value proposition for organizational foresight activities must be made sufficiently strongly that the line units of the organization perceive it.
Taken together, overcoming these six obstacles in planning form a tall order. They appear collectively to be a stretch well beyond the contemporary capability to fulfill them. Few would argue with their desirability, but practical considerations consign them to a realm of aspiration beyond realization. The very rise in uncertainty that calls out the need for change would appear to overwhelm the attempts to deal with these obstacles credibly in a meaningful analytical construct and organizational setting.
However, more explicit recognition of the same deep uncertainty that has so complicated the task of national security policy analysis may serve as the way to deliver on the value propositions outlined. The overwhelming problem of sheer numbers-so many uncertainties multiplying in interactions with each other-that manifests as myriad pathways from the present to the future gives pause. But it also raises a fundamental and perpetual concern of analysis: Are we asking the right questions given what we face? There is also the sensitive concern of whether the NSE as a whole is well served under new circumstances by the traditional separation between its intelligence-gathering, analysis, and knowledge creation component in the Intelligence Community (IC) and deliberations by the planners and decisionmakers (known as the policy community) on the other, with little comprehensive reference between them. If more direct interaction within the NSE is deemed potentially valuable, then how can the connection between the two be brokered without introducing another problem of policy contaminating intelligence-or vice versa?
Three Pathways: The Role That Computation Might Play in Analytical Support to the NSE Beginning with World War II, computation-or information machines-came to play a role for the military. From the U.S. Navy's development of combat information centers aboard its ships to the early computers used to crack the Enigma machine and reveal its coded information, this new type of machine became ubiquitous and preponderant. Command, control, communications, and information (C3I) had always been crucial in determining battle outcomes, but it was most conspicuous by the extreme difficulty in gaining anywhere near enough of each, to say nothing of their integration. Incremental gains in advantage were hard won. But now machines rather suddenly became capable of vastly enhancing these capacities.
We are still in the midst of this revolution. As technical means advance, computing moves from just providing information and communications to helping with decisions. The extent to which computing is brought into-or becomes in itself-a decision system deserves some consideration. There are three different but related channels for doing so-prediction, automated and expert systems, and adaptive planning-but they are sufficiently distinct that there is value in recognizing the differences among them. Each has a distinctive approach to dealing with the problem of making decisions in the presence of uncertainty.
Military commands and national authorities have always sought to improve prediction, whether of adversary actions, external conditions affecting operations, or of likely outcomes from employing a COA. We have moved from Delphic oracles or auguries reading the entrails of sacrificed beasts to more-sophisticated techniques. Computer models and simulations are powerful tools for rigorously examining systems and outcomes. Such models are extraordinary human artifacts that encapsulate mountains of knowledge and experience gained in many fields. Models keep track of myriad causal relationships and compute complex interactions among and within various systems that influence one another. As much as any other technological advance, they extend the powers of human cognition and perception.
At the same time, model-based prediction also carries limitations and presents risks increasingly likely to be present the more we proceed in a direction that might convince us that we are actually getting somewhere. If so, enhanced awareness extended beyond the narrow purview of such simulation systems themselves might prevent us from falling into potential traps.
Predictive analytics necessarily take a narrow view of what constitutes models and what purpose they serve. The term model usually suggests a representation of reality in an artifact that, although limited, nevertheless seeks as much as possible (given resources and the specific analytical requirements being served) to portray the structure of the underlying true model of the system in question. Thus, the validity of such a model is determined by its predictive power, as in physical science or engineering, or postdictive capacity to explain variables' time series as in social science. The model not only becomes the central feature of the analysis, but alternative uses of models for nonpredictive exploratory or explanatory purposes are disparaged or ignored. The analytical enterprise becomes one of seeking to reduce uncertainty and verifying which of the many different sets of assumptions about causal relationships or the values of future model inputs can be shown most likely to prove true. The output from this research path focuses on the likely future states of the systems in question and their component elements; therefore, the path only indirectly touches on those questions raised by policy planners or decisionmakers. This is not to gainsay the power of predictive analytics or disparage its achievements. But we need to recognize the point beyond which this method is no longer appropriate to the purpose when applied solely on its own. Unfortunately, the breakdown occurs precisely when it is most needed-when alternative choices for short-term action are many, systems are complex, and the uncertainties present are difficult to characterize probabilistically owing to either a deficit of information or fundamental disagreements about what the data we possess might mean. 
5
6
Another concern is that the quest for more reliably predictive modeling could create an increasing black box problem. Fewer people become capable of comprehending what the model contains and so more are disenfranchised from supplying meaningful insights, expertise, or critiques. In particular, those assumptions made explicitly at the onset to allow for computational tractability will, over time, become implicit and less apparent even to those inside the group who really understand the model-to say nothing of the policy professionals who constitute the ultimate consumers of the output. Thus, potential points of departure from an unfolding reality may fail to be perceived.
The final concern is one of dependency and a false sense of confidence. The unknown raises anxiety. The greater the extent to which we feel in control over what the future may bring, the better we can anticipate it and the more confidence we feel. Predictive models can inculcate an illusion of control. Like a witch or sorcerer from a fairy tale who can exert dominance by learning the true name of a thing or character, we feel that the more planners can learn the future's true name-that is, chart out estimates of the likelihoods of different outcomes-the greater the unspoken sense of control planners are likely to feel. We ease our anxiety as individuals and as professional planning teams at the potential cost of amplifying the opening for, and consequences of, surprise-precisely the opposite of the intended benefit from the resources devoted to predictive modeling of deeply uncertain decision spaces.
The second path for bringing computing into planning and decisionmaking is a logical extension of trends elsewhere, although the path is perhaps less applicable to the planning problem in the short term. That path is to enhance our reliance on machine learning (ML) and the various approaches to artificial intelligence (AI). The machine becomes the decision system itself rather than supporting a human-moderated process. This path is far less advanced than that of enhanced prediction and remains more of a prospect than a tangible alternative. Expert systems do exist for remote medical diagnostic screenings and robotic surgeries too delicate for a human surgeon to perform reliably or achieve satisfactory outcomes. Expert systems also exist for assessing visual data. Journalism algorithms already generate business reports or sports coverage. But there is also increasing interest in using such autonomous systems in the military; if perfected, such expert systems also could conceivably be useful applications in national security policy planning.
As with predictive modeling, it is possible that future advances in computing will make this an increasingly tractable and accessible alternative. But this is unlikely to be the case in the foreseeable future. Therefore, this path will also have shortcomings that planners will need to be aware of. The first is that AI-based systems have substantial black box aspects. The entire purpose of a recursively trained system is to develop system-generated algorithms permitting it to achieve reliably positive outcomes against an increasingly complex set of challenges or indicators and to do so in a fraction of the time that humans would require. But by their very nature, such algorithms may be difficult for observers to fathom or document. Thus, it will require ceding a measure of human control. Added to this are the welldocumented cases of algorithmic bias in which the repetitive reinforcement of algorithmic assessments might incline the system toward inferences and thus solutions based on unintended or even undesirable foundations. Researchers in this field have also found that unintended features in the data sets themselves can reinforce the tendency toward bias. Finally, there is a substantial ethical dimension in relying principally on the AI system's expertise, depending on the decisions being generated. Adding such automaticity to weapon or decision systems would bring us into a new world with uncertain prospects.
The last of the three paths is the one we explore in the balance of this chapter: a human-in-theloop approach to computer-based analytical support that creates adaptive planning systems within which human deliberation is supported by iterative analyses based on and feeding back into those deliberations. The approach represents a division of labor between computers and models doing what they are best at-tracking connections and generating cases based on data inputs, models and assumptions-and people doing what they are best at-discerning patterns, drawing inferences and, above all, posing more and different questions.
Before further considering the technical tools that may help fulfill the needs discussed in the first section of this chapter, it is worthwhile stepping back and looking at a critical juncture within the NSE. By focusing on it, we can motivate an approach toward change across the fullest set of NSE processes.
A characteristic recent debate in the NSE has been how to improve the ability of the IC component to provide the policy community with reliable information (and the analyses used to convey information) when confronting the unknown. More generally, we can refer to this as the problem of knowledge-gathering and characterization-a function largely but not exclusively in the IC. This task is considered the IC's lane in the ideal-that of pure intelligencegathering and exposition as opposed to the functions of policy planning, deliberation, and implementation. If the lines are blurrier in practice, there is nonetheless a wall intended to exist between intelligence and its knowledge formation activity and the policy deliberation process. The IC's straying beyond its limits would be viewed as running the risk of corrupting both the intelligence-gathering and policy deliberation processes-that is, politicizing the IC.
This ideal conception of the IC's role as focused solely on knowledge occasionally places it in the roles of either making forecasts or filling in blanks. At the same time, the prediction game has grown increasingly perilous, as shown by the IC's lapses in providing early warning of the Soviet Union's collapse; the Arab Spring; the September 11, 2001, terrorist attacks; and Iran's Islamic revolution. Perhaps the conception of the IC's modern role should be a bit different given the acceleration (along with decreased predictability) of change. Perhaps it always should have been so.
The IC must always be looking beyond the horizon. But for what purpose and as measured by what standards of performance? The same IC machinery used to gather information and frame analyses could be put toward endeavors with enhanced potential value within the NSE: providing analytical support to policy decisions. This line of effort would not be providing policy advice, but it would go beyond situation reporting as an end deliverable. This enhanced knowledge project can enhance policy actors' understanding of available COAs and the potential implications of those COAs across different plausible futures. And in the context of "wicked" problems with a plethora of variables-many hard or impossible to characterize by probabilities-the task would be to provide analytically informed assessments of which variables should weigh most prominently in the decision process so that short-term actions can closely conform with long-term policy objectives across many plausible futures.
This means a shift from trying to answer the question of "What will happen?" when it is objectively difficult to do so-that is, analysis as means to resolve which among the clashing assumptions will prove true in the future. Instead, the decision support focus would seek to provide input to illuminate questions more recognizably useful to the policy decision process:
• What future possibilities might affect the ability to achieve policy goals? • How fragile is the current or intended strategy or COA to such changes?
• How could we modify our strategy to reduce exposure to such vulnerabilities? This shift in posture and purpose for the data collection and analysis functions within the NSE would not require corresponding shifts in personnel, training, or modeling infrastructure, nor would it detract from reporting requirements. Rather, the same machinery can be leveraged to serve an enhanced rationale-one that is consonant with the historic tenets and purposes of IC activities but perhaps more suited to shifting and uncertain times. It would contribute to making the NSE more adaptive in two senses of the word. First, it would be better tuned and potentially responsive to the pace of external changes. Second, it could enable a posture for policy planning and deliberation that would be tuned to the need to strive for and embody robustness within the design of plans.
The term robust carries several denotations. Most relevant for this discussion is to define a plan as robust if it is one that performs well, compared with the alternatives, over a wide variety of plausible futures. 7 Robustness in this sense is a comparative quality. It emphasizes the decision support over the neutral knowledge-gathering aspect of the NSE. Robustness focuses on comparative advantages, disadvantages, and trade-offs among alternatives. A robust strategy might not be an optimizing strategy based on a specific set of presumed circumstances. It is more likely to do well enough across many plausible future circumstances than do as well as possible in many of them. But to the extent that it fails altogether in meeting objectives, it is likely to fail more gracefully (i.e., with less dire consequences) than might an optimizing strategy when it finds itself confronting similarly invidious circumstances compared with those it had been designed to operate within.
Performing well suggests a trade-off between meeting policy objectives and performing satisfactorily in many different futures. It also suggests the explicit establishment of criteria for assessing trade-offs. National security decisionmaking rarely has a single bottom line suf- 7 Jonathan Rosenhead, Martin Elton, and Shiv K. ficient to stand in for all interests. For one thing, actions designed to achieve a short-term goal (e.g., disrupt adversary preparations) could be injurious to long-term objectives (e.g., returning adversary behavior to international norms). For another, any action has costs, whether weighed in terms of dollars, administrative attention, or political capital. And any action may well have direct or indirect influence on other interests. For all these reasons and more, the NSE decision process is a multi-attribute valuation problem requiring balance and nuance. In this context, performing well means meeting the several criteria established by policy leadership. This factor alone would mean leaving optimization behind and instead conducting analyses on a basis that comes closer to the satisficing approach that Herbert A. Simon suggested more realistically approximates the behavior of decision leaders within organizations: Their job is to find positions they judge to be well hedged between opportunity and threat. 
8
Robustness and the capacity for adaptation that often is its motive force might be values used to distinguish and select among policy alternatives, but they are not ends in themselves. Robustness is a comparative value; there is need for a comparative yardstick that still centers on the goals set by policy leadership. So rather than measuring in absolute values, it is useful to employ the concept of regret to weigh choices. The regret of a robust strategy under a specific set of conditions is the difference between its performance along one or more attribute scales and that of a strategy that would have been optimized for those conditions. By definition, the latter strategy would have zero regret. The same is true for any two or more candidate robust strategies. By definition, one would have zero regret-among the available choices, that is the one that would do the best. Each of the others, if they did not perform the same according to the selected measure, would have some level of regret. This allows assessment of policy choices across many plausible futures and the several attributes for measurement that will have been selected by the decisionmakers. 
9
NSE decisionmakers increasingly face growing uncertainties, dynamic links among complex systems, contention over assumptions and perceptions, divergent interests, and need for coordinated action even if consensus on trends and futures is elusive. Deep uncertainty exists when it is not possible to predict-or agree on-the probable values of future factors, competing models of causation cannot be rejected with the available evidence, or normative agreement on how to assess outcomes as successes or failures is contentious. 
10
11
12
The hard challenge lies in the nontrivial effort of adapting the methods and conjectures of this nascent field and combining them with others in innovative ways to provide the envisioned capabilities to the NSE. Most renderings of the Heilmeier Catechism 13 used by the Defense Advanced Research Projects Agency to explore the merits of prospective research programs would ask, among other questions, what are the questions of interest that decisionmakers (for the purposes of this discussion, NSE decisionmakers) need to address, and what are the challenges to current methods for doing so? The prior portions of this chapter addressed these two points. The balance of this chapter is designed to answer the Heilmeier questions: What would a new approach look like, and how could a new program help to improve capabilities? Although there is no example of a complete structure for robust decision-based analytical support within the existing NSE, we walk through how such a process might be conceived and conducted in the next section. The focus is on the capabilities of a reconceived decision process rather than the details of actual practices within the offices and agencies all such candidates lead to disagreeable results in absolute terms when compared with policy objectives. that play roles within the NSE. Innovation would be required on the human and organizational aspects of implementation and process engineering, at least as much as for the technical aspects, for effective transitioning of this technology. Understanding how to do the least injury to existing working and reporting practices and being tuned to the differing formats of argumentation used within and between different offices (e.g., well-constructed narratives versus graphics or numerical tables) would be important values.
What follows is a high-level sketch of one possible robust decision-based architecture within the NSE. At several points, we will reference examples drawn from different subject areas. These serve solely to illustrate and provide more detail on process. The methods discussed are intended neither to be comprehensive nor exclusive of the possibility of other techniques. Because of this chapter's focus on entry points for incorporating advanced computing into NSE decisionmaking, we will base the discussion on the Robust Decision Making (RDM) method for creating human-in-the-loop adaptive reasoning systems. 
14
A National Research Council panel concluded that an effective approach to "wicked" problems, 
15
16
11
The model-based RDM method is not intended to produce better predictions but instead uses quantitative models and data to inform better decisions. 
17
11
18
No serious discussion of strategy can occur without specifying the end objectives for which strategies are just the means. The parties to an NSE deliberation may be prepared to enter the decision structuring step immediately (center top of 
Figure 11.1)
11
How sensitive is the COA to the conditions?
What is the best short-term COA?
What COAs are of potential value?
Agree on decisions a more formal decision analysis is not feasible. Several nonquantitative DMDU methods can provide support. We will discuss two methods operated in parallel.
Standing in the present and looking forward is difficult, confusing, and contentious. It may prove easier for a mixed group to instead place itself in an explicitly defined future and look back. 3HF is a foresight technique that has been applied as a group tool for defining shared vision. 19 The focus is on establishing in some detail the characteristics of a desirable future (3rd Horizon) through a structured, qualitative process, characterizing the present (1st Horizon) condition in similar terms, contrasting the two, and identifying trends, obstacles, or trade-offs that might prevent the ideal 3rd Horizon condition from being realized. The heart of the exercise is to then identify and contrast alternative pathways for crossing the intervening period (2nd Horizon)-the "zone of conflict" in Curry and Hodgson's parlance. 20 These different pathways or COAs can be viewed as transition trajectories corresponding to different candidate strategic concepts.
In extending the 3HF output, ABP may be used to examine different alternative COAs 21 each based on one or more strategic concepts-to effect the transition across the 2nd Horizon. 22 ABP is a narrative technique originally intended as a way to discover important but implicit and potentially vulnerable assumptions within plans. When used at the outset of a policy deliberation, ABP framing can deconflict the more-usual advocacy behaviors found in policy discourse and enhance discussion of alternative choices.
ABP does so by comparing COAs side by side to elucidate for each their explicit and implicit load-bearing assumptions-those assumptions that, were they found to be invalid in the future, would then call into question the value of that COA as a vehicle for bringing about desirable outcomes. During this process, COAs may be modified or hybridized to shore up revealed weaknesses. After the winnowing process, each COA is then assessed for what signals might give early warning of impending vulnerabilities and what ancillary actions may be taken to either shape circumstances to be more conducive to the COA or hedge against its 19   20  
Curry and Hodgson, 2008.
11
As the world changes, the systems and concepts well suited to current conditions (solid black line) may work increasingly less well in the future unless they also change. If they did so, they would be better suited to achieving objectives in the 2nd Horizon time frame (dashed black line). Because of fundamental uncertainty and despite what we may intend, the systems put in place to sustain the 3rd Horizon state may be more or less well suited to achieve the planning group's vision (dotted lines). Choice of COA, early warning, and hedging and shaping actions can affect actual outcomes.
Returning now to Figure 
11
Broken assumptions
First Horizon Current state
Third Horizon Desired future state Alternative strategic approaches that might leave to the planners the task of determining what the results may imply for the decisions at hand. The former brings the decision question into the heart of the analysis.
The decision structuring step gains its power through reviewing the factors of importance to a problem and placing them in one of four functional categories that will be explored in an RDM analysis as shown in the text box. 23  The framing in the text box aids decision analysis in several ways. It provides a design specification for the model software in the RDM analysis. For example, the assignment of factors to the category of external uncertainties (X) versus that of policy levers (L) (which may be assembled as building blocks into a variety of candidate COAs) brings the actual policy process into the analysis; what is an external uncertainty to one group of policy actors is precisely the sphere of action of another. This reinforces a means and ends framing for the analysis versus detailed modeling of an entire system only to later find that only portions of that system are relevant to selecting among COAs (as opposed to predicting future outcomes.) This makes the modeling component more parsimonious of modeling resources.
Beyond the uncertainty that comes from not knowing values of important future factors, there is also structural uncertainty when there are alternative beliefs regarding causal relationships (R). This type of uncertainty means that two trained professionals, such as former United Nations Ambassadors John Bolton and Samantha Powers, can consider the same body of evidence and draw different policy conclusions, in part, because of different conceptions of underlying causal relationships. Finally, it is important to place exploration of measures of outcomes (M) at the same level of the other categories. Rather than just being the sausage that emerges from the NSE factory, policy choices will need to be made on the basis of trade-offs among prospective gains and losses. Thus, determining what set of outcomes will be deemed successful is also an active process of exploration. Creating the Table 
11
(Figure 11.1)
24
The resulting information is then evaluated in the discover vulnerabilities step. RDM invites an iterative process of discovery, reframing of questions, and COA refinement as indicated by the arrows showing recursive flow in the Figure 
11
In particular, scenario discovery is a process of determining COA vulnerabilities. Because of the limitations of unaided human perception, such ML-based algorithms as the Patient Rule Induction Method (PRIM) 
25
26
27
The implications of this capability for opening the aperture of NSE policy deliberation are potentially profound. Rather than the less tractable and often troubling question of "What will happen?" a new-and more operationally useful-one takes its place: "What would we need to assume or believe will be true to recommend selecting COA 1 instead of COA 2?" This crucially changes the focus from outcome prediction to illumination of choices among alternatives and supports decisionmakers in hedging against risk.
Applying the prior steps (often several times) provides data for trade-off analyses to compare candidate COAs. Because the goal is not to forecast outcomes but to compare the robustness of alternative plans for meeting defined policy objectives across different future states of the world, the key is the concept of regret: How much would we regret (in terms of measurable objective value forgone) having chosen a particular COA compared with the alternative COA that would have been optimal for that set of conditions? When trade-off analysis occurs, it is likely that prior iterations of COA modification will have reduced many potential vulnerabilities within the remaining modified candidate COAs. Therefore, we are not interested (from the perspective of policy decisions) in any remaining uncertainties and vulnerabilities that do not change the preference rankings among alternatives. Rather, we are now able to identify and focus on those variations in possible external circumstances that would change that order of preference ranking.  of the odds of that scenario occurring. COA 1's expected outcome regret is higher at almost every point than all the others. COA 2 performs well when the assumed odds for the stressful scenario are low, while COA 4 does better in situations in which stressful conditions might be more expected. COA 3 would probably not be considered in an analysis that was keyed toward optimization or that looked only at two to three scenarios. It also is dominated at most points. However, perhaps it represents a hybrid version of COAs 2 and 4. It shows the characteristic of failing gracefully: Across the unknowns, its regret measure is at most points second best. Its performance appears robust to uncertainty as measured by the ratio shown on the horizontal axis. 29   Output to Determine a Robust Strategy
The end process to RDM provides decisionmakers with strategy choices selected for robustness (the right side of Figure 
11
11
Probabilities have not been brought into the analysis proper prior to the trade-off step, nor has the generation of the set of plausible future values that form the test screen for plan performance been done on a random basis. 30 This means that the analysis has a drill-down capacity; senior leadership might ask to see individual cases and outcomes that lead to the curves shown in 
Figure 11.4
11
11
Popper et al., 2009
21, No. 2, 1979
The NSE is broad and encompasses many issues that may be analyzed through formal computer modeling; these issues include the transition toward a modernized nuclear triad architecture while dealing with nuclear-armed adversaries, reduction of the vulnerability of the domestic defense industrial base to different types of shock or disruption, and strategies of assistance for developing countries' transition to non-fossil fuel energy. But there are also principal preoccupations in geopolitics and diplomacy for which no formal models exist and for which modeling is not part of analysts' training. 
1
11
The word model usually implies a quantitative formalization. This fails to capture the concept's full potential. 
2
In this light, the world of foreign policy and diplomacy is rife with sophisticated, complicated models. However, they remain implicit, defined by the knowledge and accumulated experience of individuals, and not codified as formalized statements. A model sufficient to support RDM reasoning may be created through formal elicitation from subject-matter experts (or planners) of causal statements about the system or issue being considered. Differences between individual expressions of these causal constructs may be tested in precisely the same manner as any other exploration of differences within RDM. 
3
Any implementation of a resulting robust strategy itself has a recursive value in the Figure 
11
11
Foresight activities in any organization can prove to be a precarious value in that looking forward beyond the institution's preoccupations may be considered, consciously or unconsciously, at best as a luxury or indulgence; at worst, it may be considered a drain on the time and resources of hardworking people whose task it is to get the immediate job done. The only way to escape this trap is for the functions of sensing and adaptation-the core of foresight activity-to be perceived by those within the organization as providing value. The Observations and Data loop in the far right of Figure 
11
Journal, Vol. 6, No. 3, Summer 2009)
formal mathematical models, decision trees operated as Bayesian networks, or a rulesbased lexicographical mapping without being forced into the predictive role of assigning them probabilities.
A rebuttal to the argument that such models would be ad hoc and, therefore, potentially (or even necessarily) spurious, is the simple statement that such models exist and are already the implicit basis for argument and disagreement in the NSE deliberation process. The only difference with current practice is to elicit these tacit models, make them explicit, and render them in a form for comparison, testing, and leveraging their power for decision support in an innovative application. For an analysis conducted in this manner, the model is part of the analytical machinery but also an important output to be tuned and reevaluated as performance, outcomes, and data are subsequently gathered and assessed.
Using such a Robust Decision approach to NSE deliberations, even if formal models do not exist, would allow a realization of the ASDA vision inherent in Figure 
11
Enhancing the capability for analytical decision support in NSE policy processes requires reexamining the role and orientation of the classic posture toward intelligence-gathering, analysis, and dissemination. Embracing the concept of robustness-with its prospect of allowing for more adaptive NSE policy in the uncertain times that lie ahead-suggests the value of similarly reexamining the optimization-based strategic approach that is implicit in traditional policy analyses. More often than we would care to admit, policies are selected with the unstated assumption of maintaining relevance from the time of their implementation until the time for evaluation arrives. Although few policies are truly fire and forget in the NSE policy realm and course correction will be required-particularly when human adversaries are involved-the structure of deliberations is framed in a way that carries little explicit recognition of that fact. Red teaming is an adversarial, post-processing step following plan formation rather than an essential component of its gestation.
The idealized robust decision-based framing of an NSE policy process presented in this chapter takes as one of its postulates that any policy implemented in a sufficiently complex setting, especially one that is adversarial, is in truth a policy experiment. If so, let the policy deliberation process reflect this reality in its fundamental framing. Doing so on a formal basis will be a great challenge. Yet, there is a profound asset buried within today's NSE that bodes well for eventual success: Many of the dedicated, talented, and creative professionals within that establishment make informal attempts as individuals to perform this reframing as best they can. It remains only to develop the structures that would permit these inclinations to become better supported. In addition to the tooling that already exists to support deductive reasoning within the NSE, there is also a need to institute a similar apparatus to assist in the inductive reasoning process ("What if . . . ?") that is its natural counterpart; this is a faculty that individuals routinely use but that has fewer means for expression within an agency hierarchy or inter-agency process.
Methods and application experience for each of the steps in the process in Figure 
11
11
Paul K. Davis, RAND Corporation Some aspects of great-power competition are straightforward, but other aspects are more shadowy. 
1
Russia's interventions in Georgia and Ukraine are well known. 4 More generally, Russia has pursued subversion programs to fragment target states and create UGS. 5 This part of Rus-sian strategy has intellectual roots going back a century. 
6
7
8
9
10
11
Today, hybrid and political warfare dominate the Russian battlefields with other major powers. Interestingly, Russia claims that it has been the victim rather than instigator-a victim, for example, of the color revolutions, of Western efforts to entice Ukraine and other former Soviet states into the North Atlantic Treaty Organization (NATO) and the European Union, and of efforts to topple regimes it does not like. 13 In any case, Russia has embraced the warfare methods with enthusiasm and has produced structures that suggest systematic study. One has been touted as the Gerasimov Doctrine with new rules of war, 14 although a more realistic view is that Russian doctrine has merely evolved modestly from previous doctrine. 15  The Russian definition of hybrid war seems to be as follows: 16   Hybrid war: a strategic-level effort to shape the governance and geostrategic orientation of a target state in which all actions, up to and including the use of conventional forces in regional conflicts, are subordinate to an information campaign.
Other definitions appear in more-extensive reports on hybrid warfare and operations in the gray zone by the Center for Strategic & International Studies and the RAND Corporation. 17
China's views of hybrid warfare are as broad as Russia's but reflect its own history and geographic realities. Ross Babbage has written extensively on Chinese thinking and actions, 18  noting origins as far back as Sun Tzu in 500 BC. Mao Tse-tung's thinking reflected his study of commanders ranging from George Washington in the American colonies to T.E. Lawrence in the Middle East. And, of course, Mao had ample experiences of his own. Mao's thinking preceded cyberwarfare as it is seen now, but he would have embraced it.
As discussed in Babbage's review, China has pursued its strategic ambitions in a long sequence of hybrid warfare operations, which have adhered well to Mao's principles. These involved annexation of 
Tibet (1950
Tibet ( -1951))
(1950
( -1980
( ), war with Vietnam (1977
( -1987))
Little is new about hybrid and political warfare, and definitions vary. Christopher Paul notes commonality across concepts: 
19
Second, adversaries can pursue these competitions in a gradual or incremental way, creeping or nibbling their way to success, and they can be conducted in a delayed or difficult to attribute manner or seek to remain below thresholds for escalation, creating challenges and dilemmas for the other competitor.
Paul goes on to note what is arguably newer and ominous. In modern times, the United States has been competing with adversaries who are practicing hybrid warfare gradually while the United States has seemed unaware that it is in a war. Babbage makes a similar point: China has been engaged in hybrid and political warfare for years, whereas Western decisionmakers still see themselves in a state of peace. 20   
Against this background, this chapter takes first steps toward sketching an analytic architecture to aid development and execution of adaptive strategies for dealing with great-power conflict and competition over UGS, interpreted broadly to refer to hybrid and political warfare and political-economic competition. Achieving an appropriate analytic infrastructure would require a substantial effort because the subject-area knowledge is fragmented, multiple government agencies are involved, and some of the technical-analytic needs pose frontier challenges.
Given this, the remainder of this chapter suggests an initial analytic architecture to inform U.S. planning for competition over UGS, particularly in the form of hybrid and political warfare. An analytic architecture must deal with conflicting objectives, differing policy-level perspectives, and changing contextual realities. Any notion of optimal strategy is folly, but much can be done nonetheless. 21 Unfortunately, much of what is needed for a sound analytic contribution lies at the frontiers. The shortcomings start with the science: Hybrid and political warfare have been well studied, but much of the social science is about statistical correlations found in historical data or quarrels about one or another overly simplistic theory. It should be about an integrated causal theory to inform decisions for the future. Doing better is a grand challenge for social science. 22  Therefore, I present some features of an analytic architecture that should help the United States with planning. These features seem valuable and plausible but exceedingly ambitious. They deal with the following:
• strategic planning for adaptiveness • system thinking • portfolio analysis methods for conceiving strategies that balance a variety of activities • qualitative and semi-qualitative methods for integrating fragmented knowledge • gaming, game-structured simulation, and analysis for discovery, exploration, and insight • special analytical challenges (e.g., multi-resolution modeling and improved theory-data connections).
There is a logic to this list. Strategic planning should emphasize adaptiveness; it should conceive issues in system frameworks (multiple countries, time spans, dimensions of competition, domains of action); and it should see strategies as portfolios of diverse actions with diverse objectives. Planners should draw on integrated knowledge, much of which will be qualitative and changing. Enhancing that knowledge could be improved with human gaming, game-structured simulation, and exploratory analysis. Although myriad issues exist, some particular challenges merit special attention, notably multi-resolution modeling to connect 21 See also Chapters Ten and Eleven (Edward Geist, "Why Reasoning Under Uncertainty Is Hard for Both Machines and People-and an Approach to Address the Problem," in Aaron B. Frank and Elizabeth M. Bartels, eds., Adaptive Engagement for Undergoverned Spaces: Concepts, Challenges, and Prospects for New Approaches, Santa Monica, Calif.: RAND Corporation, RR-A1275-1, 2022; and Steven W. Popper, "Designing a Robust Decision-Based National Security Policy Process: Strategic Choices for Uncertain Times," in Aaron B. Frank and Elizabeth M. Bartels, eds., Adaptive Engagement for Undergoverned Spaces: Concepts, Challenges, and Prospects for New Approaches, Santa Monica, Calif.: RAND Corporation, RR-A1275-1, 2022). 22 Paul K. Davis and Angela O'Mahony, "Improving Social-Behavioral Modeling," in Paul K. Davis, Angela O'Mahony, and Jonathan Pfautz, eds., Social-Behavioral Modeling for Complex Systems, Hoboken, N.J.: John 
Wiley & Sons, 2019, pp. 20-24.
Ideal strategic planning takes a long view and a system perspective that addresses multiple objectives, recognizing that some are in tension and that some will change. It may address matters on short-, medium-, and long-term timescales. The options considered are composites of multiple building-block options that address one or more of the many challenges. Although strategic options may be characterized by catchy phrases suggesting one or another focus, all respectable options must address all objectives to a greater or lesser extent. 
23
It is perhaps a cliché that planning should be adaptive, but it is less clear how to make that happen. An earlier paper on the subject influenced planning activities in the Office of the Secretary and Office of the Joint Staff. 
24
12
To pursue such a strategy for adaptiveness, one needs to study the system and its development, monitor the apparent effectiveness of actions, and adjust as necessary. This involves rethinking operational-and strategic-level objectives and strategies as more is learned and the system changes. Even if tactical-and operational-level actions are successful, they may not bring strategic success, as illustrated by the 20-year U.S. experience in Afghanistan. Perhaps this is from not understanding the system or anticipating the side effects of actions; perhaps the system has changed; or perhaps strategic thinking and objectives have changed. In any case, the portfolio of instruments must be adjusted on different timescales (Figure 
12
Addressing this issue would probably require changes in organizational structure and doctrine, education of senior leaders, information systems, incentive structures, and analytic architecture. Such matters are far beyond the scope of this chapter but are quite important. It is relevant that some large corporations have relevant mechanisms as part of succession planning. Also, both civilian government agencies and military organizations are familiar with examples in which new commanders consciously build on their predecessor's work rather than overfocusing on how they can change everything. 
Some authors describe adaptation challenges in terms of metaphors, such as John Boyd's Observe, Orient, Decide, Act (OODA) loop
25
12
12
26
Core strategy
Operational strategy
Tactical effectiveness?
Operational effectiveness?
Years?
Days or months?
Hours or days?
Can cross-level feedback, learning, and adaptation be integrated and speeded?
other settings, the approach suggests the need for on-the-ground personnel to understand firsthand local narratives and how they are affected by events and information operations. 
27
System thinking is crucial in strategic planning, as discussed in numerous books and papers. 30  Roughly speaking, it refers to framing problems in a way that recognizes all the contributors and processes contributing significantly to what is being addressed, such as critical components of machines, processes, and organizations; important interactions with the external environment; and different aspects of what the system does. System thinking contrasts with focusing on only one part of the system because it is the easiest for the organization to address, because it exhibits the most-evident distress signals, or because it is being stressed in headlines. It is also in contrast with the common approach of relegating many crucial matters to the set of exogenous factors.
Understanding complex systems is notoriously difficult. Diagrams are powerful ways to make sense of them. These have a variety of forms and names, but the term influence diagram conveys the sense of a diagram that shows how elements of the system relate to each other. 31  Figure 
12
12
As mentioned earlier, portfolio analysis is a good mechanism for framing strategic planning in a way that lends itself to periodic review and adaptation. 34 Such analysis does not optimize for some notion about the future; rather, it seeks to find strategy that strikes a sound balance across considerations given the tensions, uncertainties, and disagreements among decisionmakers. Some aspects of balance can be informed by historical experience or other empirically based information, but a sound balance also depends on judgments that are difficult to systematize because they involve so many considerations and value issues. No stable utility function exists.
A layered (multi-resolution) portfolio strategy can reflect actual strategy rather than just some one-line bumper sticker or some particularly visible single activity. That is, the initial activities of strategy use a particular mix of military, economic, and political instruments 35 in pursuit of multiple short-term operational objectives consistent as a whole with 33 Elisabeth Bumiller, "We Have Met the Enemy and He Is Power Point," New York Times, April 27, 2010. The article's depiction of a particular system dynamics diagram is humorous, but its hairball diagram reflected serious work to interpret counterinsurgency doctrine. It was useful to those involved (for more on this diagram, see Brett Pierson, Walter Barge, and Conrad Crane, "The Hairball That Stabilized Iraq: Modeling Fm 3-24," in A. Woodcock, M. Baranick, and A. Sciaretta, eds., The Human Social Cultural Behavior Modeling Workshop, Washington, D.C.: National Defense University, 2010) but was not suitable for broad communication. 34 Davis, Gompert, and Kugler, 1996. The approach was inspired by financial portfolio analysis, as in Harry M. Markowitz, William F. Sharpe, and Merton H. Miller, The Founders of Modern Finance: Their Prize-Winning Concepts and 1990 Nobel Lectures, Charlottesville, Va.: Research Foundation of the Institute of Chartered Financial Analysts, 1991. That provided language, constructs, and metaphors. However, national security portfolio analysis must be very different from, for example, a pension fund's investments in stocks, bonds, and real estate. Historical empirical data are not a good basis for national security planning. For more evolved versions of the approach (and earlier references), see Paul K. Davis, Analysis to Inform Defense Planning Despite Austerity, Santa Monica, Calif.: RAND Corporation, RR-482-OSD, 2014. 35 This set should include what others refer to as DIME (diplomatic, informational, military, and economic), DIME-FIL (diplomatic, informational, military, economic, financial, intelligence, and law enforcement), long-term strategic objectives. Again, the approach is intended from the outset to support adaptive strategy. 
36
12
The first requirement is to help decisionmakers orient themselves by seeing the multiple strategic objectives and how options address each of them. This means, for example, comparing options in a multi-criteria scorecard rather than just by some aggregate score. The next requirement is to be able to quickly see the basis for the top-level assessments. That is, the tool should allow drill-down or zooming to see at a glance how the options rate at a next level of detail and how that aggregates to the top-level assessment 
(Figure 12.6)
37
38
12
In this notional analysis, Option 1 is likely to have moderate success in the short term but has no long-term value. It would have very low risk. Because long-term effectiveness is very low (below a threshold of 3), the strategy is rated very poorly overall.
The second strategy is economics oriented and seems better but not impressive. The third focuses on a covert option (e.g., for regime change) and has the potential for great success but is regarded as quite risky. The last strategy is a combination with all instruments, but with a more deniable and less risky covert-action component; it is thought to be adaptive to events. Overall, it is the most attractive. At some future point in time, review of strategy might conclude that the portfolio should put more emphasis on economic sanctions or a different and covert operation. Or it might back away from covert activities because of unexpected negative consequences. Table 
12
Table 
12
12
12
12
The evaluations in this layered portfolio analysis (i.e., the colors of the table's cells) might come from wargaming or simulation of test cases. Or they might be entered subjectively by experts familiar with past studies, wargames, and modeling exercises.
Strategic perspectives. An important aspect of strategic portfolio analysis is highlighting alternative strategic perspectives. Assessment of strategic-level decisions often depends sensitively on controversial judgments and values. These disagreements can often be highlighted by combining them artfully into two or three alternative perspectives (e.g., optimists versus pessimists, short-term versus long-term emphasis, hawks versus doves, or technology-push versus demand-pull). Noting disagreements is often of great interest to policymakers who are distrustful of aggregate scores. Top-level objectives. Portfolio-analysis work should begin by identifying top-level objectives, which are often in tension, contradictory, or too sensitive to mention. 40 For this chapter, some possible top-level objectives to be considered are • establishing and enforcing agreed international norms that severely limit interference in internal affairs • retaining flexibility for covert actions deemed in national interest 40 As an example, all can agree on the benign objective of deterrence, but the United States also has objectives as nurturing friendly governments and weakening unfriendly ones. To a U.S. adversary, those objectives constitute threats. NOTE: Colors and numbers correspond to assessments of very bad (red, 1), bad (orange, 3), marginal (yellow, 5), good (light green, 7) and very good (green, 9). The evaluations account for estimated adaptations.
• defeating and eradicating violent extremism (including the violent Islamic extremism with which the United States has been at war for two decades) • reversing trends toward governments hostile to the West • fostering growth of liberal-democratic governments and reversing trends toward autocracies • assuring U.S. access to critical resources • fostering commerce in ways favorable to the United States • promoting respect for and trust in the United States and its policies.
The tensions among these should be evident.
Different portfolio structures for different purposes. Even strategic planning requires taking diverse perspectives. The paradigm of portfolio analysis with drill-down is widely applicable, but different specializations are needed-for example, for planning on five-year or 20-year horizons, or planning for a mix of operations rather than a mix of experimental probes or planning for a mix of new technologies and tactics. Such matters are illustrated elsewhere. 41   The Objective of Strategic Portfolio Analysis: FARness
The strategic portfolio analysis, as presented here, is intended to assist in finding strategies (options) that address the sometimes inconsistent objectives that are common in strategic planning. As elaborated elsewhere, 42 the intent should be to find a strategy that is flexible, adaptive, and robust-that is, one that exhibits the elements of "FARness." This might seem obvious. Why would one not want such a strategy? In practice, however, this philosophy profoundly affects analysis and decisionmaking. The reality is that the strategy chosen today will affect the ability to deal with events later, but one cannot reliably predict those events or the circumstances in which they will occur. Furthermore, even if consensus exists on the strategy's objectives and constraints, those factors will often change (e.g., how this occurred in the years after the United States invasion of Iraq). It may also be necessary to respond to unanticipated shocks (either adverse or fortuitous). Therefore, one concern is whether the strategy pursued will be able to deal adequately with all of these issues. Will it be flexible enough to allow changes of strategy, will it provide adaptiveness sufficient to cope with new circumstances, and will it be resistant to or resilient (robust) after adverse shock and able to exploit fortuitous developments?
Planning for FARness can be difficult. Often, the tyranny of the best estimate will manifest itself in numerous ways: "We won't need ___ because that scenario simply won't happen; instead, we need to focus on our top priority and put our resources against that!" 41   42  
See Davis, 2014
(Popper, 2022)
In nonmilitary domains, planning for FARness relates, for example, to governments securing property rights that would allow them to expand the width of rivers far beyond that required to deal with forecast sea levels, or to governments developing contracting relationships that would allow them to buy more vaccines as necessary depending on how the coronavirus disease 2019 ( COVID-19) pandemic develops.
Although most people understand the virtues of buying insurance for automobile accidents, home fires, or health disasters, these same people-and politicians, governments, and corporations-often seek to cut costs that are thought of as probably unnecessary. That is seriously wrongheaded. How much insurance is enough, of course, is an important question. An important role for portfolio analysis is illustrating ways in which modest investments can buy a great deal of insurance.
Where does the knowledge to construct and evaluate options come from? Quantitative analysis is sometimes possible based on empirical data, but an analytic architecture should also make good use of qualitative methods. Fortunately, there is, within social science, increased appreciation for qualitative knowledge. This has come as an antidote to decades of excessive focus on quantitative methods that were once touted, mistakenly, as more objective and scientific. What follows touches lightly on a few types of qualitative modeling that might be valuable in codifying knowledge about great-power competition in hybrid and political warfare, sometimes over UGS.
Factor trees are diagrams that have proven suitable for convergent communication, discussion, and classroom instruction. They are similar, in some respects, to system diagrams, but they characterize the primary factors driving an outcome at a snapshot in time. They suppress dynamics, particularly feedback loops, and relatively weak interactions. They provide a broad, top-down summary view. First introduced in counterterrorism research, they have been used in a variety of applications. 43  The factor tree is a layered (multi-resolution) depiction. At the top level of Figure 
12
44
45
46
The factors of a factor tree are intended to be comprehensive, with the tree integrating previously fragmented knowledge and theory. For example, when the report generating Figure 
12
In some cases, it can be useful to map a qualitative factor tree into a computational model. This enables broad exploratory analysis to understand what alternative combinations of factors would produce good or bad outcomes. The result is still qualitative in the sense of being approximate, rough, and imprecisely defined, but it can be used for systematic exploratory analysis. Such a mapping is nontrivial, because all the factors must be defined and combin-ods seem potentially well suited to the challenge of planning for adaptiveness in complex adaptive systems (CAS).
Earlier applications of strategic portfolio methods to U.S. Department of Defense (DoD) planning focused primarily on military capabilities. For those purposes, it was appropriate to use suitably chosen warfare-scenario sets as test cases when evaluating strategies. However, for adjusting portfolio strategies that address hybrid warfare threats, new analytical methods will be necessary. Some should be akin to what corporations use when adjusting their portfolios. If the past portfolio provided a mix of business-as-usual investment and more experimental investments (e.g., introducing a new product, entering a new market, or research and development to generate new products), then rebalancing the portfolio should involve killing off investments that are not proving fruitful or promising and putting more money into efforts that are doing so. 54 For investment in hybrid and political warfare, the issues might be as follows: 55   • What is the likely and possible payoff for the United States in engaging in a particular long-term competition's hybrid and political warfare? What are the risks? • Which elements of previous strategy are succeeding, not succeeding, or worse? Should some elements be deleted? Should some be better supported and, if so, how? Why would such additional support for existing activities be expected to pay off? • Are past failures from inadequate coordination across government agencies? If so, what might improve the situation (e.g., more military training of an ally and more economic aid to an ally showing notable economic progress limited by capital? What are the likely and possible payoffs, risks, and costs? Such questions are merely sensible and normal. However, the analytic means for answering them are not as well developed, much less systematic; nor is there a norm of routine review followed by significant adjustments.  although allowing for and appreciating differences in style and culture across the communities of gamers, modelers, and analysts.
Game-structured simulation is when the primary decisionmaking is made by model agents that mirror the human teams of a wargame.
The most historically ambitious effort in game-structured simulation started in 1981 after a decision by the Secretary of Defense and a formal competition of concepts. 60 The origin of the effort was concern about an analysis of the strategic nuclear balance with the Soviet Union, but the ideas about analysis carried over. The winning proposal in the competition suggested that automated wargaming could employ AI agents interchangeably with human teams. 61 After a thinking period before actual development, a substantially modified concept was laid out. 62 It was then implemented as the RAND Strategy Assessment System (RSAS), the architecture for which is shown in Figure 
12
The challenges then and now were very different. The military aspects of the RSAS featured a large simulation of kinetic warfare (the Joint Integrated Contingency Model, which is still used today), whereas this chapter relates more to war in the shadows and cyberspace. The higher-level decisions by the Red and Blue agents of the RSAS were about whether to engage in large-scale conventional warfare, limited nuclear warfare, or general nuclear warfare. The game was largely two-sided, between the Soviet Union-Warsaw Pact and the United States-NATO. The RSAS also used a Green agent, an assemblage of simple models representing the many other countries in play, sometimes with important roles (e.g., Soviet teams or models might be deterred by French nuclear weapons; basing permissions were required and might be delayed). Nonetheless, a richer depiction would be necessary for the work of interest in this chapter.
Notably, the RSAS had multi-resolution features, which enhanced comprehensibility and explainability; it also enabled theretofore impossible uncertainty analysis in many dimensions. As examples of multi-resolution features, the RSAS could use a scripted model for strategic mobility or a more-sophisticated model accounting for details of ship movements, port capacities, and the like. A combat model might reflect logistics with a simple days-of-supply and rate-of-resupply method or something more elaborate. Political models could make decisions based on simple situational assessments or more-sophisticated "look ahead" calculations (running the simulation within itself to estimate the consequences of one or another strategy).
The RSAS could represent actions and adaptations on different timescales: tens of minutes for nuclear missile exchanges; hours for commander decisions about where to send fresh troops; days for theater commanders to make decisions about changes of strategy (often with back-and-forth communication with national leaders); and minutes to days or weeks for national leaders deciding on major changes of strategy, including escalation or de-escalation. There was no attempt to simulate, even speculatively, events over a period of months. 63  The concept of using game-structured simulation with AI models and teams being interchangeable is apt for the challenge of this chapter. Some admonitions are appropriate, however:
• The AI models needed should be cognitive models-models based on the kind of factors and reasoning attempted by high-level decisionmakers (e.g., about adversary intentions 63 DoD was interested in studying protracted nuclear war because the Soviet Union prepared for it, but we failed to find a credible path for doing so with the RSAS. Human gaming would have been more suitable.
and the concrete political, military, and economic risks of various options), not datadriven machine-learning (ML) algorithms. 64 They should be able to reason. 65  • Such models should be more thoughtfully cognitive than typical Agent-Based Modeling (ABM). They should behave in ways informed by history, human gaming, and political science. Alternative behaviors would be important to consider. Behaviors might be unscrupulous and low-minded actions in single-minded pursuit of narrow national gain or-to the contrary-might be high-minded, as in honoring international rules of the road and reflecting defensive strategy rather than aggression. This would require alternative models along with variable parameters. 66 • The content of the AI models should be guided by in-depth research rather than by aggregate quantitative political science or superficial elicitations of expert opinion.
Although the 1980s RSAS incorporated a good deal of complexity and its simulations often generated surprising events (i.e., events that might not have been anticipated before the simulation), the RSAS predated many insights that we now associate with the theory of CAS. Furthermore, model-building technology was far more primitive. Moreover, the RSAS was not stochastic, and the rules driving agent behavior were motivated more by top-down theoretical considerations than by bottom-up mechanisms that often produce emergent behaviors in CAS.
A modern version of the RSAS would be better able to generate complex developments, such as realistic emergent behaviors, and to do so with n-party game-structured simulation.
It is doubtful that such work would be usefully predictive: The behavior of CAS is famously sensitive to initial conditions and random events along the way. Or to be more careful, CAS are sometimes sensitive to such things, depending on where the CAS is in its state space. A frontier issue in social-behavioral modeling is how to recognize when a real-world CAS is in a portion of its state space that permits reasonably predictable and controllable interventions or whether its state is such that interventions will have highly unpredictable consequences, including some that are seriously counterproductive. 64 For a short survey of ML and AI for social-behavioral modeling, see Osonde Osoba and Paul K. Davis, "An Artificial Intelligence/Machine Learning Perspective on Social Simulation: New Data and New Challenges," in Paul K. Davis, Angela O'Mahony, and Jonathan Pfautz, eds., Social and Behavioral Modeling for Complex Systems, Hoboken, N.J.: John Wiley & Sons, 2019. 65 ML and AI can find algorithms that reproduce complex behaviors exhibited in past data, but that is different from AI that can reason about future possibilities. As Judea Pearl has discussed, AI is still not good at cause-effect relations, but it will be. See Kevin Hartnett, "How a Pioneer of Machine Learning Became One of Its Sharpest Critics," The Atlantic, 
May 19, 2018;
and Judea Pearl and Dana Mackenzie,
In recent years, considerable research has sought to exploit human gaming through computational methods. Some examples: (1) online games, including massively multiplayer online games, are used in deterrence-related studies; 67 (2) online gaming and game communities are studied as a source of social and cultural data; 68 and (3) new methods of simulation analytics are under development. 69   
Colleagues and I have recently used a very low-tech version of the RSAS-related ideas in a study of how to influence peer competitors in crisis or conflict. 70 One DoD approach had been to construct adversary models using a host of expert inputs, attach some Bayesian updating features, and use the model itself to predict adversary behavior. We instead suggested an approach that combines human exercises with analytical thinking. Participants in an exercise develop very simple alternative models of how the adversary is reasoning (i.e., cognitive models), develop possible strategies for influencing that reasoning under uncertainty, and then adjust the strategies to be more adaptive-adaptive enough to be plausibly effective across many adversary models. The intent is to overcome the "tyranny of the best estimate," avoiding errors at both extremes: villainizing the adversary and assuming war is inevitable or, to the contrary, assuming the best about the adversary and becoming vulnerable to surprises. An analogous approach could be taken when addressing challenges of grayarea conflict and political warfare, including those involving UGS.
A recurring theme in this chapter has been the need for multi-resolution analysis. Although it is common to imagine that what is needed is a maximally detailed model that can generate 67   lower-resolution implications by aggregation, the reality is that our knowledge depends on the iterative exchange of information across alternative levels of detail and through alternative lenses, as suggested schematically in Figure 
12
The simplicity of many diagrams (e.g., Figure 
12
12
12
Such issues arise routinely when working across levels of resolution. As a matter of theory, the correct way to aggregate and disaggregate will depend not only on context, but 71 Paul K. Davis and James H. Bigelow, Experiments in Multiresolution Modeling (MRM), Santa Monica, Calif.: RAND Corporation, MR-1004-DARPA, 1998. 72   The Two-Way Flow of Information Across Levels
at each level • Each level informs both higher and lower levels
High resolution on how the estimates will be used. It should be possible to lay logical and pragmatic foundations on how to deal with such matters and to build software methods and tools to assist in doing so, but for a variety of intellectual and organizational reasons that challenge has not yet been undertaken. One explanation of this failure has to do with camps within the analytical world. For example, different camps exist for system dynamics method and for ABM. Connections between the two are few. The reasons are in part historical. ABM as we usually think about it in 2020 has largely evolved from developments in the 1980s and 1990s associated with the Sante Fe Institute. 73 Such work is bottom-up in character, with a paradigm of investing individual-level agents with simple rule sets that generate emergent phenomena similar to important phenomena observed in the real world. A substantial literature illustrates how such ABM can relate to topics as diverse as racial segregation and collapse of societies. A textbook for the NetLogo language has many examples. 74  System dynamics work has a more top-down character and represents dynamics in terms of macroscopic stocks, flows, and interactions, including feedback loops. The preeminent textbook has many examples, 75 and the System Dynamics literature (i.e., the particular modeling methodology developed at the Massachusetts Institute of Technology 
[MIT]
Simon & Schuster, 1992;
John H. Holland, Hidden Order: How Adaptation Builds Complexity, Heather Mimnaugh, ed., New York: Perseus Publishing, 1996;
John H. Holland, Emergence: From
Science, Vol. 54, No. 5, 2008.
From the viewpoint of policy analysis, the reasoning that underlies policy choices will typically be macroscopic, but it needs to be consistent with microscopic realities. How can that be accomplished if there is no crosswalk? As an example, suppose that an intervention is contemplated to assist a target government. It might focus at a high level on improving the rate of growth of the gross domestic product or the quality of governance. However, the strategies adopted might fail because the underlying culture rejects the intervention actions in that they disrupt transactional arrangements among factions that are important in making things work.
From a conceptual perspective, what is needed is (1) use of microscopic game-structured system simulations, which will almost necessarily involve agent-based components that generate the emergent phenomena of interest, ( 
2
3
As noted in a recent study for the Defense Advanced Research Projects Agency and a subsequent book reviewing social-behavioral modeling, 80 another frontier challenge is improving the degree to which empirical analysis is informed by good theories and theories are suggested, tested, and improved by data. This is difficult because the empirical data often do not connect well with the parameters of models representing theory. Furthermore, social scientists tend to use data to test hypotheses rather than testing coherent, integrative theories. The study suggested that large research programs consider creating virtual social-behavioral modeling laboratories (SBMLs) to ensure a combination of collaboration and competition among researchers. Figure 
12
Many advances are being made on better relating theory and data in the modern era of interconnectedness and computation; such advances involve improving inferences about causality, 81 using both theory experiments to study social polarization and influence, 82 and using ABMs and virtual games in connection with laboratory experiments. 83   81 Amy Sliva, Scott Neal Really, David Blumstein, and Glenn Peirce, "Combining Data-Driven and Theory-Driven Models for Causality Analysis in Sociocultural Systems," in Paul K. Davis, Angela O'Mahony, and Jonathan Pfautz, eds., Social-Behavioral Modeling for Complex Systems, Hoboken, N.J.: John Wiley & Sons, 2019. 82 Michael Gabbay, "Integrating Experimental and Computational Approaches to Social Influence," in Paul K. Davis, Angela O'Mahony, and Jonathan Pfautz, eds., Social-Behavioral Modeling for Complex Systems, Hoboken, N.J.: John Wiley & Sons, 2019. 83 Lynn Carol Miller, Liyuan Wang, David C. Jeong, and Traci K. Gillig 
In this chapter, I have discussed some methods that could be brought to bear in improving DoD's ability to understand and develop adaptive strategies for great-power competition involving gray areas and political warfare, including activities involving UGS. My intent has been to suggest some steps toward an analytic architecture. No such architecture exists. I conclude that a new analytic architecture is needed to aid strategic planning for competing with great powers in UGS-which is broadly construed to address hybrid and political warfare and political-economic competition. Such planning must deal with developments in CAS, so the architecture must be conceived accordingly-a radical departure from the past. Analytical tools should help characterize the nature of the system's state and the feasibility (given that state) of influencing developments while controlling risk and evaluate the relative merits of alternative composite strategies for doing so while accounting for the behaviors of adversaries. The strategies should be reflected as portfolios of overt and covert political, military, and economic actions in different domains, levels of detail, and timescales. Some actions will prove successful; some actions will be ineffectual; and some actions, counterproductive. Thus, the architecture should anticipate timely but coherent adaptiveness. Adaptations may involve modest adjustments, significant rebalancing of the portfolio, or major changes with revised objectives. One role of analysis will be to aid in planning for FARness-that is, finding strategies that can be flexible, adaptive, and robust in allowing for, respectively, changes of objective and mission, unexpected circumstances, and either adverse or opportunitycreating shocks. This contrasts with planning on the basis of best-estimate assumptions alone. Another role will be aiding actual strategic adaptations along the way.
Constructing such an architecture, populating it with sound but useful methods and tools, and educating users would require a major effort. It should be heavily influenced by the best subject-area minds and would involve competition of paradigms, methods, and styles. That would not be a project for computer scientists merely seeking a new application for their favored methods. The challenges of competition and collaboration in undergoverned spaces (UGS) raise many issues and opportunities for multi-stakeholder research. These run the gamut from understanding the mental models and relationships among multiple stakeholders to adaptive governance processes that involve stakeholders in the decision processes themselves. Recent years have seen expanding interest, understanding, and experience in engaging stakeholders as a key target for collaboration and decisionmaking, across all aspects of complex problem management, including research. Many opportunities exist for research that can improve the capabilities for engaging multiple stakeholders in UGS. This chapter provides an overview of how multiple stakeholders can be involved in the research process and also considers how these different modalities relate to one another. We focus on approaches that center stakeholders as either the focus of research or as the coproducers of research and on how each can add value for policymakers. Second, we examine each of these modalities, in turn, identifying the general principles of practice, the different approaches and tools in conducting research in these modalities, and the challenges of doing so. We next examine future investments that could catalyze improvements in multistakeholder research and make the case for how these investments could drive improved multi-stakeholder governance.
Multi-stakeholder research is any research process that involves a diverse group of people, businesses, governments, or other entities that have some interest in the process or outcome of the research. In some contexts, this may be an output of a research group representing different interests, while, in other cases, it might represent an engagement with stakehold-ers who are the subjects of research and collaborators in its production. Different stakeholders might have different questions they want answered by the research; the questions might evolve over time, and stakeholders might have different incentives to share results or engage other parties. Questions, methods, and partnerships will evolve over time as research needs shift in response to changes in the operational and strategic landscape.
Multi-stakeholder research can take many forms, varying in how stakeholders and researchers interact and in the goals of the research, as shown in Table 
13
Research focused on stakeholders can help the United States better understand and provide communications that are responsive to existing stakeholder frames. Stakeholders can also participate in research designed to help the U.S. government and other actors communicate with one another, come to common understandings, and jointly manage UGS. In these approaches, research questions and methods may be driven by government agencies or their proxies in a top-down approach to research.
Stakeholders as participants can coproduce knowledge that facilitates common understandings and engagement, and they can provide local or specialized knowledge accessible only through some stakeholders. To facilitate multi-actor and adaptive governance, stakeholders can also participate in research facilitating deliberation designed to develop new mental models and understanding of problems. Adaptive governance refers to flexible and learning-based collaborations and decisionmaking processes involving both state and nonstate actors, often at multiple levels, with the aim of adaptively negotiating and coordinating the management of complex systems. Research with stakeholders as participants addresses questions that are driven from the bottom up by stakeholder needs and interests or through a twoway dialogue between the subjects of research and the relevant stakeholders.
We discuss both of these: stakeholders as the focus of research and stakeholders as participants in research.
To enhance understanding (see Table 
13
To enhance communication, multi-stakeholder research can focus on how various groups can be encouraged to understand and change behavior on the basis of specific research outputs and/or other information. The audiences for such communications might include both the stakeholders who were the focus of research and other audiences of interest to policymakers sponsoring and consuming the research. Sponsoring agencies are often, but not always, the consumers of research. In the cases in which they are not, communications might need to consider both sponsor and consumer needs and different communications targeted to each when possible. Research on effective communication methods encompasses too broad a space to cover in depth within this chapter, but interested researchers can refer to the citations in the footnotes on where to find more information on effective communication with stakeholders
1
Coproduction of knowledge (see Table 
13
4
For instance, the Arctic Council (AC)'s Alaska Native Tribal Health Consortium established the local environmental observer (LEO) network in 2009 to collect concerns about environmental change and pollution in the region. Stakeholders-in this case, subnational private actors residing in the Arctic-are deeply involved in the data collection process of the LEO network, helping to coproduce knowledge about environmental change and arctic contaminants. Such coproduction does not preclude research on stakeholders; rather it widens the aperture to involve more participants in the research, with the goal of increasing the relevance and usability of the resulting information.
A deliberative process (see Table 
13
5
The following two sections look at the two modalities in terms of their core principles, their tools and approaches, and the challenges involved in using the tools and approachesspecifically in terms of long-term competition and UGS.
We first consider approaches to multi-stakeholder research that treat stakeholders as a focus of the research process. This involves both efforts to understand what stakeholders are think-ing, feeling, and experiencing and to develop communication approaches that are effective in reaching target populations of interest. The benefits of working to understand these worldviews can be substantial. This process offers researchers and policymakers more insight into the beliefs, motivations, biases, and values of relevant populations. This insight can identify areas of cooperation, complementarity, or competition among different groups; potential areas for misunderstanding and misinterpretation between groups; and solutions to policy problems that might otherwise be overlooked. Furthermore, research focusing on stakeholders allows researchers to determine how to subsequently engage stakeholders in a way that fits with those stakeholders' worldviews, generating improved understandings of problems and enabling the development of communications that are effective at reaching populations of interest.
An example in which research on the motivations and capabilities of stakeholders has been productively employed is a recent study of the benefits and risks of private-sector attribution of cyberattacks. 6 In this study, the authors clearly lay out the differing motivations of private-sector actors to attribute cyberattacks to nation-states and discuss the benefits and risks of doing so to U.S. government interests. Ultimately, the study argues that the complementary interests and capabilities of the U.S. government and the private sector provide significant opportunities to collaborate in attributing cyberattacks as a form of deterrence in an undergoverned competitive space. Work like this makes clear that multi-stakeholder-focused research on security problems can help identify opportunities for increased collaboration and communication.
Prior to developing communication messaging, or opportunities for collaboration, multistakeholder-focused research efforts must first identify the underlying beliefs, values, and perceptions of the populations of interest (referred to as either worldviews or mental models). To determine a worldview, researchers should use several basic principles to (1) understand cognitive biases that might be affecting mental models; (2) deploy methods to obtain those mental models given the cognitive heuristics; and (3) account for special needs for stakeholder engagement in UGS. These steps are iterative, and researchers might find that they need to move between steps 1 and 2 several times to adequately specify a mental model.
Many ways exist to understand a stakeholder's mental models and associated cognitive biases (e.g., work conducted between the 1960s and 1980s by such authors as Edwards, Einhorn, and Hogarth). 7 Hundreds of biases have been identified, including in online settings. 8 Three biases are worth briefly mentioning because they may particularly affect the 6 Sasha Romanosky and Benjamin Boudreaux, "Private-Sector Attribution of Cyber Incidents: Benefits and Risks to the U.S. Government," International Journal of Intelligence and Counterintelligence, Vol. 34,  No. 3, 2021.   7 Paul Slovic, Baruch Fischhoff, and Sarah Lichtenstein, "Behavioral Decision Theory," Annual Review of  Psychology, Vol. 28, No. 1, 1977; and Hillel J. Einhorn and Robin M. Hogarth, "Behavioral Decision Theory:  Processes of Judgement and Choice," Annual Review of Psychology, Vol. 32, No. 1, 1981.   8 Daniel Kahneman, Paul Slovic, and Amos Tversky, eds., Judgment Under Uncertainty: Heuristics and Biases, Cambridge, United Kingdom: Cambridge University Press, 1982; Steven J. 
Sherman and Eric Corty, "Cognitive Heuristics," in R. S. Wyer, Jr., and T. K. Srull, eds., Handbook of Social Cognition, Vol. 1, Mahwah, N.J.: Lawrence Erlbaum Associates, 1984;
Slovic, Fischhoff, and Lichtenstein, 1977
Anchoring and adjustment biases occur when a person uses a piece of information to help inform later answers. This cognitive bias may especially come into play in an undergoverned space because information may not be as readily available and thus information from a trusted source may be relied on even more heavily than normal. Researchers may need to be particularly aware of this bias when structuring their communications.
Regarding affect, it is important to understand stakeholders' moods, which would alter how they respond to the research questions and processes. Different stakeholders might respond to events in significantly different ways according to a variety of varying contextual and indi-vidual factors, including their emotional affect. Particularly in the context of UGS, affect has the potential to alter a respondent's perceptions about risk. For example, people who are more fearful (or less angry) exhibit increased risk estimates of terrorism. In the context of longterm competition, fading affect bias may result in memories of negative events remaining salient to stakeholders longer than equally strong positive events and continuing to influence decisions over a longer time.
Confirmation bias, in which people are more likely to accept information that aligns with their worldviews, can lead to overconfidence, such as when Turkey's leadership was overconfident in its Syria policies because the policies agreed with what it thought was true and did not fully take into account how other actors would actually behave. 
12
Once worldviews are fully understood, research teams can work to design communications that meet the needs of key populations. For interested readers, there is a substantial amount of existing literature on effective science and policy communication tools. The body of work in this space emphasizes the challenges of communicating research effectively in a crowded, contested media environment, the need to use communication tools to build trust between parties, and the need for research on how to communicate research to bridge the divide between science and policy. For example, the Intelligence Community (IC) and its initial briefings to presidential candidates not only help the presidential candidates digest the material but also gather information about the candidates to better inform future engagements between the parties. This effort affords the IC the opportunity to determine what kinds of communications are most likely to be effective. For this chapter, we do not survey the literature in depth-because good summaries are available elsewhere (see earlier discussion about citations); instead, we briefly describe some existing tools and note that research into effective multi-stakeholder communication is crucial for developing trust and partnerships between researchers, policymakers, research subjects, and other relevant parties and should not be neglected relative to other areas of multi-stakeholder research discussed later.
Understanding how cognitive heuristics and biases influence worldviews and decisionmaking requires targeted research efforts to identify and characterize these belief systems. Typically, methods seek to identify either stated or revealed preferences. Here we describe two general methods of identifying these preferences: asking people (via interviews, surveys, and focus groups) and observing people. Table 
13
Interviews are a commonly used method to gather multi-stakeholder input.
Interviews can be open-ended or semistructured, seeking to gather oral histories or specific metrics. Interviews provide the ability to question stakeholders, with a time length that can be tailored in near real time. As a result of their flexibility, interviews have been used to gather data in an undergoverned space, such as interviews following a plane crash in the Ukraine. 13 Dyadic interviews (where two stakeholders are interviewed together) 14 have the additional benefit of revealing interactions between people. For example, in a study in Syria, researchers asked parents about health care for Syrian children and explicitly focused on interactions and points of disagreement. 15   13 C. Joris Yzermans, "The Experience of Sudden Loss of a Colleague or Neighbor Following the MH17 Plane Crash in the Ukraine: A Qualitative Interview Study," BMC Psychology, Vol. 8, No. 1, 2020. 14 David L. Morgan, Jutta Ataie, Paula Carder, and Kim Hoffman, "Introducing Dyadic Interviews as a Method for Collecting Qualitative Data," Qualitative Health Research, Vol. 23, No. 9, 2013.   15 Riham M. Alwan, "Beliefs, Perceptions, and Behaviors Impacting Healthcare Utilization of Syrian Refugee Children," PloS One, Vol. 
15, No. 8, 2020
Surveys often come after a set of interviews to flesh out the initial mental model. Survey methodology is well studied and can be tailored to nearly any kind of research question. 16  For example, surveys can answer questions about open-ended thoughts, discrete choices, or public perceptions. 17 In an undergoverned space, surveys were used to, for example, determine multi-stakeholder input on the health and environmental situation in Aleppo households in war-torn Syria. 18 Note, questions must sometimes be carefully constructed to decrease unnecessary risk to the participant. For example, wartime surveys in Afghanistan might be structured to compare groups of people rather than individual responses. 19   Focus Groups
A focus group is a small number of diverse stakeholders brought together to consider a topic with the goal of gaining representation from different groups (demographic and otherwise) that might affect the result. 20 Its primary value is that it can gather a variety of views on a specific topic in a relatively short time and, in conjunction with other methods, can allow researchers to understand how individual views may shift or be expressed differently in group settings. However, on its own, a focus group can make it difficult to extract individual under-standings from the group's discussion, because the social dynamics and moderator behavior can influence how people respond. 21   
Another method to obtain multi-stakeholder input is through direct observation. 22 In certain UGS or remote spaces where one cannot interact with people and there is no advanced technology, direct observation might be one of the only methods to obtain data on stakeholders' mental models. For example, in a study on a tribe in central Amazonia, researchers used direct observation techniques to gather improved understanding on subsistence hunting. 23  Direct observation can be targeted to in-person or online actions and behavior. For example, browsing history, transactional data tracking, meeting notes and attendance records, and interactions on social media would all be considered forms of direct observation. All these data collection efforts help identify stakeholders' actions, preferences, and possibly mental models. These types of methods are quite commonly used in cyberspace. 24 For example, social media data could be used to monitor the needs for emergency first responders as has been recommended for the U.S. Coast Guard. 25 For example, one could use these types of methods to understand longitudinal tweets in Syria, 26 to determine stakeholders' inputs on social movements in Africa, 27 or to understand the behavior of citizens in cyberspace. 28 Of importance is the need to interrogate assumptions about the credibility of sources and information from social media. 
29
For research to be impactful, it must be useful and relevant to stakeholders. There are multiple ways to achieve this goal, and there are significant benefits to be gained by considering multiple stakeholders in the research process. Most significantly, focusing on stakeholders in research provides an opportunity for researchers to develop a deeper understanding of the beliefs driving stakeholder actions and insight into which issues and messages are most important to particular individuals and groups. This understanding can translate into improved messaging and communication with different groups, but it can also help identify opportunities for more engaged coproduction or deliberation processes.
The decision to involve multiple stakeholders in research can also carry costs and risks that researchers should consider in their planning. These could be higher costs to engage with many heterogeneous stakeholders and to reach particularly isolated or difficult-to-access groups. The increased time required to conduct engagements can also be a source of tension in situations in which an adaptive management process needs to quickly respond to changing conditions on the ground. Many of these challenges can additionally translate into larger transportation, food, or lodging expenses. Other challenges to accessing these stakeholders might be low internet availability and usage and slower data collection if using asynchronous online data collection methods. Populations of interest may also be distrustful of researcher intentions, thus requiring engagement over longer periods-or the use of different research partners-to develop trust and rapport that enables good research.
Physical risks to researchers and stakeholders are also a consideration, because it may be dangerous to physically reach some groups. In many settings where there may be some need to smooth tensions in person, it is standard practice to use a trained moderator. 
30
31
In the context of long-term competition, the impacts of these failures can have enduring consequences. To the extent that researchers are (or are believed to be) representatives of the U.S. government, then actions that breach social trust in populations can make it harder to conduct research with the same groups in future. Approaches to ameliorating this risk-such as sampling enough participants to be able to aggregate and anonymize the results without fear that any person will be identified-can increase costs. In settings where people interact, implementing ground rules, such as the Chatham House rules (i.e., participants are welcome to use what they learned but not to attribute it to someone), could be one way to lower this risk. This extends to people after they have left UGS; consider, for example, a study of Afghan refugees in the United States 34 that highlights findings on how to ask the questions while preserving privacy (and reducing other risks, such as emotional risk).
Finally, work in UGS can carry substantial emotional risk to participants. Proper review by groups, such as Institutional Review Boards, will help identify sources of, and reduce risks to, participants (although, in some cases, this may mean that the research is not allowable). This may also lead the researcher to use remote technologies (such as virtual reality [VR] or augmented reality [AR]) or increase cyber protections for the participants. Another concept of importance is working to provide positive experiences and feedback opportunities. It helps to define workshop roles and responsibilities in advance, such as by providing a biography sheet to the participants in advance of the workshop. Obviously, a large part of this aspect is creating a research environment where stakeholders feel welcomed to participate and provide input and that their interactions moderated in a positive, constructive manner. For more on political psychology, see works by Jervis and Larson. 35  While stakeholder-focused research in the context of long-term competition and UGS can carry substantial risks and impose higher costs on the research team, these risks can often be mitigated and may be outweighed by the substantial benefits that can accrue. 32 Klima and Jerolleman, 2017. 33   
Stakeholder research that treats stakeholders as partners in the coproduction of knowledge or in deliberation requires thinking about the nature of the problem being analyzed and designing ways to bring stakeholders into the research process itself. This type of multi-stakeholder research can use or rely on the approaches to stakeholder focus described earlier, but it can also open the door to deeper collaboration in which diverse groups actively participate in the framing and formulation of problems to be studied and decisions about what to do with the resulting knowledge. Although not appropriate in all contexts, this focus of multi-stakeholder research can have significant benefits, particularly in efforts to address complex problems and design adaptive policy. One of the primary benefits of this type of research is that it can help identify previously unexplored problem framings and solutions by allowing stakeholders to discuss and synthesize imperfect, partial understandings of a problem. Moreover, bringing stakeholders into research processes at the question formulation, data collection, or analysis phases could help create opportunities to develop shared language and understandings of problems over the research effort that might help minimize conflict over results at the end of the process.
Several frameworks for dealing with complex problem environments exist. We focus here on one that emphasizes both competitive and collaborative strategies for addressing a complex stakeholder environment. Following the work of Nancy Roberts, we can conceive of the multi-stakeholder landscape for research partnership according to the following three questions, the answers to which shape the space for research strategies: 
36
• To what degree is that power contested by different stakeholders?
The first question is used to assess the degree of dispute over the nature of the problem and potential solutions; the second, to assess where power to address the problem is centered; and the third, to identify the degree to which that authority is contested by other actors, as represented in Figure 
13
Researchers could also consider how stakeholders interact with one another in choosing strategic approaches to research with multiple stakeholders as participants. Such approaches can consider not only relative power imbalances but also the degree to which interests differ and knowledge is fragmented across actors. 38  In more tractable cases, knowledge is complete, interests are aligned, and power imbalances are low, creating conditions for cooperative solutions. In less tractable situations, knowledge is fragmented, interests compete, and power is dispersed and contested. Almost by definition, long-term competition in UGS is likely to be of the latter kind. However, such underlying con- 38 John Alford and Brian W. Head, "Wicked and Less Wicked Problems: A Typology and a Contingency Framework," Policy and 
Society, Vol. 36, No. 3, 2017
These considerations affect coproduction and deliberative processes in distinct ways. Collaborative deliberative processes in highly contested spaces may be much more difficult to coordinate and manage than coproduction processes, largely because deliberative and decisionmaking processes can shape the outcomes of contested questions. The research portfolio of the AC (discussed earlier) was initiated in a period of relatively low geostrategic conflict (the 1990s) and explicitly excludes military concerns. Since that time, the geopolitical landscape has become much more contested, and the structure and the provisions of the council may no longer be well designed to address deliberation needs in such a more hotly contested environment, even though the knowledge coproduction tools may still be valuable to address less-sensitive questions. 40   
Many already existing tools can be used to do research with stakeholders both as the focus of the research and as coproducers in the research process. It is often possible to use these 39 Michael Bloomberg, "City Century: Why Municipalities Are the Key to Fighting Climate Change, " Foreign Affairs, Vol. 94, No. 5, 2015.   40 Oran R. Young, "Is It Time for a Reset in Arctic Governance?" Sustainability (Switzerland), 
Vol. 11, No. 16, 2019.
Table 
13
We have ordered these approaches roughly according to the structure of the research process. At one end of the spectrum, scenario processes provide researchers relatively more control over the process and what insights emerge, while at the other end of the spectrum lie such techniques as deliberative processes, in which stakeholder interests and concerns weigh most heavily in shaping the outcomes of the process. Most of these tools may serve multiple purposes, although some are easier to use than others to inform modalities of multistakeholder work: knowledge, communication, coproduction, and deliberation. In practice, most research with stakeholders as participants will also use stakeholder-as-focus methods, such as interviewing and focus groups. Many of these techniques can also be applied as teaching techniques and analytic tools. It is incumbent on the researchers to be clear about the purposes they intend any multi-stakeholder work to serve, because that should inform the design and content of the procedure chosen. Newer research has begun to provide design principles to aid in structuring these engagements. 
41
Scenario development, exploration, and analysis methods can help engage stakeholders in concepts that they did not know ahead of time (e.g., unknown unknowns). There are many examples of scenario exploration used in UGS, such as a scenarios workshop on future directions for European Union-Chinese relations 42 and efforts to understand the future of Arctic exploration by various stakeholders. 43 They have been widely used in U.S. military planning historically, and recent research has identified ways to improve the process to meet the challenges of an increasingly complex threat environment. 44 Successful use of scenarios requires developing buy-in from stakeholders on the parameters considered; otherwise, stakeholders may "fight the scenario."
Participatory modeling directly engages stakeholders alongside researchers in creating formalized and shared representations of reality instantiated in various modeling formalisms, such as influence diagrams, causal loop diagrams, and Agent-Based Modeling. 45 These representations aim to capture the implicit and explicit knowledge of the stakeholders-in particular, mental models of relevant systems and the dynamics under various conditions. Emerging work in this space has included work to apply fuzzy cognitive mapping in participatory contexts. 46 These representations can also be used as boundary objects to facilitate under-standing and communication among groups and to generate simulation models that can be used in analyses that help to identify and compare the impacts of alternative solutions and their ability to achieve various goals. The process of coproducing models with stakeholders and researchers helps engage both expert and local knowledge and can also enhance the salience, legitimacy, and credibility of the analyses for all involved. Although commonly used in environmental studies, participatory modeling has also been applied to some UGS, such as implementing fiscal policy in Ukraine. 47  
Deliberative methods encompass a wide variety of means to involve multiple stakeholders in structured group discussions. As with other forms of public engagement, deliberation can be used in various types of venues distinguished by the amount of inclusiveness among those invited to participate and the extent to which the participants are empowered to make decisions. Inclusiveness can range from working with a small, select group of carefully chosen stakeholders to being open to everyone. Empowerment can range from the deliberating group only providing information to those making the decision to having full decisionmaking authority. 51 Although there are different models (e.g., Fishkin, 52 Cohen, 53 and Gutmann and Thompson 54 ), deliberation typically proceeds with a specific process that involves both discussion or consensus-building and voting. Here, we describe three illustrative methods. 55   Group Consensus Methods Delphi and other group consensus methods provide a way for research teams to recruit individuals for repeated activities centered on rating and evaluating information. 56 The focus could range from estimating uncertain parameter values to evaluating potential actions. One of the benefits of Delphi approaches is that they can be run and facilitated remotely and anonymously. 57 Although such methods were originally developed to facilitate the assessment of quantitative information, more-recent innovations can be employed to allow researchers to also use it in qualitative contexts. 58  Deliberative Polling and Citizen Juries With this method, a small group of randomly sampled individuals, representative of the demographics from a particular community, comes together for a few hours to a few days to discuss and reach a collective decision or recommendation on some policy questions. These discussions are informed by carefully balanced briefing materials, often skilled facilitation, and sometimes direct testimony and interaction with experts. 59 Citizen juries have been used to make recommendations on constitutional reform, climate policy, and ballot initiatives. 60  Deliberative polling bookends such deliberations with opinion polling to measure the effect of the deliberations on views of representative citizens.
Deliberation with analysis represents an iterative process in which stakeholders deliberate on their objectives, options, and problem framings; researchers (analysts) generate decisionrelevant information; and stakeholders revisit their objectives, options, and problem framings in response to interactions with each other and with researchers' information. 61 The process is intended for situations in which the problem framings and stakeholders' understandings evolve through these interactions. Such frame reflection is often valuable when boundaries between research and policy may be shifting or fluid. 62 Deliberation with analysis often relies on computer simulations, and a typical case would involve the deliberative body deciding assumptions to use and policies to test and then making initial recommendations and requesting analysis of additional policy options after viewing the results of the initial simulation runs.
Expanding beyond thinking of stakeholders as the focus of research to coproducers can enrich adaptive decisionmaking, but doing so also carries costs. First, it creates opportunities for new structures for research and decisionmaking. Previously excluded actors may become involved, thus enriching the understanding of a problem, particularly in cases in which certain types of information may only be accessible or interpretable to some groups or individuals. Coproduction, deliberation, and multi-actor decisionmaking processes can create spaces in which iterative cycles of analysis and decisionmaking can occur, thus helping to develop cycles of adaptation that are essential in responding effectively to complex problems. However, such processes may not be appropriate in all cases because they present challenges and risks while requiring nontrivial adjustments to the traditional process of public management and research for management. As noted by Head and Alford,
The conventional structures and systems of the public sector are not scoped to address the tasks of conceptualizing, mapping, and responding to wicked problems. Project management for tackling wicked problems through long-term targeted interventions would require a substantial and unaccustomed degree of flexibility in the structures and systems of public governance. 63   Some of these challenges are familiar and similar to costs discussed already when thinking about stakeholders as participants: increased time and costs to doing research and increased potential for information to leak to hostile parties or bad actors. Others are more specific to the challenges of making decisions in highly complex spaces. These are discussed in reference to Robert's framework.
In cases in which authority is concentrated and uncontested (large power differential but considered legitimate), Roberts makes the case that an authoritative body can simply decide and act to solve the complex problem. 64 However, determining when legitimate authority exists and is truly uncontested is not a trivial matter, and there is significant potential to miscategorize a problem as meeting conditions for authoritative solutions, when that is not the case. 65 When that occurs, problems may be inappropriately simplified, thus missing opportunities for better solutions. Even though authoritative approaches offer benefits to policymakers (e.g., rapid implementation and organizational simplicity), these benefits can undermine desired outcomes in complex problems. This is because strategies devised using authoritative approaches can ignore key features of complex problems because those strategies are "usually beyond the cognitive capacity of any one mind to diagnose or comprehend." 66  In cases in which authority is dispersed and contested, Roberts advises a competitive approach to problem-solving. While these approaches can spur fast rates of innovation, structuring research activities in this fashion is challenging within the traditional framework of public management and can be costly; it could increase conflict or consume resources (e.g., litigation, defensive investments, deterrence) that could otherwise be devoted to solving 63 Brian W. 
Head and John Alford, "Wicked Problems: Implications for Public Policy and Management," Administration and Society, Vol. 47, No. 6, 2015
Finally, collaborative approaches to coproduction can also be costly. They require adjustments to the traditional understanding of the roles of decisionmaking and analysis to bring researchers and policymakers into dialogue. These adjustments may require using communication and negotiation principles to navigate the boundary between science and policy. 69  Agreement on new principles for guaranteeing the quality and unbiasedness of information may be required to effectively connect knowledge and action. Existing criteria are salience, credibility, and legitimacy, but actors may have different definitions and interpretations of what these components mean in practice, leading to disagreements over the quality and appropriate use of different research outputs. 70  Emerging models in the physical sciences suggest ways to address these challenges. Convergence research draws on insights on the function of scientific teams to identify conditions and processes that can inform how diverse teams can work together most efficiently. The Natural Hazards Engineering Research Infrastructure Facility represents one way to build systems of interdisciplinary and transdisciplinary research that enable collaboration across diverse organizations and disciplines to answer applied science questions. 71 However, even with an explicit commitment to this type of work, deep interdisciplinary research is difficult to conduct in the context of institutional structures, incentives, and training processes. Addressing these difficulties requires restructuring management systems, which will be addressed in more detail by Steven W. Popper in Chapter Eleven. 72 However, alongside these 67 Alford and Head, 2017. 68 Head and Alford, 2015; Alford and Head, 2017.   69 Carina Wyborn, "Co-Productive Governance: A Relational Framework for Adaptive Governance," Global Environmental Change, Vol. 30, January 2015. 70  broad changes to the way that public systems are governed, a more narrow set of tools exists that can be used to develop cooperation and collaboration, reframe thinking about variables, links and options, and design multi-stakeholder research practices tailored to the needs of long-term competition in UGS or alternatively governed spaces. 73 In the next section, we focus on investments that could be used to improve this narrower set of tools to facilitate research in multi-stakeholder settings.
This section discusses some examples of places where investments could catalyze improvements to the practice of multi-stakeholder research in undergoverned and long-term competitive contexts. This is not intended to be a comprehensive survey; rather, it is intended to provide a few examples of potentially productive avenues for research. We begin with several areas in which investments could generate returns across a variety of multi-stakeholder research activities. Then, returning to the original taxonomy proposed in Table 
13
Recent years have witnessed significant advances in the analytic tools, practices, and institutional context for multi-stakeholder engagement that enhance capabilities for understanding, communication, coproduction, and deliberation in complex decisionmaking contexts. Innovations in this space are the result of both changes in technology and advances in social science that both improve the use of these tools and inform the spaces in which they are used. Many of these advances have occurred in the field of environmental management, which is a more governed space and generally perceived as more collaborative than others discussed in this report. Similarities are the long-term nature of the complex problems and the importance of initial conditions and path dependence in shaping the future option space. As a result, many of these advances have relevance to contested, less-governed spaces of national security and defense. Some can be translated relatively directly from collaborative environmental contexts and used to improve adaptive planning and governance, while others would require more fundamental restructuring.
Multi-stakeholder research is particularly valuable for problems involving complexity, uncertainty, and ambiguity. In these cases, multiple values, goals, and understandings of a problem can inhibit effective decisionmaking to address policy problems. 
74
Like decision support, investments in tools to facilitate engagement also hold value for multi-stakeholder research. The past 25 years have seen an explosion of different engagement methods for increasing the salience, relevance, and credibility of decisionmaking processes for addressing complex policy problems. 
75
Successfully applying research in these focal areas to UGS requires three specific kinds of research investments-those aimed at (1) understanding how competition influences the design of tools for decisionmaking, (2) improving structures to facilitate deeper engagement across disciplines and stakeholders and sustaining that engagement over a research program, and (3) employing regular, rigorous evaluation. Some basic features of competitive spaces differentiate them from collaborative research and affect which approaches to multi-stakeholder research may prove most appropriate. It may be hard to identify and engage a research coalition in a highly competitive context. It may be difficult to align the timescale of decisionmaking with the speed of multi-stakeholder research processes. Even something as simple as the questions being asked can reveal privileged information to competitors and place research subjects and strategic aims at risk. This chapter has necessarily focused on collaborative spaces that account for the significant bulk of existing research. Future research might examine how best to deploy these techniques in competitive spaces.
How best to develop and sustain multi-stakeholder research coalitions is an unsolved question in the environmental space and one likely to bedevil national security applications. Institutional missions, performance metrics, siloed information, time pressure, inertia, and many other factors drive many interdisciplinary research coalitions to fracture along institutional or disciplinary lines, thus preventing knowledge synthesis and effective coproduction, which are crucial in complex decision environments. Research that tests new structures and approaches for stakeholder research in competitive contexts, such as ad hoc committees or the type of partnerships used in the convergence research already discussed, 76 could help improve understanding and avoid some of the pitfalls that have frustrated the application of these methods in environmental spaces.
Finally, evaluation is a core part of design for complicated engineered systems. In governing complex problems, decision support tools and engagement processes are the core technologies that enable adaptive governance. They should be evaluated with the same rigor as we would the components of a physical system.
Generating knowledge from or with stakeholders in undergoverned or competitive contexts can require creative approaches to data collection and analysis, because in-person processes may not be possible or because stakeholders in the research may be unwilling to fully participate. Remote engagements facilitated by technology can help provide researchers with valuable information about target populations when in-person access is not possible, but these same remote and virtual engagements through traditional technologies may miss highly valuable elements of nonverbal or contextual information.
One newer technology that can facilitate interaction methods is virtual, augmented, and mixed reality (VAMR). 77 VAMR couples three different but closely related immersive technologies. VR presents computer-generated or prerecorded images through a VR headset that conceals the wearer's eyes and isolates the wearer from the real world. 78 Users view images and sometimes receive other haptics (e.g., sound, joystick movement) that convey presence in a virtual environment. AR combines VR computer graphics with real-world scenes and interactions. 79 It overlays fully virtual worlds with contacts, interactions, and navigation with the real world. For example, AR can be understood as a graphics technology where the layer of the virtual world is placed on top of the real world without allowing the two to interact. 76 CONVERGE is an NSF-funded initiative to increase capacity for rapid response interdisciplinary disaster research through research network development, training, funding, and data management (see Natural Hazards Center, University of Colorado Boulder, "About CONVERGE," webpage, undated). 77 Apostolakis and Pickett, 1998. 78  Michael Deering, "High Resolution Virtual Reality," Proceedings of the 19th Annual Conference on Computer Graphics and Interactive Techniques, Vol. 26, No. 2, 1992.   79 Borko Fuhrt, ed., Handbook of Augmented Reality, Berlin, Germany: Springer Science and Business  Media, 2011.  Mixed reality is an advanced version of AR, where the physical and digital world are allowed to interact. 
80
Data collection at scale is another area for development, given that it improves the capacity of researchers to gain valuable knowledge about stakeholders when direct engagement (even remote) is not feasible. Extracting information from remote sensing, Internet of Things (IoT), social media, and other secondary sources can provide important information about population beliefs and behavior, but it can be challenging because of the potential for bias in the data and the difficulty of quality checking the findings from existing tools to exploit large data sets.
Finally, research to address the privacy implications of some of these remote and passive data collection activities would be valuable. For example, in the context of VAMR, research suggests that machine-learning (ML) algorithms can learn to identify individuals with high accuracy using their physical movements while interacting with these systems. 
81
Regardless of whether knowledge is generated through unidirectional or bidirectional processes, communication is key to multi-stakeholder research, and improved technical approaches for rapid, accurate, interpretable, and trustworthy communication channels and improved understanding of how to design communication for multi-stakeholder purposes are needed.
Developing culturally appropriate instantaneous translations can significantly improve the ability to communicate research findings or develop coproduction processes with differ-ent groups. This aspect needs to go beyond efforts to translate words and involve finding ways to visually and narratively present research findings that are acceptable and interpretable to different groups. 82 For language and related communication issues, there is the possibility to improve such tools as Google Translate. This improvement could enhance the ability of people who speak different languages to interact without the need for a person as a translator.
Trusted communication channels are crucial for ensuring that messages are heard and responses are appropriate. Researchers attempting to build a base for future multi-stakeholder coproduction or deliberation processes will find that developing ways to ensure the trust in, and the credibility of, information channels is a crucial precondition for deeper engagement efforts. Research efforts to reduce privacy risks while still communicating crucial information, defang misinformation efforts, and improve cybersecurity are all essential to this process. Questions about how to improve access to information while simultaneously ensuring the quality of the information accessed is an important secondary issue. For instance, research into fifth-generation (5G) technology is beyond the scope of this chapter, but it is relevant to some of the topics discussed here.
As multi-stakeholder research engages with new methods and procedures for communication, it also raises several questions related to privacy and emotional risk. For instance, research on how to obscure the purpose or intent of research questions from some actors may also be necessary in highly contested spaces, both to protect U.S. strategic interests and to protect individuals or populations involved in multi-stakeholder research.
Research on multi-stakeholder communications increasingly recognizes that humans preferentially receive and process information according to consistency with group identity and comfort with the communications networks through which it arrives. 83 To address these challenges, one research frontier uses social network analysis to understand how information flows within communities and then applies influence maximization algorithms on the network to understand how to best disseminate messages. These algorithms can be designed to help pursue different constellations of goals-for example, ensuring equitable distribution of messages to all the members of a community irrespective of the density of their networking and the particular groups to which they belong. 84 To date, this work has largely been focused on uncontested messages involving good health practices and warnings of natural disasters. But future research might usefully extend these concepts and methods to more contested information-for instance, by seeking to understand and manage the flows of information to individuals that they might find consistent and inconsistent with their values and identity. 82 National Academies of Science Engineering and Medicine, 2017. 83 Dan M. Kahan and Donald Braman, "Cultural Cognition and Public Policy," Yale Law & Policy Review,  Vol. 24, 2006.   84 Aida Rahmattalabi, Shahin Jabbari, Himabindu Lakkaraju, Phebe Vayanos, Eric Rice, and Milind Tambe, "Fair Influence Maximization: A Welfare Optimization Approach," in Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence, Edinburgh, Scotland, 2020.
Research needs and opportunities exist related to both the process of coproduction and tools that might enhance it. Although there is widespread agreement and much anecdotal evidence that coproduction processes yield significant benefits, coproduction practitioners report many challenges-for example, power imbalances among participants, such as those between researchers who may have increased access to funding, specialized knowledge, and prestige relative to many stakeholders. Power imbalances also exist among the stakeholders, and cultural differences may make it easier for some to participate relative to others. Addressing such barriers is an important area of research in environmental areas and may be at least as salient when using such methods in less governed spaces.
Coproduction processes aim to enhance learning. For instance, participatory modeling has been shown to help participants understand multiple perspectives, promote systems thinking, and improve relationships among participants. 85 But understanding of such learning processes remains limited, with little understanding of how long any new understandings persist among participants after the exercise, whether and how new understandings diffuse among those who did not participate, and how new understanding affects action. Some studies have used surveys, interviews, discourse analysis, and mental model elicitation to track such effects, 86 but future research could greatly improve the ability to derive benefits for coproduction processes. Future research could also explore the use of new technologies for data collection in the context of coproduction. An example would be the use of VAMR (discussed earlier) as a tool to facilitate collective model-building. 87 Ideally, coproduction could enhance the learning and response cycles inherent in such processes as ASDA cycles. But such learning processes present significant challenges from misaligned incentives and inadequate mental models. 88 Work in political and economic forecasting suggests that different types of accountability systems affect decision processes and outcomes. 89 Future research could examine how changing incentives on participants in the kinds of problems typically encountered by the Defense Advanced Research Projects Agency, the U.S. Department of 85 Jordan et al., 2018.   86 Joshua Radinsky, Dan Milz, Moira Zellner, Kelsey Pudlock, Curtis Witek, C. Hoch, and Leilah Lyons, "How Planners and Stakeholders Learn with Visualization Tools: Using Learning Sciences Methods to Examine Planning Processes," Journal of Environmental Planning and Management, Vol. 60, No. 7, 2017;  Jones, 2009.   87 Alexey Voinov, Karen Jenni, 
Steven Gray, Nagesh Kolagani, Pierre D. Glynn, Pierre Bommel, Christina Prell, Moira Zellnerg, Michael Paolissoh, Rebecca Jordani, et al.
Making, Vol. 12, No. 6, 2017.
Improving coproduction is hampered by a lack of conceptual clarity about what is occurring in multi-stakeholder processes. 90 Consolidating research across fields on the dynamics of group decisionmaking could help address this challenge by improving measures of participation and expanding evaluation of stakeholder engagement processes. Highly detailed, large data sets on how different groups interact across engagements and how multi-stakeholder processes evolve would be highly valuable in driving the field forward and enhancing understanding of the actual processes that are occurring. This requires moving beyond simplistic understandings of how information and knowledge are exchanged across stakeholder groups. Knowledge exchange between stakeholders, including comanagement processes, is highly shaped by research field, and work to translate lessons across fields would be a valuable step. 91  In addition to improving understanding of the coproduction process, there may also be important opportunities to improve the available tools. For instance, the VAMR tools could greatly enhance engagement and help stakeholders obtain a more visceral understanding of the dynamics of systems. ML and other statistical tools might help elicit explicit representations of causal relationships contained in the mental models of many participants. Research has only begun to explore potential possibilities in such areas.
Much literature and practice suggest that facilitating deliberation among diverse stakeholders requires an ability and propensity to respect and consider multiple points of view, particularly for situations in which important aspects of the challenge are contested. The traditional quantitative tools of risk and policy analysis are organized around single, bestestimate probability distributions to describe uncertainty about the state of the world and often aggregate preferences into a single utility function to rank alternative outcomes. These methods and tools are poorly designed to bring quantitative information into debates with contested problem framings. One promising research area aims to develop analytic, multiscenario, multi-objective decision support tools that better reflect such multiple points of view. Such approaches are often gathered under the label Decision Making Under Deep Uncertainty (DMDU). 90 Ioan Fazey, Lukas Bunse, Joshua Msika, Maria Pinke, Katherine Preedy, Anna C. Evely, Emily Lambert, Emily Hastings, Sue Morris, and Mark S. Reed, "Evaluating Knowledge Exchange in 
Interdisciplinary and Multi-Stakeholder Research," Global Environmental Change, Vol. 25, No. 1, 2014;
Andrew M. Parker, Sinduja V. Srinivasan, Robert J. Lempert, and Sandra H. Berry, "Evaluating Simulation-Derived Scenarios for Effective Decision Support," Technological Forecasting and Social Change, Vol. 91, 2015;
and Min Gong, Robert Lempert, Andrew Parker, Lauren A. Mayer, Jordan Fischbach, Matthew Sisco, Zhimin Mao, David H. Krantz, and Howard Kunreuther, "
Environment," Environmental Modelling and Software, Vol. 91, No. 3, 2017.
92
Other DMDU work explicitly facilitates deliberations among multiple stakeholders in the context of polycentric governance. For instance, recent work using multi-objective Robust Decision Making (MORDM) methods helped four neighboring cities in North Carolina jointly link and then manage their water systems in the presence of differing objectives and under conditions of deep uncertainty about future demand and climate. 97 In the context of ASDA, the resulting strategies were dynamic-that is, they evolved over time in response to financial and environmental triggers, and they covered multiple timescales, such as short-term operational decisions (e.g., reservoir management rules) and long-term infrastructure investments. The analysis linked timescales by shifting the operational decisions from rule-based procedures to dynamic risk-of-failure triggers and by shifting the infrastructure investments from static to adaptive policy pathways. The analytics were then organized in visualization packages and decision support processes that facilitated deliberations among representatives from the four cities to help them develop independent but coordinated strategies that satisfied multiple objectives in many plausible scenarios.
Recent work links such multi-scenario, multi-objective decision support approaches to social science approaches that recognize that stakeholders go beyond having differing objectives and expectations, actually clustering into distinct worldviews. 98 Such worldviews consist of correlated sets of values, beliefs, and policy preferences that shape how individuals understand, judge, and act in the world. Mixed qualitative and quantitative approaches, such as fuzzy cognitive mapping, can provide insights into the multilayered understandings of complex problems held by different stakeholders. 99 With these new approaches, social scientists use many of the methods described in the discussion earlier in this report to identify the worldviews in a community (see the "Approaches to Multi-Stakeholder Research: Stakeholders as a Focus of Research" section). Analysts then work separately with stakeholders from each worldview to coproduce a quantitative policy analysis. The solutions from each worldview are then viewed from the vantage of the others. In general, the solutions are each dynamic, as already described. These information products can then facilitate deliberations seeking to improve understanding among the parties, potential reframing of each of the worldviews, and potential compromise solutions. Such work is in its initial stages, and future research might improve each of its understanding, coproduction, and deliberation elements.
Finally, as with coproduction, deliberation processes would also benefit substantially from efforts to develop consistent evaluation metrics and protocols to understand which processes are most effective in driving salient, credible, and legitimate multi-stakeholder decisions, specifically, differing degrees of power dispersion and contestation. Trust in processes is crucial to developing trusted research outputs, and in contested spaces, the basis of trust may vary from traditional scientific rationalities. Bureaucratic imperatives, social relationships, existing sources of authority and legitimacy, and cross-cutting issues all influence what research processes are considered salient, legitimate, and credible. Once lost, credibility and legitimacy can be difficult to restore in the short term, so additional research should particularly focus on how to maintain trust among multi-stakeholder research partners, raise 98 Robert J. Lempert and Sara Turner, "Engaging Multiple Worldviews with Quantitative Decision Support: A Robust Decision Making Demonstration Using the Lake Model," Risk Analysis, Vol. 41, No. 6, June 2021; Marco Verweij, "The Remarkable Restoration of the Rhine: Plural Rationalities in Regional Water Politics,"  Water International, Vol. 42, No. 2, 2017.   99 Alison Singer, Steven Gray, Artina Sadler, Laura Schmitt Olabisi, Kyle Metta, Renee Wallace, Maria Claudia Lopez, Josh Introne, Maddie Gorman, and Jane Henderson, "Translating Community Narratives into Semi-Quantitative Models to Understand the Dynamics of Socio-Environmental Crises," Environmental 
Modelling & Software, Vol. 97, 2017.
Multi-stakeholder research can significantly improve the ability of the U.S. government to pursue its interests in competitive UGS. The research investments suggested here could enhance the practice of multi-stakeholder research. First, such research could expand the types of stakeholders engaged in the research process and the ways in which they are drawn into the process (as consumers, creators, or subjects) and could help research teams identify and address practical problems more effectively. Therefore, broadening the number of groups engaged in research could increase the types and levels of expertise engaged in solving a problem and prevent the capture of the research process by any one stakeholder interest.
Second, this research could also speed the ability of research teams to derive and adjust problem framings and mental models. An enhanced understanding of how participation and decisionmaking function in multi-stakeholder spaces would improve both conceptual thinking about these methods and the ability of practitioners to deploy them in a wide variety of contexts, particularly in cases in which they are not considered part of standard practice (such as AI research, or research conducted with strategic competitors). It could also enhance the ability of research teams to build trust among disparate actors by choosing processes that are most likely to achieve the goals of salience, credibility, and legitimacy.
Third, rigorous testing and evaluating of these approaches could help dramatically improve and consolidate practices in this space, driving an improvement in quality across a variety of fields. Coupled with investments in tool-building to speed the ability of multistakeholder teams to create shared languages and visions and identify spaces for compromise, this could enhance the nimbleness of multi-stakeholder research and enable more fluid engagements to respond to changing conditions and compositions of stakeholder groups. Most important, research investments would be required to design or adapt tools to enable multi-stakeholder research in competitive contexts.
Finally, there is much research on the governance of complex problems, but the practice of adaptive governance is significantly hampered by the lack of practical tools that can be deployed, either to change the practice of public management or work within its confines to enhance nimbleness in changing and uncertain contexts and adapt them to the specific needs of DoD stakeholders. The review presented here has described some core principles and tools that already exist for multi-stakeholder engagement and tried to identify some areas where investment in the tools of research would help improve the quality of insight derived from these complex multi-stakeholder research processes in research on long-term competition and UGS.
When the U.S. Army discovered the danger that improvised explosive devices (IEDs)-along with rocket-propelled grenades, explosively formed penetrators, underbody mines, and small arms fire threats-posed to the light tactical vehicle that it was using in Iraq as part of the force, it needed to respond quickly and agilely to determine how to address the problem. As a result, the Joint Improvised Explosive Device Defeat Organization was formed in 2006 to explore and identify ways to defeat this kind of threat to light tactical vehicles with the goal of speeding up the strategic process of getting a solution into the field fast. The result of that process was Mine-Resistant Ambush Protection (MRAP), which was fielded as a solution to the problem. This was an example of the rapid development of a new weapon that went through the force generation and operating processes to get to the field. However, even in this rapid example, although the time line to sense a problem, identify a solution, and then acquire and field the solution was shorter than normal, it was years long. To be more competitive, the U.S. Department of Defense (DoD) must be able to adapt not just its strategy but the warfighting capabilities that are available. That, in turn, requires making many more DoD processes agile.
Although some advances in agility can come from people and processes alone, technology does frequently play a significant role, in both short-term tactical exchanges and long-term strategic competition. The promise of using technology to achieve greater agility is often alluring. However, there are important trade-offs to consider when doing so involves considerable changes to existing combinations of people, processes, and technologies. 1 The associated increase in the reliance on machines that suggest or even take action can leave humans who are accustomed to being "in the loop" feeling left out, thus jeopardizing both the transparency and the interpretability of decisionmaking. Maintaining an Army that is ready to fight now while also modernizing for the future fight requires agility, but not just for agility's 1 Technology-driven initiatives that ignore various sources of organizational norms, for example, do so at their own peril. But when these factors are adequately addressed, these initiatives can lead to advantageous shifts in culture that otherwise never would have come to pass. sake-it must be focused on "winning."
2
3
Technology that automates even a limited aspect of strategic decisionmaking often promises to accelerate decisionmaking, but assessing the quality of the resulting decisions against those derived without such aids can prove difficult. The presence of lethal adversaries further complicates DoD-specific choices of whether to take these technical leaps from similar choices found elsewhere. For example, the race toward autonomous commercial vehicles is driven primarily by market forces, but the balance of safety is one of our own making. In contrast, when U.S. adversaries develop automated weapons, successful defensive measures might require more automation than DoD might otherwise be comfortable with. An examination of how far to the left of "bang" this dynamic should or could extend-how alert, prepared, and able to respond DoD should be before the problem manifests (e.g., the need for MRAP before IEDs emerged)-goes all the way back to the force generation process.
One key part of the force generation process is Planning, Programming, Budgeting, and Execution (PPBE), which is one of three processes that support the Defense Acquisition System. The PPBE process focuses on financial management and resource allocation for current and future weapon systems. PPBE decisionmaking within DoD is exceedingly complex, and it is further examined in the remainder of this chapter through an Army-oriented lens associated with the inputs to and outputs from the Total Army Analysis (TAA) process. 
4
5
Table 
14
14
14
Force structure plan Previous appropriation years Fortunately, this process has been mostly successful. For the most part, one can tell a logical and rational story that connects the dots of national defense strategy, the PPBE process, and the battlefield in a forward direction. Debating whether and where the process has or has not broken down is not the focus of this analysis. Instead, this chapter examines how it could be augmented or improved with greater use of technology and discusses the desirability of these potential improvements. Thus, instead of discussing the demise of the Future Combat Systems Program, this chapter presents different questions:
• How many decisions went into this year's TAA process? • Is it possible to reproduce the information inputs that went into those decisions and the business rules or other logic that went into their adjudication? • How would those decisions change if an important aspect of the operational environment were to change? How long would it take to rerun the entire analysis if necessary? • How many permutations of alternative futures were considered?
Increasing the bandwidth and lowering the latency in the information exchanges between generating and operating force processes is not enough. Without agile decisionmaking processes on both sides, more information at greater speeds might overwhelm, not inform.   The PPBE Process as an Undergoverned Space Strategic direction and resource allocation that are the focus of PPBE are ultimately controlled by elected officials, but, in the PPBE process, the machinery that actually churns through all of the intermediate analytical and procedural steps is a massive assembly of hierarchical administration that performs a similar function year after year. Although on the surface this might seem to suggest that PPBE is an overgoverned space, this chapter differentiates between bureaucracy and governance; the latter focuses specifically on the processes used for direction and control. To be clear, the PPBE process has large amounts of bureaucracy and governance-it has so much that the scale itself might lead one to label it an undergoverned space. Each silo may make sense by itself, and the reasoning behind the decisionmaking within may be known to all of its internal participants. Furthermore, the hierarchical thread is often visible from the top down. Even though cross-functional teams do emerge, 
6
The pros and cons of greater agility can be considered first through this internal enterprise perspective, largely from the generating force; the presence of external competitors and adversaries is not considered from this perspective. But when the scope is expanded to include the operating force, this exemption ceases, because the enemy gets a vote. The handoff from generating to operating-whether of trained and ready troops or of materiel solutions, concepts, capabilities, and organizational structures-is another area of the undergoverned spaces label. Again, to be clear, there is a thread that can be connected between the capabilities and readiness levels of the force that the Army has on the battlefield today and the choices that were made as part of the preceding years of the PPBE process, but it is not always as clear and deliberate a line as it could be, and improving the strength of this connection is the target of significant and ongoing attention. 
7
The further left one moves toward force generation in Table 
14
8
This level of transactional activity might be too granular even for the data-driven approaches that are considered in this analysis, and one might have to squint to see how the more efficient routing of case-processing workflows or resolutions of unmatched transactions end up moving the needle in strategic competition. The benefits do accrue, and in aggregate-including second-and third-order effects, such as reduced needs for repetitive labor and training or other automation by other means of robotic processes; the savings and efficiencies are substantial at the scale of the U.S. Army. But they are essentially optimizations within a given design and not the process of creating a new design-a task that continues to be carried out primarily by humans (and for good reason, given the limitations of technology). Opportunities exist both in operating within a given construct and in better mapping the aggregate benefit of lower-level optimizations to higher-order objectives, as well as in facilitating the higher-level decisionmaking that leads to the fundamental design changes to the operating environment.
A separate but related area of progress on the force generating side is the steady evolution beyond massive data calls to feed information requirements for the various deliberations that occur regularly within DoD. Although some cross-service obstacles do remain, both DoD as a whole and the U.S. Army specifically have made real progress in standing up data platforms that are capable of ingesting authoritative data in nearly real time and generating dashboard equivalents of the Microsoft PowerPoint presentations that used to take rooms full of people days, weeks, and sometimes months to assemble. Not only does this make the descriptive process more efficient, but it also allows decisionmakers to ask diagnostic questions about how a particular data point can be broken down and further analyzed and forms the basis from which more-advanced predictive and prescriptive analytics can subsequently be performed.
Despite this progress, the gap that remains, which is the motivating problem for this analysis, can best be summarized with a humorous criticism occasionally leveled at such efforts: Any investment in creating these dashboards for senior leaders should also set aside money for a human being to interpret that dashboard and explain it to those senior leaders. It is true that data literacy might increase direct senior leader engagement with these tools, but, for the time being, the actual decisionmaking largely occurs outside the confines of the computer screen. It is informed by the data and connected to the daily transactional processes, but it is a predominantly human endeavor. Artificial intelligence (AI) still has a ways to go before it gets a seat at the table next to best military judgment.
However, there are several developments on the horizon that might bring this vision one step closer to reality. The race toward successful convergence of better data management, descriptive and diagnostic dashboards, best military judgment, the Military Decisionmaking Process (MDMP), and AI will yield significant benefits to those who win. The next two sections discuss what lies ahead for using technology to improve the agility of force generation and lay out possible emerging pathways toward achieving that goal.
This section highlights two key semi-automated, data-driven approaches to achieving greater agility. The first is the ability to compress the time it takes to turn the crank, so to speak, and update complex decisionmaking given new information. The second, which is more of a stretch goal, would be to leverage the same foundation for use in the vast number of what-if experiments that modeling and simulation and AI algorithms require to train on and produce novel insights and competitive advantage.
The first approach must solve the following problems: (1) accurately encapsulate the inputs to decisions and the logic by which they are governing; (2) sense changes to those inputs and trigger updates; and (3) propagate the outputs and their effects as inputs to other relevant decision points. With these problems solved, it is possible to achieve the turn-thecrank-faster goal by keeping the state of the "virtual" model up to date with the "physical" organization and using its output as a human decision support tool. The second (or stretch) goal can be met by feeding a model artificially generated inputs as one-off explorations or at scale to elicit emergent behaviors. In other words, the objective is to have a digital twin of a very complex organization;
9
10
14
The idea of digital twins has its roots more in manufacturing and complex equipment than in organizational management. Recently, the digital twin concept has been extended to modeling an organization's well-defined business processes at the operational level-for example, optimizing supply chains by stress-testing disruptions or new ideas; testing new products or marketing campaigns on digital models of customers and business partners; updating a bank's business model to adapt to changes in customer behavior, regulations, fintech technology, and society in general; and managing large complex systems, such as cities in planning and disaster response scenarios that must balance such factors as costs and efficiency of transportation and other core functions. 11 The leap that is being explored in this analysis is what would be possible if the same concept could fully be leveraged to aid strategic long-term decisionmaking in complex organizations.
Given the evolution shown in Table 
14
Recently, however, BPI tools have reached a level of maturity at which they are able to analyze transactional-level data from workflow-based systems to automate some of the data flow between the architecture and the physical process. If the transactional data are thought of as the physical process and the architecture is thought of as the digital model, process mining can provide an automated data flow of what the process looks like in practice, including the variants that are most common, and pinpoint the steps that are causing the greatest inefficiencies (e.g., identifying that an upstream data error is causing manual processing downstream that requires significant attention and causes major delays). This might cross the threshold for consideration as a digital shadow.
Moving in the other direction, from model to physical process, the same tools are capable of generating predictive models that can indicate that a particular transaction is headed toward a particular variant of the process and flag the transaction for intervention, either automatic or manual, to set it on the right path. One might think of this link as qualifying as a digital twin with automated data flows in both directions, although this is a bit of a stretch. However, the decisions involved are fairly constrained to the given environment and can hardly be considered strategic, and they do not rise to the level of designing new processes. That is not to say that BPI and process mining insights cannot inform strategic decisions, but, at this point, they are limited, for the most part, to informing the humans who are making them.
A related technological advancement that has yielded benefits is robotic process automation (RPA) and the evolution of RPA to intelligent process automation (IPA). These tools are not necessarily tied to enterprise architecture or process mining, although they can be. 
12
Another significant advance, farther up the ladder of decisionmaking complexity, is the creation of enterprise data lakes within both the U.S. Army and DoD. Partially in response to National Defense Authorization Act (NDAA) and Evidence Act requirements and partially because of senior leader demand to break through information siloes to provide more-timely inputs for resource-informed decisionmaking, such data lakes have been massive undertakings that build on the foundations of formal data strategy and data governance bodies to ensure that the gathered information is authoritative and up to date. As previously mentioned, one of the principal advantages of this work is that, once the data flows, transformations, and visualizations have been created, subsequent measurement and assessment that previously required time-and resource-intensive data calls can now be performed in real time and can further allow consumers to perform drill-down diagnostics that can unpack nearly all data points. As a point of reference, equivalence has already been reached in replacing important PowerPoint briefings that took months to generate with dashboards showing the exact same metrics based on the most-recent and most-authoritative data. Further decision-support tools that leverage advanced predictive or prescriptive algorithms are also making their way into these platforms, but, for most important decisions, the humans are still fully in charge-and with good reason. 13 Automation does not address any of the potential pitfalls that are inherent in further delegating decisions to algorithms, such as bias in the training data or flaws in the model; it can amplify the negative consequences if they are otherwise ignored. 14 Foreshadowing this chapter's discussion of pathways, these tools have also been drivers of methods to formalize and document decisionmaking rules and information inputs in the interest of transparency and fairness required by good sense and various regulatory regimes.
Despite all of these advances, organizations as complex as DoD or the Department of the Army are not yet on a trajectory toward achieving the benefits of full digital twins. Several key impediments remain. As previously mentioned, the most important limitation is that there is still a significant number of decisions that are not documented, let alone automated. To some degree, it is the most important strategic decisions that fall into this category. Potential solutions to this challenge are described and reviewed in the discussion of pathways ahead.
A second significant impediment is the scale of the apparatus that is involved with the PPBE process, let alone its intersections with the operating force (e.g., Combatant Command Integrated Priority Lists). With the exception of the discretionary latitude given to DoD, the U.S. Army, and the other services to make execution adjustments within a given year, the rest of the machinery involves the preparation and submission of the President's Budget request to Congress, Congress itself, and the many other arms of government (e.g., the Treasury) that are all now operating on the PPBE time frames (see 
Figures 14.3 and 14.4)
Other process changes, with little or no connection to technical enablers, are primarily policy-oriented solutions. These are important, but they are not the focus of this analysis. For example, existing lines of effort aimed at addressing limitations associated with budget appropriation categories ("color of money") that inhibit agile IT acquisition practices might alleviate some of these concerns. 
15
The TAA process serves as a good example of how the status quo presents opportunities for greater agility that might be within reach given current technical enablers (see 
Figure 14.5)
Although TAA is an important step in the prioritization process that aligns resources to strategy (and one that actually does rely on modeling and simulation and historical data), most and perhaps even all of the adjacent steps-whether development of national strategy or congressional review of justifications for spending-exemplify the type of strategic decisionmaking that has yet to fully benefit from advances in technology in the same way that process mining and IPA have moved the needle elsewhere.
The argument here is not that there is a one-size-fits-all, technology-driven solution for performing the analysis required to support any of these steps-the variation in approaches that are now used probably reflects the best-fit solutions available. Instead, the hypothesis is that there might be a one-size-fits-all solution for encapsulating the logic, inputs, and outputs at each step and the relationships between them such that evaluation in response to changing conditions is nearly continuous instead of periodic or episodic. The straw man counterargument is that some strategic analysis does not have inputs, outputs, or logic that can be encapsulated, which should give pause for other reasons. In between is the argument that encapsulating the logic, inputs, and outputs is possible but has heretofore been too difficult an undertaking to pursue. Even if there is a middle space of steps that are "difficult but not impossible" to encapsulate, such as strategic tabletop exercises with nonzero-sum trade spaces, such steps are likely the exception and not the rule. They either may remain as manual steps that must be performed, no matter how much automation has been introduced into the rest of the process, or will successfully be encapsulated at some possibly lower level of resolution that provides a reasonable trade-off between fidelity and automation.  Table 
14
The digital model, shadow, and twin framework previously introduced can be further expanded by incorporating the notion that there are degrees of autonomy that such an evolution may adopt. In the context of this analysis, autonomy factors primarily into the automation of data flows from the digital twin back into the organization. When the model or twin suggests a new decision based on some update to the inputs or the decision logic, does that decision flow directly back into the organization in a fully automatic fashion with little to no supervision? Is there a human monitor in the loop? Or is the decision provided as a suggestion in an alert to a human in the loop who must subsequently take action? To summarize   
18
From these examples alone, the following can be surmised: (1) automation has been incorporated at the most-strategic levels of decisionmaking (at least by some); ( 
2
Returning to the previous juxtaposition of the generating and operating force, the thinking at the pointy edge of the spear in the latter is pushing the bounds of the autonomy spectrum and branching out into long-term planning horizons and consideration of sequences of competition below armed conflict, armed conflict, and return to competition. Figure 
14
With manual data flow in both directions (see Table 
14
Moving up a step to a digital shadow with some automation-in which the inputs, business rules, and decision outputs could be captured as part of a regular workflow-strategists concerned with long-term competition would be able to see where the U.S. Army is in the process at any given time, early enough to manually intervene when strategy and execution fall out of alignment either because of externalities in the operational environment or because of misinterpretation of guidance in previous steps of the process. Monitoring whether policy changes around adaptive acquisition are encouraging the desired patterns of behavior and quickly diagnosing problems where they exist are examples of additional utility that a digital shadow provides over a digital model. This same foundation, coupled with the right set of metrics, could also open the door to countless optimization exercises, what-if scenarios, and forecasting excursions by feeding new inputs to existing decision points and business rules. 20  As important, and perhaps even more so, would be the ability to perform the same exploratory exercises by adjusting the business rules themselves instead of or in addition to the use of new data. The benefits that such an advance would provide, in terms of understanding both the direct impacts of a decision and the second-and third-order downstream impacts and remaining consistent with upstream guidance, could also have operational and tactical utility.
For example, revealing how the decision to extend the period of basic training might affect those managing stationing given limited housing, barracks, ranges, and other constraints would better allow decisionmakers in one area to understand a fuller picture of the enterprise impacts and trade-offs from their decisions. To some degree, this already happens in practice, such as when a program that was previously resourced according to assumptions that were valid at the time is subsequently given more funding or has funding taken away because of shifts in priorities or the ability to execute. However, these adjustments are done manually and on an ad hoc basis: There is no model that is constantly running in the background, examining how previous decisions should be revisited as the inputs to those decisions change in real time. When model interpretability and transparency are accounted for as requirements, these solutions need not feel like intractable black boxes, and the signals they reveal, on reflection by humans, often find grounding in intuitive explanations that can 20 The objectives for optimization can vary in granularity from a binary indicator of whether the inputs provided resulted in a win or a loss to detailed measurements of underlying observables of interest. For the former, winning is meant to encapsulate some definition of the term at the most strategic level that approximates the practical definition used for tabletop exercises and board games, such as RAND's recently released board game Hedgemony. (See Michael E. Linick, John Yurchak, Michael Spirtas, Stephen Dalzell, Yuna Huh Wong, and Yvonne K. Crane, Hedgemony: A Game of Strategic Choices, Santa Monica, Calif.: RAND Corporation, TL-301-OSD, 2020.) For the latter, any metric or set of metrics that are outputs of the model, such as budgets, readiness levels, and case processing backlogs, could be the objective for optimization. be further validated and explored in greater detail and become the basis of an iterative cycle of improvement.
Up until this point, this chapter has not discussed closing the loop, so to speak, by connecting the outputs of the digital twin back to the physical organization in an automated fashion, and it seems unlikely that the levels of capability and comfort required to reach that stage will be reached anytime soon. When the operations tempo on the operational force goes into high gear, the impact on the generating force is felt, and, if the generating force's ability to support operations is impeded, the consequences can be significant. Occasionally allowing backlogs to accumulate or deferring nonessential tasks is a prudent response, but it is not one that can last indefinitely, because this mode eventually breaks down if the time and space to catch up using both manual and automated means are not also provided. As is the case with a deferred maintenance strategy in facilities management, there is a threshold beyond which the accumulation of risk is too great and the future costs become too high. Therefore, in the periods of greatest stress over long periods of time, it is not inconceivable that the U.S. Army might wish to delegate more and more decisionmaking to machine-speed mechanisms with increasing levels of autonomy-up to and including human-in-the-loop, human-on-the-loop, and fully automatic modes of operation. 
21
22
23
Having laid out a progression of potential benefits of the digital twin approach, this chapter now turns back to the counterargument that it is sometimes best to move slow to go fast for strategic issues. 
24
25
26
It is quite possible that the PPBE process-with its timescales and human deliberative subprocesses-continues to be the best way ahead for strategic competition. But what caused suboptimal outcomes for some of the examples in the preceding paragraph is the confusion of operational speed (moving quickly) with strategic speed (reducing the time it takes to deliver value). Given that distinction, there are variants of the digital twin approach that could succumb to the same mistake. However, there are clearly others that would aim directly at the goal of delivering value faster by exploring the solution space in even greater depth than is possible with human cognition alone. 
27
Even with a gaze that looks significantly into the future, the vision for concrete benefits remains fuzzy. The incremental gains that can be seen today from applying the digital twin approach to games that are less abstract and more concretely defined can help spur the imagination. Many strategic thinkers in the Army and elsewhere are avid chess players, but the implications of competition, either armed with or lacking approaches that allow for winning strategies to be learned by complex digital twins trained not only with rules and history but through extensive competition against each other, might be better highlighted by advances in AI's ability to compete in a different game: Go.
In 2016, at a pivotal moment in game 2 of the matchup between the human Go champion at the time and AlphaGo, the AI developed by Google DeepMind, the machine played a move-move 37-on a portion of the board that was extremely uncommon for that stage of the game. Many thought it was a mistake. But as the game unfolded, the decisive action subsequently occurred in the region of the board that the machine had chosen for its unorthodox earlier move. In part, the success was the ability of the DeepMind algorithm to search the possible space of moves for strategies that were predicted to be successful and, specifically, for strategies that the human opponent may have overlooked or otherwise discounted the longterm benefits of because of their negative appearance in the short or medium terms.
Despite this highlight for the DeepMind team, and AlphaGo's ultimate 4-1 victory in the series of matches, an equally important part of this story is that the human player, Lee Sedol, was able to recover in a subsequent game, appearing to exploit a weakness that he detected in AlphaGo's approach. That weakness was in AlphaGo's ability to comprehend the magnitude of extreme all-or-nothing choices. Lee Sedol's 78th move in game 4, known as the "divine move"-one possible implication of which is that humans are still in a shaping and creation role, even for computers as advanced as AlphaGo-involved both an underestimation by AlphaGo that a human would make such a move and an underestimation of the durability of the advantages gained in the short term over the remainder of the game. 
28
As with Deep Blue's earlier triumph in chess and Watson's subsequent victory in Jeopardy, so too for AlphaGo did the combination of a game or model with defined rules and emerging computing power (in AlphaGo's case, next-generation AI techniques, such as deep learning) make the difference between winning and losing at increasing levels of sophistication-and, for the Army and DoD, "winning matters." 29   Pathways to Get to a Digital Twin Having made a case for the desirability of a digital twin strategy for agility in DoD strategic planning, this chapter now returns to the nontrivial problem of how this evolution might unfold. Several of the pillars of a technical foundation already exist, but there are also many wicked problems that must be overcome-problems that DARPA and the service labs are uniquely suited to tackle.
This chapter has already discussed the role of enterprise architecture, business process modeling and process mining, robotic process automation and IPA, and enterprise data lakes with dashboards and more-advanced analytics as reference points for the status quo. Two additional advances tie all of these together and tie the entire set of capabilities to its potential application to the PPBE process: (1) the evolution of decision platforms that leverage business rules management systems and business rules engines to provide the scaffolding for organizations to manage these components throughout their life cycles and ( 
2
DMN provides a way of attacking the problem from a higher level of abstraction. 
32
14
14
33
1
artifacts, then it might be the case that much of the information required might already be contained within these corpora even if they are somewhat opaque to computers in their current form. For the sake of argument, assume that these vast troves of information could be made machine-readable in some semiautomated fashion, with 70-80-percent coverage of the entire process and 80-90-percent accuracy of the translation. Would that be a sufficient start? If it would not, what would the threshold be, and how quickly could additional technological innovations help close the gap? If data-mining unstructured text is the first, bootstrapping step, using a mix of human and machine interaction to make the model accurate enough for use would be the second. Some of the ground of using machine learning-based approaches for this subsequent calibration of a digital twin to real world data is already being covered through the use of reinforcement learning to help improve the performance of digital twins in manufacturing. 
35
Second, the documentation that is available for the bootstrapping step suffers from at least two deficiencies: It generally lags reality and is not kept up to date in real time, and the processes, inputs, and decision logic that are documented do not always reflect the entirety of the factors used to reach a decision.
Third, the reality on the ground at lower echelons in DoD does not always reflect complete alignment with strategy at the national level. The United States has been described as a difficult opponent in part because of what is perceived to be a gap between the two. 
36
As previously mentioned, the greater agility that might result from the use of a digital twin is of little value if actions are otherwise constrained by policies, processes, mechanisms, or dependencies on partners who cannot respond with the same level of agility. Reengineering all of the business processes associated with the PPBE process to account for the existence of a digital twin capability is well beyond the scope of the discussion in this chapter and is particularly difficult to do given the uncertainty of exactly what this capability might look like. Nevertheless, high-level discussion of basic dynamics can be illustrative.
In some ways, the evolution in personnel management of Continuous Evaluation (CE) might provide relevant insights. 37 The subject of CE (i.e., of an individual's suitability for a sensitive job) might not rise to the level of a strategic disruption, but the fundamental shift from a paradigm of periodic checks to a mechanism that continuously evaluates an automated flow of data against a set of business rules suggests that many parts of the PPBE process could be affected. The implications might stretch all the way to triggering an update to the National Security Strategy in light of a major technology surprise (e.g., an unexpected addition to the list of states with nuclear weapons; an advance in capabilities, such as hypersonics), to shifting investment priorities in response to changes detected by the interaction of digital twins of second-or third-order effects from the actions of an adversary or competitor, or to resolving a digital twin-generated warning that an impending action that had already been subjected to significant human deliberation has nevertheless become inconsistent with current conditions and strategy.
Existing processes might allow a timely reaction to one such occurrence in a given year or the use of emergency procedures to overcome bureaucratic hurdles that prevail during the status quo. But if volatility, uncertainty, complexity, and ambiguity all accelerate, process changes that accept the outputs of a digital twin as at least a prompt to give existing actors greater flexibility to respond will probably produce a more resilient system than one that relies on frameworks that were created primarily to deal with exceptional circumstances. 38 If the digital twin says that it is critical to move more money around in the year of execution than is allowed by legislation, regulation, or policy, or if it says that U.S. national security priorities should be reordered, maybe its suggestion should be reviewed and potentially accepted.
The potential variation in pace suggests that, like the Aegis system-which can run in different modes that put humans in, on, or out of the loop-the mechanisms and processes associated with a digital twin might need to accommodate different levels of autonomy. Table 
14
• Human review or challenge of incremental steps in the process • Human execution of decisions
On: Data go to human operators in command . . . . . . but the system works without them.
. . . but the PPBE occurs without them.
• Ability for humans to audit or explain outputs, including automatically generated national security priorities • Human execution of decisions
Out of: • Limited explainability for humans of how the system's calculations work • Automated execution of decisions Aegis system with notional ideas of related process implications for a digital twin. 
39
Constructing digital twins of the organizations involved in long-term competition (including U.S. organizations and, to the extent possible, those of U.S. competitors and adversaries), structuring the environment as a game in which these twins could interact and reveal emergent behaviors, and exploring how other, related technological advances have been used successfully elsewhere might or might not be strategically fruitful. 
40
Authentically Describing and Forecasting Human Behavior for Policy Analysis: A Review and a Path Forward
In current military operational models, the human aspect is still often represented in a mechanistic way, bearing little resemblance to observations, as if all humans always act the same way in a situation much as a machine would. In reality, human behavior is not deterministic. Without proper representation of behavior, and the reasons behind the behavior, the validity of the model may be seriously flawed, making its performance and predictions questionable.
-North Atlantic Treaty Organization (NATO) 
1
(1)
In his article "Rethinking Competition," Philip J. Root posed challenges to authentically incorporating human behavior into modeling. Those challenges centered on Justin Kelly's and Mike Brennan's Act-Sense-Decide-Adapt (ASDA)
2
• How can we understand, describe, and, to a reasonable extent, forecast human behavior in UGS? • How can we do this using limited and often uncertain data with sufficient accuracy and transparency to earn the trust of leaders, thereby helping them improve policy for the inherently murky challenges posed by strategic competition? 3
These are fundamental questions that are centered on understanding how and why people select behaviors. Human disposition to act is an essential yet elusive variable in policy analysis. Experience with modeling and simulation (M&S) in support of operations in Afghanistan and Iraq has proven that these challenges do not have immediate and satisfactory solutions. There might be no dramatic leap forward from our unreliable understanding and forecasting of human behavior to an authentic, accurate, and dependable model and process. Advances toward the ambitious objectives laid out by Root, Kelly, and Brennan require a shift in scientific approach and a plan for incrementally improving prototype models.
This chapter offers a long-term, phased solution to the challenges posed by Root. Recommendations are based on the RAND Corporation's "will-to-fight" research portfolio. This is an ongoing, five-year series of projects designed to improve understanding of human behavior in conflict. 4 Although this chapter centers on the RAND will-to-fight work, it does so only as an example of prospective approaches. The RAND work builds from hundreds of existing studies, and it is not by any means intended to be a complete solution to the many challenges extant in human behavior forecasting. Other theories, models, and methods should be closely examined.
The proposed approach in the RAND will-to-fight research portfolio is to understand and forecast human behavior and then represent that behavior in a model and simulation that will support policy analysis. Doing so is a shift from general practice in large-scale constructive simulation. Although the approach is different from existing approaches and perhaps more challenging to implement, if successful, it would help policymakers meet objectives for rapid and accurate behavioral forecasting. It would also help broader scientific efforts to model human behavior progress in the future. In turn, this approach would support advances in human-machine teaming, human performance, and influence activities.
I argue that the best way to realize the ambitious objectives of increasing the flexibility and adaptability for competing in UGS-as envisioned by ASDA and supported by new research and development programs, such as the Defense Advanced Research Projects Agency's (DARPA's) Habitus program-is to create a reasonably authentic biopsychosocial, systemof-systems model of human beings; to use that model to focus collection and analysis of data with scientific rigor; to build human agents from this solid baseline; and then to evolve the model, data, and simulation to achieve increasing (but probably always imperfect) authenticity and speed of accurate and adaptive analysis and forecasting over time.
The remainder of this chapter first discusses the difference between realistic M&S and the pursuit of authenticity. It also discusses previous efforts to incorporate human behavior into M&S programs. Then, it examines some of the reasons that attempts to incorporate authentic human behavior have failed. This examination is followed by a discussion of the approach used in RAND's will-to-fight research and of efforts to move toward creating a reasonably authentic biopsychosocial, system-of-systems model of human beings.
Achieving at least a practical modicum of authenticity would be a significant advance in human behavioral modeling beyond efforts to portray realism, or realistic behavior. RAND's will-to-fight research team has argued that there is a fundamental difference between realistic human behavior and authentically modeled human behavior. 5 Realistic simulated human agents used for training or forecasting can be made to mimic agentic choices and a variety of human behaviors. Some elements of realism are predicated on scientific models of components of both simple and complex human behavior. For example, some simulations portray mostly independent components of human physiology (such as the gaze heuristic that describes how agents track and catch a moving object) or the effects of fatigue, while others portray unitary group reactions to such stimuli as changes in state-level policies or, at the local level, changes in traffic or gunfire. 6 Agents can be tailored to support or reject policies, to emote, to hesitate, to flee, or to charge forward aggressively in emergent situations.
Existing simulations have shown that realism can be programmed and made sufficiently complex to help some consumers accept various replications of human behavior. Designers can-and have-convinced military leaders and policymakers that the outcomes of designer 5 Connable et al., 2018. 6 See, for example, Gerd Gigerenzer and Wayne D. Gray, "A Simple Heuristic Successfully Used by Humans, Animals, and Machines: The Story of the RAF and Luftwaffe, Hawks and Ducks, Dogs and Frisbees, Baseball Outfielders and Sidewinder Missiles-Oh My!" Topics in Cognitive Science, 
Vol. 9, No. 2, 2017.
7
8
9
10
As these premier military simulations evolved with a near total lack of human behavior representation, several small teams experimented with human-centric models and simulations with the aspiration of integrating human behavior into the premier simulations. 14 These 11 Connable et al., 2018, pp. 113-157. 12 For another review and similar perspective, see NATO, 2009. 13 In our will-to-fight work, we applied a 24-factor coding analysis to 62 of the 75 games and simulations reviewed for our research. This analysis revealed the degree to which each simulation pursued some form of human behavior representation. Was will to fight included in the game or simulation? Did agents or groups make semi-independent decisions as unique actors, or did they simply follow predetermined branch-andsequel pathways for action? Did the simulation address aspects of human behavior (such as cohesion, leadership, fatigue, or surprise), casualties, or terrain that might affect decision outcomes? Was the human element central to the adjudication of the simulation? Each simulation received an aggregate code of zero to five, with zero representing no human behavior representation and five representing a simulation centered on human behavior. Of the 18 military computer simulations coded, none scored higher than one out of five possible points. Eleven simulations received a composite score of zero, and seven received a composite score of one. For example, we found that the Joint Conflict and Tactical Simulation (JCATS) effectively has no represented human decisionmaking beyond a binary casualty breakpoint threshold, and a student-designed simulation called SPARTAN II contained a rudimentary suppression model but no agent differentiation or individualized decisionmaking. See Connable et al., 2018, Chapter 3.   14 There is considerable literature on human behavior in agent-based simulation and group behavior simulation. In addition to reviewing simulation technical reports and associated articles, interviewing developers, and experimenting in some of the simulations (one of which RAND developed: the Joint Integrated Contingency Model [JICM]), we reviewed the broader literature. In addition to the other works cited in this chapter and in Chapter 3 of 
Connable et al., 2018
Barry Silverman at the University of Pennsylvania designed PMFServ, a standout effort to represent holistic, realistic, and, to some extent, authentic agents. 15 A New Zealand research team designed the Map Aware Non-Uniform Automata (MANA) agent-based distillation model to show how such factors as morale and cohesion might influence decisionmaking in combat. 16 Cognitive models for simulation proliferated. 17 Some research teams and contractors attempted to build holistic behavioral models integrating many different aspects of human behavior. 18 For example, the U.S. Marine Corps' Irreducible Semi-Autonomous Adaptive Combat (ISAAC) model and simulation was intended to move away of Complex Adaptive Systems," Proceedings of the 2010 Winter Simulation Conference, 2010; and Barry G. Silverman, Ben Nye, Ray Kang, Ceyhun Eksin, David Pietrocola, and Gnana Bharathy, "Toward an Extensible Repository of Socio-Cognitive Models: Challenges for Synthesis," briefing, Social Theory and Social Computing: Steps to Integration conference, Waikiki, Hawaii, 
May 22-23, 2010.
Some human behavioral M&S was directly relevant to efforts to understand human behavior in UGS. For example, the UK Defence and Science Technology Laboratory built the Peace Support Operations Model (PSOM). 20 This model sought to show the likely reactions of a civilian population to policy actions taken during what are now commonly referred to as stabilization operations. Developers applied a theory-driven approach based on "economic theories of utility and marginal utility" to represent group decisionmaking. 21 At approximately the same time (in the late 2000s), developers at the Jet Propulsion Laboratory built the Athena Software Development Model (SDM) to answer similar questions: How would populations react to military decisions and actions in stabilization operations? Like PSOM, the Athena SDM relied on rational choice heuristics to portray decisionmaking. 22  Social science understanding of human behavior has advanced since these models were published. Experimental simulations of both individual and group behavior are too numerous to track or analyze here. It is enough to say that, despite these advances, very little of this group-level work has made its way into the models and simulations that are most often used to inform U.S. military leaders and policymakers.
One reason these models have not previously been adopted into advanced simulation is that they lack authenticity; despite their often convincing visualizations and findings, they fail to achieve a suspension of policymaker disbelief or to win over potential advocates.
Several problems have hindered-and continue to hinder-human behavior research.
The lack of authenticity in human behavior research stems from three closely related and, in some cases, overlapping central problems in the approaches that modelers and simulation designers have taken to represent human behavior. Each of these problems is reflected in a different type of approach. In the first case-here called the reverse design approach-individual and small group behavior is replicated as a series of decisions and actions, with little or at least less focus on the theory or theories that might explain the behavior. In the second case-here called the narrow theory approach-larger group behavior (according to our cited analyses, generally more than about 30 people) is adjudicated through the application of a single or dominant theory of human behavior that might or might not lead to a suspension of disbelief and might or might not be sufficiently enduring to justify the expense of its development. In the third case-here called the data-pull approach-the availability of data drives the design of the model and then the design of the simulation. 
23
Reverse Design: Putting the Cart Well Before the Horse Large simulations funded by the U.S. military or allied militaries are typically built by teams of programmers working on deadlines, not by researchers pursuing gradual advances in scientific knowledge. 
24
25
• What behaviors will we show? 
26
Design teams also must contend with computing limitations that restrict the number and complexity of calculations that their software can be asked to perform. They do not want to design a simulation that will crash the military system that it is designed to work on. In addition, design teams are limited by their contracts. In all cases that our team has observed or been informed of in the past five years, large simulation contract stipulations discourage or simply do not encourage the incorporation of human behavior. Behavioral complexity is often viewed as a programmatic risk. Finally, designers told us that most military consumers of simulations do not want to see authentic or even realistic human behavior because uncertain behavior causes unwanted variations in simulation outcomes. 
27
28
29
While combat simulations tend to be designed in reverse, from outcome to theory, models and simulations of civilian population behavior tend to be driven by theories of threshold behaviors, message receptivity, and influence dynamics to make them responsive to actions taken by military forces in games and simulations. 
30
rules-based behavior of some kind. These models tend to either represent populations with no agency-simply responding to stimuli-or employ variations of rational choice or costbenefit heuristics familiar to economists. 31  When attempting to simulate decisions across a large village or city, it makes practical sense to elevate the level of analysis and computation from the individual to the large group. Centering on a single or small set of behavioral theories and reducing variables keeps the computational demands manageable. It also saves an enormous amount of time and labor considering the alternative: modeling every individual and their interactions with the group and then managing at least two layers of interwoven behavioral outputs.
Starting with a theory and working toward an outcome is more logical than moving in the other direction. However, this approach places big bets on a theory and provides little opportunity to fully explore alternative explanations for behavior. A top-down (or, really, top-only) approach assumes away complexity, marginalizes the potentially acute impacts of emergent and divergent behavior, and precludes detailed examination of causality. Models and simulations designed around a single theory are particularly difficult to modify or improve on over time. Rapid obsolescence is a built-in risk because favored theories come and go and new advances in computing power allow ever greater fidelity at the agent level.
Top-down, theory-driven models of group behavior run two more-serious and moreimmediate risks. Consumers of the model and simulation have to believe the central theory to benefit from its outputs. It takes only a bit of skepticism to render these models null for policy decision support. Then again, senior consumers can become so enamored of a single approach that they fail to apply the healthy skepticism that they might apply to a more complex model. Such caveats as a model is always just a model fall away as simulations give the appearance of solving complex sociocultural problems on the fly. This occurred in Afghanistan in the early 2010s, when senior leadership attempted to apply PSOM to real-world data to support real-time combat decisionmaking. 32  More-recent sociological modeling pursues the complex middle ground between top-down theory and agent-based models that represent individual choice and emergent behavior. 33 There tion of Small Group Dynamics, New York: Cambridge University Press, 2011; Mark Granovetter, "Threshold Models of Collective Behavior," American Journal of 
Sociology, Vol. 83, No. 6, May 1978
Steven N. Durlauf and H. Peyton Young, eds., Social Dynamics, Washington, D.C.: Brookings Institution and MIT Press, 2001;
Aaron B. Frank, "Toward Computational Net Assessment,"
34
The data-pull approach was most common in the 2000s and early 2010s, at the height of the popularity of multivariate regression analyses for military applications. "Just give me the data" was commonly heard in assessment and design symposiums. 
35
Examples of this approach are the NATO Consultation, Command, and Control Agency's real-time Afghanistan assessment tool and the U.S. Army Training and Doctrine Command's Irregular Warfare Tactical Wargame. The NATO effort identified and used all available data from military, civil, and private-sector sources across Afghanistan, translated these data into relational variables, and then generated a continually shifting color code in shades of green, yellow, and red that represented the overall status of Afghanistan at a given moment.
The tactical wargame (really, an analytic tool to understand societal-level conditions) was also an effort to identify all sources of available individual-, group-, and national-level data Journal of Defense 
Modeling and Simulation, Vol. 14, No. 1, 2017
Work, Vol. 41, No. 1, 2012
Societies and Social Simulation, Vol. 15, No. 4, 2012.
15
Ultimately, both of these efforts failed, although they had been earnestly applied in the pursuit of accurate population behavior forecasting. The Army model was so complex that it reportedly could not be made to run. 37 The NATO model never succeeded in earning the trust of policymakers or battlefield commanders. 38 Between 2010 and 2020, the many failures and the few minor but generally low-impact successes of the data-pull approach effectively ended the era of multivariate regression fever. 39 Big data and machine learning are driving a similar data-pull approach to solve the problems of modeling and simulating human complexity.
Causal inference represented a seemingly insurmountable hurdle to the data-pull M&S teams. Hundreds of types of data were available, and all seemed to be relevant to understanding the socio-cultural-economic environment. But working backward to infer (let alone prove) causation between two variables (and hundreds of interdependent variables) was neither efficient nor, arguably, possible. Since 2010, RAND researchers participated in and reviewed several research efforts to produce an authentic holistic environment and earn consumers' trust in a way that might support more-effective policy decisionmaking-none succeeded.
As thousands of researchers, modelers, and simulation designers struggled to build a working, holistic representation of human behavior, they were accompanied by a cacophony of warning sirens indicating dangerous gaps in capability and a lack of meaningful forward progress. From at least 2005 through the late 2010s, Western military leaders, DoD staffs, 37 A member of the M&S team provided this information to me in a face-to-face conversation in the early 2010s. I do not have access to other sources that might confirm or deny this information. 38 For more on the debate about this approach and contemporaneous approaches, see Jim Bexfield and Cy Staniec, "Allied Information Sharing Support to ISAF and Support to Afghanistan Transition Metrics," briefing, Brunssum, The Netherlands: NATO Research and Technology Organisation, SAS-091, August 2012; and Ben Connable, Embracing the Fog of War: Assessment and Metrics in Counterinsurgency, Santa Monica, Calif.: RAND Corporation, MG-1086-DOD, 2012. 39 I make this observation in the wake of a ten-year engagement with the operations analysis and M&S communities, detailed in some of the cited works in this chapter, such as 
Connable, 2012;
Connable et al., 2014;
and, most recently, in
Connable et al., 2014, pp. 87-88.
In 2008, an analytic consortium tasked with assessing U.S. defense capabilities to analyze irregular warfare identified nine "extremely high risk" gaps in analytic and modeling capabilities for decision support. 40 These extremely high-risk gaps included understanding civilian populations, understanding civil-military operations, and, more broadly, understanding interaction between actors. In 2009, the Defense Science Board Task Force on Understanding Human Dynamics wrote, "Unfortunately, the current ease of programming is turning adequate programmers into poor modelers capable of turning out tools with impressive interfaces, but little theoretical power under the hood." 41 Also in 2009, a team of NATO scientists examining human behavior representation in simulation warned, "[T]he scientific knowledge in this field is still fragmented and has not reached a useful level of modeling." 42  A standout argument was made by then-Vice Chairman of the Joint Chiefs of Staff Gen Paul Selva. In the 2016 Joint Concept for Human Aspects of Military Operations (JC-HAMO), Selva and his staff stated that the U.S. military did not understand human dynamics and had few tools available to rectify this gap. 43 Selva recognized conflict-both competition and highorder war-as a primarily human endeavor. He singled out the need to understand the will of humans to forecast their decisionmaking. He proposed an operational framework that closely mirrors the ASDA framework. Figure 
15
Despite these clear and well-articulated warnings and demands for improvement, there has been little progress toward integrating practical human behavioral modeling into constructive simulations. These gaps were identified in the previous sections of this chapter. In working on human behavior assessment, modeling, and simulation, I have identified two trends in defense leadership that have inhibited progress. 40 Irregular Warfare Methods, Models and Analysis Working Group, Final Report, Ft. Leavenworth, Kan.: TRADOC Analysis Center, 
August 18, 2008
Staff, 2016, p. 14;
Root, 2020, p. 7
The idea of investing resources in developing a holistic model and simulation of human behavior is simply unpalatable to many senior science leaders, military leaders, and policymakers. Although many leaders with whom we spoke were enthusiastic about the idea of holistic and authentic behavioral modeling, all but a few rejected out of hand the possibility of making progress. 44 Common refrains were, "It's too difficult," "It would take too long," and "It can't be done." Such conclusions build on a false dichotomy: Perfect authenticity is too hard or impossible, so nothing should be done. But I argue for increasingly improving authenticity over a very long time, with the sober understanding that models are always just models and that true authenticity-a 100-percent faithful, scientifically accurate representation of human behavior-is probably never going to be possible. The argument in this chapter is for improving authenticity, not for trying to attain perfection.
Skepticism, and in some cases outright cynicism, about the prospects of human behavior modeling and representation fed into the second observed leadership dynamic-lack of championing. Selva's 2016 report stood out as one of the only senior calls for improvement. 45 The JC-HAMO is no longer available on the website of the Joint Chiefs of Staff, 46 and our project team did not identify any other four-star flag rank advocates for human behavioral M&S. Lack of additional top-level advocacy represents at least some failure of leadership and a collective failure on the part of the scientific community-including our own team-to provide a clear and convincing argument and pathway toward addressing the gaps that DoD has already identified.
The Will-to-Fight Model: An Empirically Derived Organizing Framework Despite this collective lack of progress in the field, most of the pieces are in place to methodically develop and, over time, implement an authentically (but not necessarily fully authen- 44 Supporters tended to hold the rank of O-6 (colonel or naval captain equivalent) or, in a handful of cases, O-8 (two-star flag equivalent). 45 The Vice Chief of the Joint Chiefs of Staff is generally not empowered to drive joint force policy or force design or to have significant influence over service policy or force design. I believe that Selva and his staff are to be commended. The JC-HAMO stands out as one of the most forthright joint concept papers in the era after the September 11, 2001, terrorist attacks. 46 A November 2020 internet search for the title of the report (with and without quotes) returned no matches. There does not appear to be an explanation for the removal of this document from the official website. Other, older joint concept documents can still be found at .mil websites. The Joint Concept for Operating in the Information Environment cites the JC-HAMO and identifies it as a living companion document (Joint Chiefs of Staff, Joint Concept for Operating in the Information Environment (JCOIE), Washington, D.C., 
July 25, 2018, p. 4, footnote 17)
Beginning in late 2015, RAND undertook an effort to model and simulate humans' will to fight in conflict situations, defining will to fight as "the disposition and decision to fight, to act, or to persevere when needed." 47 Initial efforts for the U.S. Army were informed by the existing efforts cited throughout this chapter and through several collaborations with scientists, modelers, and simulation designers in both the public and private sectors. The 2015-2017 effort led to the creation of a structured analytic model and tool for assessing the individual, group, organizational, state, and societal influences on human will and decisionmaking.
Figure 
15
Analysis and simulation have been central to the ongoing research. Building from the finding that constructive computer simulations of human behavior tended to ignore most learning patterns and drivers of human behavior, such as cognitive schemas, emergent adaptation, psychological explanations of personality, cultural influences on behavior, individual physiology, fatigue, and fear, we implemented a complex model of human behavior into a NetLogo prototype and then into the Infantry Warrior Simulation (IWARS). 48   
While we derived our model of influences on human behavior inductively from the various literatures, we pursued what might be called holistic realism in our initial simulation experiments and, later, in our simulation development work for the Army Modeling and Simulation Office (AMSO). We applied and then evolved a reasonable but not yet authentic model 47 Connable et al., 2018, pp. 2-3.   48 For information on IWARS, see Joshua 
Eaton, Ryan Kalnins, Mitchell McKearn, Preston Wilson, and
Modeling and
Statistical Analysis," 2014 Systems and
Information Engineering Design Symposium, Charlottesville, Va., 2014;
and Nazli N. El Samaloty, Roger Schleper, Mary Anne Fawkes, and Dean Muscietta, "Infantry Warrior Simulation (IWARS)," Phalanx, Vol. 40, No. 2, June 2007.
In IWARS experiments, each individual agent in our simulation had a living trait-state personality that interacted with group factors, such as leadership and cohesion. 49 Over thousands of simulated combat runs in three increasingly complex scenarios, we showed that adding holistic realism to individual agents substantially changed the odds of task success or failure in comparison with the supersoldier agents found in most constructive simulations. This finding reinforced similar findings from experiments cited throughout this chapter and was consistent with the broader observation that combat models can produce more variability in results than defense planners often assume. 50   49 Connable et al., 2018, Chapter 3.   50 J. A. Dewar, J. J. Gillogly, and M. L. Juncosa, Non-Monotonicity, Chaos, and Combat Models, Santa  Monica, Calif.: RAND Corporation, R-3995-RC, 1991. 
Given the success of the modeling effort and our initial simulation demonstration, we expanded on our work through AMSO. Over two years, we converted the RAND Will-to-Fight Model into a human behavioral model and tested this model in both tabletop games and simulations. Gradually, we moved from holistic realism to incorporate elements of authenticity.
In place of our original IWARS personality model, designed by a single psychologist for Microsoft in the mid-1990s, we integrated the commonly accepted five-factor personality model into a realistic (still not yet authentic) agent with the factor-by-factor RAND Willto-Fight Model. Between 2018 and 2020, we implemented this agent model into the Army's premier OneSAF simulation.
In mid-2020, we began implementing our human behavioral model into the Unreal 4 gaming engine. 51 As of late 2020, the human behavioral model has been accepted for full incorporation into Version 10 of OneSAF.
Modeling and simulating various implementations of the Will-to-Fight Model has established an empirically derived, holistic framework for organizing and analyzing the factors that influence human behavior. Although the model is not yet fully authentic-and might never be-it can be used to assess the disposition of an individual or group to select from several available actions in a given situation. Together, the realistic simulation behavior generated from the model and the analyses of scientific theory and practice central to the research have set the conditions to move toward authentically modeled and simulated human behavior representation.
With the notable exceptions cited, most efforts to model and simulate human behavior have started with a desired outcome, a single theory or narrow set of theories of human behavior, or simply large sets of data. Our approach starts by accepting a simple organizing theory of human behavior that mirrors our will-to-fight construct: a biopsychosocial framework for variable and data organization and eventually authentic causal modeling. 52  
The term biopsychosocial is widely credited to physician George L. Engel. In the early 1980s, Engel sought to change the process of medical evaluation by adding in psychological, environmental, and social considerations. In Engel's proposed approach, doctors would consider the influences of psychological traits and states, social relationships, and environmental factors in interpreting medical symptoms rather than focusing solely on the narrower, more mechanical process of matching biological symptoms to probable biological or environmental causes. Individual patients would be treated as individual systems-of-systems (a person's body and mind) operating within a broader, interconnected system-of-systems 
(society, environment, etc.)
Engel based his work on Ludwig von Bertalanffy's General System Theory. 53 In doing so, he adopted the theory that everything-a human cell, an individual person, a machine, a village, and ultimately the whole world-operates as a system or as an interrelated system of smaller systems. Engel visualized his system-of-systems approach around the individual. Figure 
15
In his original article, Engel described the biopsychosocial framework as a model. This led to several bruising critiques attacking his use of the term model. Critics argued that his use of the term was unconventional, that there were no relational values between variables, and that there was no causal inference. 54 The biopsychosocial model was not like a schematic model of an automobile or the physiological model of the human body. Therefore, according to these arguments, it was not a model.
Although these critiques had some merit, they generally overlooked the potential value of the biopsychosocial concept. Engel was not proposing a causal multivariate regression model or any other type of computational approach to calculate holism. Instead, he was proposing an epistemic framing of a holistic, systems-based human behavior theory to replace the narrower, school-of-thought-anchored approaches applied by medical, psychiatric, and socio-logical researchers and practitioners. 55 Engel suggested pursuing holism, while the scientific community generally rejected, and still rejects, holism in practice because of its many and seemingly insurmountable challenges.
As we discovered, it is hard to get two psychologists to agree on a unitary theory of human personality, let alone on the existence of a causal relationship model combining the elements of personality and brain functioning, physiological conditioning, social relationships, and more. Starting the pursuit of authentic modeling by trying to force the entirety of science into a biopsychosocial model would be counterproductive.
Starting with a holistic causal biopsychosocial model and then trying to get that model to function would be no more helpful to developing authentic human behavior representation than the notably unhelpful and unsuccessful big sim practices. We have followed our RAND colleagues and other researchers and have not attempted to get all of known science to agree on a holistic model of human behavior at the outset of our research. Creating a causal multivariate regression model of the whole human is not what is proposed, at least not anytime 55 Some of the critiques acknowledge this point, while others overlook it. To continue making forward progress toward authenticity and holism, we are following Engel's intent by accepting a knowingly and transparently imperfect system-of-systems framework. Using the factors in the Will-to-Fight Model-also not a model in many common interpretations of the word-and existing, empirically sound, and broadly accepted behavioral subsystems (e.g., a personality subsystem, a fatigue subsystem, a social cohesion subsystem), we can organize a reasonable biopsychosocial model that can be improved on over considerable time to achieve full authenticity. This is discussed in the next section.
Many researchers have described the necessary components of a holistic human behavioral model. 56 These components include a personality model, a model of the human body and its various subcomponents, a neurological model, a cognitive model, a perception model, a fatigue model, and some kind of rational choice decisionmaking model. Layering beyond the individual, we can add in a social networking model, a social and task cohesion model, and other models that are associated with the transmission of ideas, human influence, human identity, and the motivational factors that influence agentic choice.
Each of these can be broken down into many subordinate models. For example, the human body alone could be broken into a model of each organ, each limb, each joint, the nervous system, etc. Simply listing the prospective parts of a holistic biopsychosocial system is daunting.
If we accept that we are not going to achieve true holism or produce verifiable, causal results from our agent-based model in the short term, then creating a knowingly imperfect but practicable and useful biopsychosocial model becomes possible. The proposed advance is one of organization. All of these component models exist but are typically not organized or applied simultaneously. Most commonly, endogenous models of function-models of cognition, fatigue, resting heart rate variability, physiological systems, etc.-are not integrated with exogenous data.
As a first step, a biopsychosocial model simply concentrates endogenous models for experimentation and research and identifies exogenous social, cultural, and environmental data to influence these models. This system-of-systems, biopsychosocial model-informed by exogenous data-can also be used for forecasting that, although still knowingly imperfect, would arguably be an improvement over approaches described in this chapter. See the section on DARPA's Conflict, Modeling, Planning, and Outcomes Exploration (COMPOEX) for more discussion on the challenges of integrating exogenous variables into endogenous agent-level models. 56 See 
Davis et al., 2018
Our research suggests four pieces that a basically functioning biopsychosocial model must have: a personality model, a physiological model, a cognitive behavioral model, and a social relationship model. These represent the four moving pieces necessary to create a basic, functional model of human decisionmaking and behavior. The model must also have functions for taking in a broad array of other data without destabilizing behavioral representation or overwhelming computational assets.
Each of these four basic subsystems has been thoroughly and effectively modeled and, in many cases, simulated by other research teams and practitioners using empirical processes. The fivefactor model of human personality is broadly accepted by psychologists and psychiatrists. 57  There is considerable literature on five-factor modeling to support careful integration into a basic biopsychosocial model. 58 Models and simulations of the human body proliferate and are in broad use by medical researchers, sports organizations, and the military. For example, the Santos physiological model effectively represents a variety of human physiology and performance. 59 Cognitive modeling is a separate field of research and practice that has many widely used models, including those cited here, such as ACT-R. Social modeling is also well estab-  Personality, Vol. 60, No. 2, June  1992; Robert E. Ployhart, Beng-Chong Lim, and Kim-Yin Chan, "Exploring Relations Between Typical and  Maximum Performance Ratings and the Five Factor Model of Personality," Personnel Psychology, Vol. 54,  No. 4, 2001; and Sonia Roccas, Lilach Sagiv, Shalom H. Schwartz, and Ariel Knafo, "The Big Five Personality  Factors and Personal Values," Personality and Social Psychology Bulletin, Vol. 28, No. 6, June 2002.   58   
Social and Behavioral Sciences, Vol. 32, 2012;
and Ahmed Mustafa, Aisha-Hassan A. Hashim, Othman Khalifa, and Shihab A. Hamed, "Adaptive Emotional Personality Model Based on Fuzzy Logic Interpretation of Five Factor Theory," Signal Processing, Vol. 2, No. 4, November 2008
The four core subsystem models in our individual-level system allow us to create an agent (and eventually, groups) with semipermanent traits and fluctuating states anchored in established literature across at least these four fields of scientific research and practice. Traitstate is a tested approach to understanding human personality and behavior. 61 Briefly, individuals have semipermanent traits-personality factors, physical capabilities, beliefs, motivations, and learned behaviors-that constitute a general disposition to act. The states of each trait fluctuate according to environmental inputs and the condition of the agent, temporarily changing the agent's disposition to act. Over time-or, in some cases, quickly, through trauma-such experiences as conflict, illness, or personal success can shift the semipermanent traits to generate more-lasting changes to behavioral disposition.
Figure 
15
As we evolved our trait-state model, we integrated an overarching will-to-fight value for each agent and for group units. Will to fight was used as an aggregate variable derived from the combination of all traits. Figure 15.6 depicts an example of a simple will-to-fight tracker for an individual agent in a tabletop exercise that we developed to help design human behavior simulations. It shows a score of 1-20, with 1 being very low will to fight and 20 representing heroic will to fight. When civilian behavior in noncombat situations is being modeled, this tracker could be used to represent agent motivation to act.
The RAND Will-to-Fight model allowed us to identify, collect, and analyze real-world information and integrate it into the behavioral traits of each agent. We designed and applied a will-to-fight assessment tool in Microsoft Excel and in Adobe PDF to take in and record data 60 Robert L. Axtell, "Short-Term Opportunities, Medium-Run Bottlenecks, and Long-Time Barriers to Progress in the Evolution of an Agent-Based Social Science," in Aaron B. Frank and Elizabeth M. Bartels, eds., Adaptive Engagement for Undergoverned Spaces: Concepts, Challenges, and Prospects for New Approaches, Santa Monica, Calif.: RAND Corporation, RR-A1275-1, 2022. 61 Charles Donald Spielberger, Manual for the 
State-Trait Anxiety Inventory STAI (Form Y), Palo Alto, Calif.: Mind Garden, 1983;
and Rolf Steyer, Manfred Schmitt, and Michael Eid, "Latent State-Trait Theory and Research in Personality and Individual Differences," European Journal of Personality, Vol. 13, No. 5, 1999.
5
6
7
65
15
COMPOEX and, in different ways, the NATO and Army models before it were laudable projects. They broke ground for efforts to model human behavior. Our understanding of these models informed our less ambitious and (in our view) more practical approach. In the initial will-to-fight work and in our biopsychosocial model, exogenous data are integrated with endogenous models. All exogenous information is used to inform endogenous models, and we model only the agents and their interactions. Figure 
15
Limiting the moving pieces in the biopsychosocial system-of-systems will help make the causal inference challenge more manageable. The factors on the left side of Figure 
15
A digital twin approach to M&S allows for realistic and, eventually, authentic differentiation of individual agents and groups. 67 Instead of relying on templates and archetypes of humans (e.g., a leader, an influencer, a follower, an outlier), a biopsychosocial model centered on the five-factor personality model allows for authentic differentiation using either real-world data or parametric data. 68 Modeling and simulating a U.S. military unit, for example, would allow detailed data collection and authentic differentiation and digital replication of each individual. Modeling and simulating a small village in an undergoverned 66 This tool was provided to the U.S. Army sponsor. It is not available to the general public. 67 See, for example, Eva Hudlicka, Greg Zacharias, and Joseph Psotka, Increasing Realism of Human Agents  by Modeling Individual Differences: Methodology, Architecture, and Testbed, Blacksburg, Va.: Psychometrix  Associates, Inc., 2000.   68  
Norman Badler, Jan Allbeck, Liwei Zhao, and Meeran Byun, "Representing and Parameterizing Agent Behaviors," IEEE Proceedings of Computer Animation, Geneva, Switzerland, 2002. FIGURE 15.8
Staff, 2016, p. 14;
Root, 2020, p. 7
Unique traits and states  
Bring together closely-related types of information into one centralized model. Show how exogenous factors like culture leadership and equipment affect the individual. Beyond ASDA: Digital Personnel Development
Using the trait-state approach, a biopsychosocial digital twin model and simulation could be used to assess, track, and improve the performance of individual military personnel throughout their careers. Data for specific purposes-such as physical fitness data and posttraumatic stress data-could be logically and scientifically organized in a digital twin. Figure 
15
DoD and other government agencies have likely spent tens of billions of dollars on the development of training simulations and constructive simulations over the past few decades. Despite this massive investment, existing models and simulations in 2021 are not holistic. They are sometimes precise but almost always inaccurate and inauthentic in their representations of human behavior. Are models and simulations really, therefore, informing better policy decisionmaking, or are they trying to suspend disbelief in ways that might be unhelpful?
Our research suggests that defense analysts, modelers, and simulation designers have been on the wrong path since the earliest days of computer M&S. The collective literature demonstrates that the community of scientific practice has clearly identified the problem and, with some disagreement, a general solution to human behavioral modeling and forecasting: Success requires a tack away from the approaches used to develop big sim models and toward the gradual, methodical, and necessarily imperfect authentic modeling and representation of human behavior.
Although the pursuit of biopsychosocial authenticity might be "too hard" given limited resources, senior leader interest, and strategic patience, it is the best approach to answer the research challenges posed by undergoverned spaces, and also by great-power competition, conventional warfighting, total force management, and any other M&S challenge involving humans. The costs involved in pursuing a consortium approach to holistic authenticity are almost certain to be substantially lower than the costs of developing realistic big sims that, by design, eschew authenticity (and, therefore, eschew accuracy and utility). Both the short-term and long-term payoffs should directly support the development of ASDA.
Short-Term Opportunities, Medium-Run Bottlenecks, and Long-Time Barriers to Progress in the Evolution of an Agent-Based Social Science
In a prescient RAND Corporation working paper in the early 1950s, the late John Nash investigated the design of parallel computing machines. 1 This was prior to the widespread adoption of what has come to be known as the von Neumann architecture, which separates data and program storage from arithmetic execution in essentially all modern computers. Already in 1954, Nash saw that there were reasons why parallel execution models were important. He recognized that, because serial processing led to bottlenecks, it would be hard for nonparallel devices to be used to model the wide variety of scientifically interesting phenomena that manifest themselves in the real world. Although Nash was far ahead of his time in these investigations, much as he had pushed the research frontier forward in the theory of games a few years earlier, his ideas fell on somewhat deaf ears. The imperative at that time was to get digital computing off the ground for weather prediction and bomb-building; 2 little concern was given to conforming with the way that nature solves problems in parallel. At that time, the digital computer was thought of as a giant calculator-a tool for solving otherwise intractable equations, a great engine for ridding the sciences of tedious manual calculations, many of which were performed by human "calculators." Indeed, the first general-purpose computer language used to automate the process of generating assembly-level code to run on mainframe computers was FORTRAN (for "FORmula TRANslation"), a relatively simple language for translating formulas from mathematics and science textbooks into working code. Whether solving the equations of fluid mechanics for weather prediction or of quantum mechanics for fissioning the atom, early digital computers and their software were designed for converting classical and well-understood mathematical equations into computer code. Now, almost 70 years later, there are many more ways to use digital computation than merely to solve equations. In this chapter, I discuss the use of Agent-Based Modeling (ABM) in the social sciences, focusing on the methodology for creating such models to faithfully represent actual social phenomena, grounded in empirical data, and not "toy" models used either pedagogically or as abstract thought experiments. Specifically, I critique the widespread use of so-called single-threaded execution for agent-based models and discuss several of its weaknesses. I then go through some of the many ways that multiple execution threads may be used in agent-based models, both for performance gains and for increased verisimilitude. With the growing availability of high-performance computing, the notion of large-scale agent-based models is becoming increasingly relevant. The related idea of full-scale agent-based models is also mentioned in this chapter, and its growing use in the foreseeable future is explored. I also attempt to illuminate bottlenecks that lie on the horizon, so far as they are known, and larger technical challenges that must be surmounted before any vision of "mirror world" or "digital twin" agent-based models is realizable.
Unlike most research, this chapter does not present specific results or describe particular models in any detail. It does discuss a modeling approach, but not in sufficient detail to be pedagogical. Rather, the goal here is to take a broad view of a relatively young and rapidly developing field and attempt to come to terms with its history, its early promise and potential, its recent progress, and its trajectory, both in the short term and over longer time horizons. In doing this, I am keenly aware that some of the claims made may be viewed as rather speculative, far from the world of mathematical theorems, statistical laws, and precise computational code that I normally inhabit. In rendering what are essentially judgments on the state of the art and editorializing about how I see things developing-where the problems, both big and small, are likely to lie and how they might be surmounted-I draw on nearly 30 years of experience with this novel methodology. After leaving graduate school in 1992, I was "present at the creation" in the early heyday of ABM, accomplished as it was on modest hardware and in what are by today's standards rather archaic programming environments. Even then, the fertility of the approach was apparent to many in the social sciences,
3
4
5
Many scientists have noted that, in certain ways, the social sciences are harder (i.e., more difficult) than the natural sciences. 9 Historically, controlled experiments were uncommon in both the behavioral and social sciences-a situation that has changed. 10 Social scientists typically had little high-quality data with which to develop and test theories, which has also changed, with the increasing availability of administrative data in digital form.
In an early statement of the feasibility and usefulness of doing social science with computational agents, 
11
4
16
Scientific research is a kind of exploration, similar to expeditions to remote parts of the earth's surface, such as those undertaken in the past millennium. Both ventures involve life on the frontier-the boundary between what is known and what can only be guessed aboutwith different explorers holding distinct ideas about the most fertile directions in which to proceed. Some believe that the same tools and methods that have worked well to reach the frontier are the best approach for continued progress, while others point out the inevitable weaknesses of existing technologies and perspectives, the reasons why progress has paused at that location on the frontier, and the need for better techniques for assailing the new heights that apparently block further progress and need to be overcome.
This tension between progress through business-as-usual methods and progress through innovative approaches manifests in various ways. Kuhn distinguished normal science from revolutionary science as one such dichotomy. 13 Normal science takes received conceptions of the world and existing tools as given and elaborates on or deepens conventional understanding. Revolutionary science rejects one or more concepts that are foundational to the reigning view and comes to conclusions that are at odds with that view. Heliocentric astronomy and quantum mechanics are poster children for the latter. In one increasingly well-accepted perspective on the philosophy of science, models mediate between theory and data. 14 Social scientists build models using the vocabulary in the leftmost column of Table 
16
One way to interpret the third column is as desiderata for more-realistic kinds of social science models. Some of the entries have a basis in experiment (e.g., other-regarding preferences). 17 Others are simply less stylized than the conventional specification (e.g., social networks in lieu of equal probability of interaction). Although it may be possible to use conventional mathematical or statistical methods to relax one of the standard conceptions holding the others fixed-as is essentially the case in Jackson's work on social networks in economics 
18
The potentially revolutionary power of ABM is that it is now possible to move from the center column to the right using computational agents. For essentially each row of Table 
16
In a famous short story, the Argentine writer Jorge Luis Borges described a land in which people had developed the ability to create maps with so much detail as to be completely faithful representations of the real world. 20 But it turned out that such maps were essentially useless because they were as comprehensive as the real world. The idea of a map is to abstract the most-useful or most-interesting features of the world to build a representation of it that emphasizes the parts that are the most relevant to some particular purpose. So-called mirror worlds, 21 or digital twins, 22 might be considered the modern, high-tech realizations of Borges's fictional maps-high-fidelity representations of actual places, often containing an amount of detail limited only by the time and energy of the creators. (That is, however much specificity is present in a digital twin, it is almost always possible to add more if it is useful to do so.) Although some might find Borges's critique relevant to digital twins, there is one enormous difference between a full-scale map and a high-fidelity computer model. The latter typically represents the evolution of its twin over time, and this process can typically be run much faster than real time, perhaps a thousand or a million times faster, so such models can be extremely useful in developing an understanding of the real world mapped at large scales, whether for policy or other purposes.
Regardless of one's position vis-à-vis large-scale models, it is not possible to build highverisimilitude representations of the real world without detailed data about it. Shorn of actual data, such models might be relevant to social life somewhere but are unlikely to be relevant to humans on earth in the 21st century. The happy situation in which we find ourselves today is that, as a consequence of the information technology revolution-i.e., the widespread (although not universal) digitization of administrative records; the online gathering and archiving of user data by such companies as Google, Facebook, and Twitter; and so on-incredible amounts of data that are relevant to social and behavioral science have become available over the past two decades, with much more likely to become available in the 20 Jorge Luis Borges, Labyrinths, New York: New Directions, 1962. 21  
David Gelernter, Mirror Worlds, or:
Emergent Behavior in Complex Systems," in Franz-Josef Kahlen, Shannon Flumerfelt, and Anabela Alves, eds., Transdisciplinary Perspectives on Complex Systems: New Findings and Approaches, Switzerland: Springer Nature, 2017;
and, in this report, Chapters Fourteen and Fifteen (Ben Connable, "
As a specific example of such data, consider firm sizes, whether measured by employees or by receipts (i.e., revenue). Data on the sizes of U.S. companies from tax filings have been available for a couple of decades. Early work with such data revealed firm sizes to be extremely skewed over the entire range of sizes, from businesses with a single worker up to Walmart, with more than 1 million employees. 27 When such analyses were repeated for other countries, comparable results were obtained, 28 suggesting that a gross empirical regularity existed that was previously only hazily understood among the very largest firms. 
29
Armed with such microdata, researchers can build agent-based models that have close connections to each real-world actor. This is not the norm today, but several examples of this approach exist, and the increasing availability of microdata suggests that more efforts of this type are on the short-term horizon. For example, consider my model, based on firm-level data from administrative sources, of the U.S. private sector. 30 Each year over the past two decades, between 5 million and 6 million U.S. firms have employed a total of between 100 million and 120 million workers annually. Data on all of these firms and employees are available, appropriately anonymized to ensure privacy. The creation of a family of agent-based models to facilitate the study of the formation, operation, and evolution of such firms began at small scale when modest computing power was available, 31 but it has grown such that the entire private sector can now be represented at full scale. 32 Such models involve hundreds of millions of worker agents, who interact directly in production operations within millions of firms, with all agents and firms represented as unique software objects. These models can be estimated from empirical data so as to closely reproduce not just the myriad statistical properties of American businesses but also their dynamics, involving monthly job-to-job transitions by millions of workers and the formation and dissolution of tens of thousands of firms. 33 Other large-scale agent-based models involve traffic models resolved at the level of every vehicle in a city and epidemiological models written in terms of all of the susceptible, infected, and recovered or removed individuals in specific geographical regions. 34  Full-scale agent-based models grounded in microdata need not be large scale, such as when a social process involves a relatively small number of people. Take a fishery, for instance, management of which might involve a very large number of fish but relatively few fishers. 
35
In the real world, people are quasi-autonomous and take actions in accordance with their own goals and objectives, whatever those might be. A variety of norms, institutions, and technologies exist in human societies for partially synchronizing human activities, as when a church announces that it will hold services on a particular day and time or when an office or store posts its business hours. Other types of synchronization are more ephemeral, as when two vehicles meet at an intersection governed by a stop sign and the one arriving earlier gets to proceed first, by mutual agreement with an established social norm. But for large parts of their lives, people act asynchronously, doing what they want to do when and where it occurs to them to do so.
Perhaps somewhat peculiarly, given how real populations behave, almost all agent-based models do not model human behavior as occurring asynchronously. The simple reason is that the computer hardware and, to a lesser extent, software on which such models live are based on the digital computer architecture of von Neumann, in which there is a single central processing unit (CPU) and data are stored in random access memory (RAM); the appropriate data are copied into the CPU when needed and sent back to be stored once they are no longer needed. In essence, the flow of control in such computer architectures is serial in nature, with limited opportunities for parallel execution. Furthermore, because the hardware has these properties, the computer languages that have grown up to use such processors have mostly facilitated the creation of programs that run serially, on a single thread of execution, not in parallel. Given that all of the major ABM programming environments-e.g., Repast, 39 Multi-Agent Simulation of Neighborhoods or Networks (MASON), 40 and NetLogo 41 -generate code that is essentially single-threaded, probably 90-95 percent of agent-based models do violence to the real world of parallel, asynchronous interactions among agents.
However, probably far less than 90 percent of the clock cycles spent on agent-based models are serial, because many of the biggest models execute, at least to some extent, in parallel, using programming paradigms that permit multi-threaded code to run on modern, multicore hardware. Creating such code is typically not easy, because parallel programming is almost as much art as science today. Writing parallel code to solve problems in the natural sciences is often very hard given that such problems might not naturally decompose into neatly separable pieces that can each be deployed on a separate thread or processor. In the social sciences, the situation can be easier because people interact with only a few others at 39  a time, making agent-based models naturally parallelizable. Typically, achieving parallelism with agent-based models is done by simply breaking the population of agents up into pieces that are appropriate to the architecture, letting those agents that are on the same thread interact, and then periodically bringing all the agents back together to be redivided into somewhat different groups to run on new threads. By writing agent-based models in this way, it is often possible to parallelize agent execution to achieve nearly linear speedup with the number of processors or cores or threads available. The trends in microprocessor design, development, and production are shown in Figure 
16
Specifically, although several add-ons to conventional serial programing languages exist to facilitate parallel execution, such as OpenMP and MPI, 42 these extensions are perhaps best suited for parallelizing models that were previously single-threaded. Newer software libraries for parallelization, although more flexible and more tailored to the hardware level, 43 are capable of providing better speedups but do not really change the basic parallel programming paradigm. Furthermore, although moving execution off bottlenecked CPUs and onto graphical processing units (GPUs) has shown promise, 44 doing so poses unique problems for agentbased models, such as working around synchronous updating. Comparing several contemporary parallel languages and frameworks shows a wide variety of performance variations, even on a relatively simple agent-based model. 45  All of these considerations suggest the desirability of making the creation of parallel agent-based models easier, their deployment on extant hardware simpler, and their migration to hardware with greater parallel capabilities-i.e., a larger number of cores in the short term-more straightforward. Some new thinking and novel technologies to accomplish all of this have appeared on the technological horizon with the promise of facilitating the growth of parallel, multi-threaded agent-based models. However, there might be systematic, structural bottlenecks to the widespread adoption of such innovations.
In the next two sections, I speculate on the capabilities of these new ideas, with an eye toward the opportunities that they provide and the limitations that are apparent at this time. I try to look beyond existing programming paradigms and hardware architectures to think Agent Model," working paper, Fairfax, Va.: Department of Computational and Data Sciences, George Mason University, forthcoming. about what a better future might look like for the parallel execution of agent-based models in the short term, when few systematic changes can be made, and in the long term, when perhaps the overall structure of languages and hardware can be evolved to accommodate the needs of future agent-based models that are likely to be very large scale.
There are several distinct motivations for creating agent-based models that are multithreaded, most of which are implicit in the previous section and which I make explicit here. These motivations represent opportunities to create new flavors of agent-based models, the full implications of which are not fully understood today.
The first motivation for parallel execution with threads is (as discussed in the previous section) that the real world of quasi-autonomous individuals is full of parallel activity, and to represent behavior in any other way is potentially problematic. Such concerns were on display in the early days of ABM, when ostensibly significant results about the well-known prisoner's dilemma model from game theory were reported by Nowak and May for interactions on a spatial landscape, executed in parallel but with perfectly synchronous updating. 
46
47
At the opposite extreme of parallel updating is the serial execution model that is the norm in agent-based models today. This is a peculiar representation of human behavior, but one with some basis in human (or modeler) cognition. Often when contemplating our own actions, we take those of others as unchanging or fixed, perhaps through some sense of what constitutes typical human interactions. For example, in the grocery store, we expect the checkout person to scan our selections, say how much we owe, and offer us a receipt. We do not expect that person to throw our groceries in the garbage can or drink the milk that we have put on the conveyor belt. Nor do we expect other customers to be eating food in the aisles or throwing items onto the floor. We imagine that we will successfully purchase food at the grocery store because everyone will engage in conventional behavior and, to a first approximation, their behavior does not interfere with ours. This is also how rules of agent behavior are often constructed. We think about a typical agent (object) and write code (methods) for the actions (messages) that it will take with (send to) other agents and the ways that it may change either its own state (instance variables) or that of the environment, depending on how other agents behave. In thinking about such interactions, whether with other shoppers at the grocery store or with other agents in an agent-based model, we typically take the rest of the world as fixed or in some stationary state. My decision calculus at the store would be quite different if I knew that an arsonist was at my house setting it on fire or that the President was about to declare nuclear war on an adversary. In the same way, the rule writer, coder, or programmer abstracts from the agents that it is not interacting with, taking their behavior as given and not interfering with its own actions. This may or may not be a good way to think about truly parallel social worlds, but it is a passable approach to social cognition. 48  To be completely clear, single-threaded agent-based models represent human behavior by freezing the actions of all agents that are not being updated. In looking at typical visualizations of agent-based models, this successive freezing and unfreezing is not obvious because the execution speed is so fast, causing many agent states to change in a short amount of time, giving the false impression of parallel interactions when only one agent, or perhaps a few, is actually active at once.
Another rationale for parallel execution of agent-based models is much more pragmatic. Many large-scale models are simply not feasible unless they can be sped up by running on parallel hardware. Such models have so many agents or so much cognition per agent, or both, that single-threaded execution would take too much wall time to make them practical. In this case, many processors and multiple cores are manna from heaven, and the question becomes simply how best to leverage the technology. There exist many parallel programming schemes, some alluded to already, and I will not go into detail on them here. Suffice it to say that there is a growing variety of approaches, each with its own set of advantages and disadvantages, and the situation is rapidly evolving. Agent-based modelers gain from the technologies created in other fields for parallelization.
Another reason for multi-threaded agent-based models is close to the previous one but distinct from it. Imagine a model of some social phenomenon that is written at full scale and produces large amounts of output that can be directly compared with the real world, because the model is on the same scale. The physicist Richard Feynman noted that "[t]he same equations must have the same solutions." 49 By analogy, an agent-based model and its real-world twin should generate the same data. In my experience with the 120 million-agent model of firm dynamics, it is simply much easier to estimate parameters of a model, particularly a large model for which each run is computationally expensive, when the model output is directly comparable with the actual target data. Of course, models are always abstractions, to greater or lesser extents, so model output will never exactly coincide with data, but to try to accomplish this with single-threaded models running at small scale adds layers of difficulty.
A fourth incentive for parallel execution, partially related to the first, is that multi-threaded agent-based models may yield different results than their serial counterparts. Conceptually, it is easy to see how this might be the case, because serial agent updating is so stilted and so computationally extreme, with most agents spending most of their lives just waiting around to be active. It is as if each agent gets 15 minutes of fame over its entire life, to execute all of its most important tasks, and is frozen in amber for the rest of the time. For a concrete example of parallel activation yielding different results, consider the so-called zero-intelligence (ZI) trader model of Gode and Sunder, 50 in which financial market agents use a heavy dose of randomness in their own behavior yet produce overall market behavior that is broadly in line with data. 51 Cliff has examined the relative performance of several distinct types of ZI traders, 52  finding patterns of stochastic dominance of certain types over others. What is interesting is that he gets different results in the single-threaded version of his code than in multi-threaded versions. He has advanced a rationale for why this is so, tied up with the details of so-called order books in financial markets, which I will not delve into here. The main point is that different results can be produced through multi-threading-that certain results from singlethreaded agent-based models might be artifacts of the way they are coded. 53   The Fifth Motivation Lastly, a not often articulated rationale for large-scale models realized in threaded environments is simply "why not," given that the current technological zeitgeist involves ever expanding computing resources. Now is a transition time, but, at some point in the not too distant future, it will be the norm to create large models that are executed on parallel machines, and it is high time to get started on this enterprise. A different way to say this is that researchers will have much more-capable computing resources in the future than at present, so it is important to figure out now the basic principles of large-scale agent computing and to elucidate them.
Toward Ubiquitous Large-Scale, Multi-Threaded Agent-Based Models: Bottlenecks Among the many metrics that guide progress in parallel ABM research, certainly one of the most important is the degree of speedup achieved, as when a model that is run on a 16-core processor runs ten times faster in its multi-threaded implementation than when single-threaded. The holy grail is linear speedup-that is, doubling the number of processors, threads, or cores halves the wall time required for model execution. 54 Workstations with shared memory and O(100) cores are now available, with distributed memory clusters with O(1,000) cores often available at universities, while the Bridges system at the Pittsburgh Supercomputing Center has O 
(10,
000)
However, as the search for linear speedups of agent-based models unfolds, there are some dark clouds on the horizon. Various problems plague the conventional threading model in general and in its specific application to agent-based models. I will go through some of these, treating them rather briefly while pointing to the literature for details.
Efficient parallelization often entails being able to partition a problem into more or less independent pieces that each run on their own threads, cores, or processors. When the 53 Researchers do not have a general theory for how and why such differences appear in single-threaded versus multi-threaded execution, an important lacuna in the ABM paradigm. 54 It is sometimes possible to achieve superlinear speedups, as when a model executed on 100 threads runs more than 100 times as fast. Such phenomena are usually due to cache behavior, and my experience is that, although real for toy models, the presence of superlinear model performance is rarely encountered in largescale agent-based models. For example, it is present in certain parallel codes for the ZI model but has not been found in any variants of my 120 million-agent firms model. See 
McCabe et al., forthcoming.
One important rationale for building an agent-based model in the first place is to replace uniform-interaction (mean-field) models, which are easy to work with mathematically but highly unrealistic, with social networks of some kind. This has the beneficial effect on ABM execution that, when it is an agent's turn to act, that agent need look at only a subset (typically small) of the agent population. The character of agent-to-agent interactions-i.e., primarily local and close-knit 55 -means that few data on far-flung agents have to be brought into an agent's decisionmaking calculus. The trick is to put all of the agents who are most closely interacting onto the same thread or process because interthread or interprocess communication is usually costly, as mentioned already. Modeling agent-to-agent interactions is the key to many and perhaps even most agent-based models; thus, getting the most-densely interactive agents onto the same thread usually results in large performance improvements. Stated the opposite way, when agents cannot be partitioned into groups with dense intragroup interactions, because either the actual interactions are not known ahead of time or the interactions are not really clustered but are more homogeneously distributed throughout the entire population, then there often will be very little speedup associated with moving to multi-threaded models.
For a concrete example, consider a spatial agent-based model in which the agents interact only with their physical neighbors. In such a model, tessellating the space into regions and putting each of these on its own thread usually results in significant speedups. 56 However, when agents either move or interact across the region boundaries and, therefore, across threads, it might be the case that significantly less speedup is realized.
A related problem involves load-balancing across threads. In common fork-join parallelism, execution can move beyond the "join" only once the last thread terminates. 57 If the threads are not loaded comparably, then there will be significant idle time as many threads wait for a few to complete their tasks, reducing performance. 58 Load-balancing can be tricky with agent-based models because naïve approaches, such as putting comparable numbers 55 Gabriel E. Kreindler and H. Peyton Young, "Fast Convergence in Evolutionary Equilibrium Selection,"  Games and Economic Behavior, Vol. 80, July 2013.   56 This is the approach taken by the D-MASON extension of the basic MASON ABM framework in Java. 57 For a discussion of the fork-join algorithm as an early and central form of parallel programing, see Linus Nyman and Mikael Laakso, "Notes on the History of Fork and Join," IEEE Annals of the History of Computing, Vol. 38, No. 3, July-September 2016.   58 Load-stealing by idle threads, as is possible with Intel's Threading Building Blocks library, appears to be a good way to deal with this problem. of agents on threads, will lead to poor performance when agent execution time is highly variable. For a real example, consider the production phase that happens each month in my 120 million-agent firms model, 59 during which all firms make output. Because the firms that grow up in this model are highly heterogeneous, with most having just a few worker agents and a few having hundreds of thousands or even 1 million employees, and because the execution time needed to produce output in a firm is proportional to the number of employees the firm has, load-balancing cannot be done simply by partitioning the firm population or the employee population; load-balancing must evaluate both populations. Poor load-balancing in that model can slow execution down by a factor of 10.
When threads are used to take advantage of all processing cores, many of the difficulties associated with writing good parallel ABM code have to do with the fact that threads running on CPUs can be interrupted at any time to perform some other, perhaps more machineor system-critical, task. The difficulties posed by existing threading technologies have been extensively discussed in the computer engineering literature. 60 Interrupts make it impossible for modern CPUs to guarantee when any specific thread will execute, and this makes the fork-join model an imperfect paradigm for ABM parallelism. In the next section, I will discuss what alternative technologies might look like. Here, I conclude with observations regarding the constraints imposed by threading technologies on agent-based models, and the bottlenecks that need to be dealt with to achieve higher levels of speedup.
Consider an agent-based model in which the number of agents is much larger than the number of cores so that each thread will manage the execution of many agents, as is common today: e.g., 120 million agents on O(100) cores. Parallelism of this type can often lead to good speedups but does not solve the problem of unknown artifacts being impressed into what are essentially O(100) single-threaded execution streams. That is, there are still more than 1 million agents executing on each core in single-threaded fashion. On top of this, to avoid generating further computational artifacts, it is important to regularly remix the agents onto different threads so that micro-correlations, such as agent i always moving before agent j and so on, do not occur. 61 Furthermore, it seems that, to write efficient parallel code, it is necessary to be able to say, at least by run-time, which agents or agent groups are going to be the biggest users of clock cycles. If it is not possible to do so, then it is practically impossible to do any kind of even approximate load-balancing, thus jeopardizing the goals of parallel agent-based models. 59 Axtell, 2016a; Axtell, 2018. 60 For example, see Edward A. Lee, The Problem with Threads, Berkeley, Calif.: Electrical Engineering and Computer Sciences, University of California at Berkeley, January 10, 2006. 61 Robert L. Axtell, "Effects of Interaction Topology and Activation Regime in Several Multi-Agent Systems," in Scott 
Moss and
Paul Davidsson, eds., Multi-Agent-Based Simulation, Vol. 1979, Heidelberg, Germany: Springer Verlag, 2001a;
and Kenneth W. Comer, Who Goes First? An
More cores, faster memory, bigger caches, better parallel programming paradigms-all of these technologies are visible from the research frontier outpost that researchers now occupy. All will make agent-based models more capable by permitting larger models that run faster and are easier to program and debug. Some approaches to "debottlenecking" are also on the horizon, and there can be little doubt that many of the problems described in the previous section are likely to find either solutions or satisfactory workarounds as ABM tools and codebases mature. Some of the challenges will be harder to resolve than others and might require significant innovations, but progress is both inevitable and, from someone who has struggled trying to bend many extant computing technologies toward the needs of ABM over the years, very welcome.
There will come a day when each agent runs on its own core, but that day is a long way off, at least for large models. This would not be a panacea anyway, given that the raison d'être for ABM in the first place is to model interactions; perhaps two agents per core is the lowest agent density worth pursuing, at least as long as interthread communication is slower-and more hazardous from a data race perspective-than intrathread operations.
From the highest-level perspective, what is needed on the hardware side are noninterruptible processors, or at least cores, so that execution streams (e.g., threads) would have guaranteed run-time. This would solve many problems of thread synchronization and would also make life easier in other ways. First, fork-join processes would have repeatable, reliable behavior. Second, debugging would be much easier because execution would be (more) deterministic. Third, load-balancing could be accomplished empirically, as when running an agent-based model many times shows which processes, routines, or agent groups need more time than others. Imagine a world of many core processors in which a few cores are dedicated to the operating system but most are available for the models and cannot be interrupted by the operating system. Specifically, some kind of CPU-GPU hybrid, in which the CPU precomputes agent partitions while the model runs on the GPU, might work for agent-based models. 62 Having 10,000 to 100,000 lightweight cores operating asynchronously could go far toward having a "supercomputer on a board" for agent-based models.
Less speculatively, innovations in other parts of the machine would also serve the ABM community well. In certain respects, an agent-based model is a giant database of agent state information, maintained in RAM and updated according to the agent objects' rules of behavior. Advances in database technologies, involving persistence and various kinds of errorchecking, might help provide guarantees of the repeatability of ABM execution. Improvements in display technologies would also be of importance for progress with agent-based models. Today's biggest 6K displays have something short of 50 million pixels. Visualizing the state of a model with 100 million agents is not possible until resolutions go up by a factor of 5 or 10 and will probably not be useful until a factor of 20 or 50 times larger is reached. Finally, because agent-based models are large databases, fast storage technologies for huge state spaces are also of interest. Recording the full evolution of 100 million agents, each 1 kilobyte, running for 10,000 periods would require at least a petabyte of storage per run, an exabyte for many runs, and a zettabyte for many model instances in a model family, assuming that the full evolution of agent states is useful to record.
Advances in Software: Agent Languages, Operating Systems, and Software Engineering
Although it might be natural to think of operating systems as more foundational than programming languages, standardized languages came first, and many early mainframe installations had highly localized, essentially bespoke operating systems. In contemplating the kinds of changes in software ecosystems that would accommodate large-scale, parallel-executed agent-based models, it is not clear whether agent-oriented operating systems or specialized programming languages would come first or which would provide the greater performance boost. Certainly, if new kinds of uninterruptible hardware were to become available, such as is discussed in the previous subsection, new operating systems would be needed. However, it is also unclear whether these would or should be agent-oriented in any meaningful way.
In the past decade, there have been various proposals for developing specialized operating systems based on agents. 63 This approach is related to self-aware computing systems. 64 Agentoriented software engineering has been around for some time. 65  Ideas about agent-oriented programming languages have been discussed in the artificial intelligence community for long enough to have reached a certain level of maturity. 66 Origi-nally envisioned as a programming paradigm analogous to object-oriented programming (OOP), agent-oriented programming has not evolved or matured as expected, despite a proliferation of tools. 67 Analogous, domain-specific programming paradigms, such as marketoriented programming, 68 remain nascent.
In the short term, it might be the creation of specialized, disciplinary-specific software libraries that will make the biggest impact on the creation of ABM codes. Typically, when a researcher begins to build a new agent-based model, they harvest code from previous, similar models, often their own. Given that there are several domains that are heavily worked by agentbased models, such as traffic, finance, ecology, and epidemiology, the creation of code libraries in these areas would greatly facilitate the more rapid creation of such models. There is currently little incentive for researchers to condense their code into such libraries, and little support from funding agencies to create software tools, so it is unclear how such efforts might successfully unfold, although there seems to be more momentum in certain areas than in others. 69  Closely related to the creation of software libraries is the idea of community software, which is used in the climate modeling community. 70 There, the husbanding of code resources-i.e., programs and data-at the Geophysical Fluid Dynamics Laboratory and the National Center for Atmospheric Research has led to the progressive evolution of both morecomprehensive and more-accurate models. A similar situation exists at the National Weather Service. No comparable efforts are underway in the social sciences, although the need for such efforts is clear. 71  Toward Automated Synthesis of Agent-Based Models One way in which agent-based models are still something of an art form requiring domainspecific knowledge is in the specification of agent rules of behavior. In the context of a microto-macro modeling paradigm, such rules play cornerstone roles in most agent-based models. Sometimes there are microdata that provide detailed guidance for what such rules should look like, perhaps data gleaned from experiments with human subjects, 72 but aggregate data are more commonly available, and, in these cases, the agent rules must be inferred according to which specifications are sufficient to "hit" the target data. In calibrating or estimating models in this way, ABM is quite similar to standard statistics and econometrics, in which parameters are inferred from data. Conventional techniques, such as estimation by simulation, 73 are readily applicable to agent-based models. However, because agent-based models can be large and computationally expensive, it is generally costly in terms of time and effort to obtain well-fitting parameters, and much less is known about the identifiability of model parameters in agent-based models than in conventional mathematical models.
In light of these difficulties, ideas have been developed of late about how to use machine learning and other automated techniques to create and calibrate agent-based models, thus leading to progress in several areas. Specifically, for agent-based models in finance, an area in which copious amounts of data are available, machine learning has been used to estimate nonlinear relationships between inputs and outputs in some well-known models. 74 Relatedly, deep learning has been used to develop a better understanding of agent-based models and has also been used within agent-based models. 75 A recent idea with many interesting implications (e.g., the amount of volatility in financial markets) involves specifying the behavioral repertoire for agents as machine learning; 76 that is, individual agents use machine learning to figure out how to behave.
More generally, evolutionary programming and related heuristic techniques have been used to solve the inverse problem of determining agent behaviors from data. 77 Among the issues that arise with such approaches-overfitting, the value of minimal models-the general question of when data are sufficient to determine model parameters-model identification, again-is crucial. In general, there will be many configurations of microsystems that are compatible with macrodata.
For example, a city has certain gross characteristics independent of the arrangement of the dry cleaning and convenience stores on the corner of State and Main Streets. Under the assumption that the main goal of most agent-based models is not to explain or predict microconfigurations, it is reasonable to expect micro-founded models to be underdetermined in the sense that the data, no matter how detailed, will not usually have enough context and historical depth to permit the digital world to evolve in exactly the same way as the real world. There are combinatorially enormous numbers of microstates that will generally be consistent with the data to be explained or predicted, as in statistical mechanics, in which the specific locations and velocities of O(Avogadro's number) particles are irrelevant if only the temperature of the room is the key statistic. Therefore, it is reasonable to expect that a host of distinct inverse methods for inferring rule systems for agent-based models will be broadly equivalent, not in the sense of yielding the same rules but in the ability to produce the same kinds of output. Questions about how to define and make use of equivalence classes of such rule systems or even the inverse methods themselves are important frontier issues on which additional research is needed, a topic that probably should be considered as high priority within the ABM research community.
Putting the Pieces Together: The Promise of Large-Scale Agent-Based Models Formulated from Big Data and Executing in Parallel
It is my hope to have communicated some of the excitement around ABM, in terms of both what can be achieved today and the developments on the horizon. The field has progressed immensely from its artificial life and OOP beginnings, running on the first generation of microcomputer hardware, with little or no data and a single thread of execution. If the coming decades experience comparable evolutionary developments, then by 2030 whole new classes of social science models should be possible, to say nothing of 2040 or 2050. 77 Chathika Gunaratne, Evolutionary Model Discovery: Automating Causal Inference for Generative Models of Human Social Behavior, dissertation, Orlando, Fla.: University of Central Florida, 2019; and Tuong Manh Vu, Charlotte Probst, Joshua M. Epstein, Alan Brennan, Mark Strong, and Robin C. Purshouse, "Toward Inverse Generative Social Science Using Multi-Objective Genetic Programming," presented at the Genetic and Evolutionary Computation Conference (GECCO 
'19)
July 13-17, 2019.
Since the mid-19th century, the idea of representing all individuals who are active in social processes in models has been a cornerstone of economics and finance, 79 spilling out into certain domains of other fields as well. However, from a practical point of view, this methodological individualist perspective was stillborn in essentially every area investigated because of the inability to render models at full scale with the real world. This was especially so given that the techniques that worked so well for physics (e.g., statistical mechanics) did not readily translate to economics because of heterogeneity, network effects, and so on. 80  I think that it is not widely understood that only now, some 150 years beyond the birth of methodological individualism, are social scientists in a position to fully realize such models, in which millions or even billions of agents take parallel, asynchronous actions using data that they glean from the world in pursuit of their own self-interests. Interestingly, the ABM method that is the only real way to realize such models today also trades in concepts of methodological holism and pluralism, recognizing the important role of emergent phenomena within systems of interacting agents. 81 The newfound capabilities-the combination of ABM as a methodology, new computing opportunities, and new data-will manifest themselves in many ways over the next decades, so that by 2050 entirely new and more comprehensive agent-based models will have taken their places in scientific and policy circles; this will supplant certain mathematical and statistical models that are today based on limited, aggregate, and infrequently updated (e.g., quarterly) data that abstract from the vast heterogeneity that is present in the real world. 78  Speculations on the kinds of new agent-based models that will appear in economics in the short to medium terms have been made elsewhere; 82 these include macroeconomic models written in terms of hundreds of millions of interacting agents, international trade models grounded in data on all individual firms that engage in export-import behavior, and economics models that are tied much more closely to the people and conditions in developing countries as opposed to regression models that are based on misspecified relationships derived from the properties of already developed nations.
In this chapter, I extend such speculations beyond economics proper to questions of political economy broadly construed, the defense and advancement of U.S. national security, and what can be done to better understand the latter through ABM.
One class of agent-based models that seems quite clearly on the horizon is policy-relevant models that are based on quasi-comprehensive administrative data and that can be used to explore alternative policy decisions. Such a model on a small scale was discussed-the POSEIDON model for fishery management. A larger model is the full-scale agent-based model of the U.S. housing market bubble circa the mid-1990s to the late 2000s, its bursting, and subsequent economic consequences, specifically as they played out in the Washington, D.C., area. 83 There are several efforts underway today to create agent-based models that are suitable for use as macroeconomic policymaking tools, 84 analogous to the role played by so-called dynamic stochastic general equilibrium models in central banks. 85 Given the accelerating developments in this field in the past decade, the coming one is sure to see systematic progress toward large-scale macroeconomic agent-based models that are grounded in microdata.
Agent-based models that are relevant to the world's great common-pool resource problems have begun to appear. 86 These will become larger-approaching a global scale-as data increasingly roll in. Agent-based models of governance, 87 international migration, 88 and other multicountry phenomena will also grow in scale and scope with better data, perhaps with data acquired from remote sensing. 89  In international relations, the role for models will grow to the extent that they outperform traditional adversary forecasting approaches, grounded as they are mainly in leader psychology and statistical assessments of resource availability. 90 Compare international relations to weather forecasting. Data on weather in the United States have been systematically gathered for a century, but it was not until approximately 1980 that computational weather models could outperform people who were experts on the historical data. 91 As better data on international political actors combine computationally with richer behavioral models, those countries that can synthesize agent-based models that predict the actions of neighboring countries will thrive in the international system. From this perspective, it is not a question of whether such models will appear but rather when the research establishment in some country will invest sufficient resources to make such a paradigm viable, after which other countries will play catch-up.
Thus, large-scale, high-fidelity agent-based models of real-world social phenomena are coming soon, rendered for both scientific (positive) and policy (normative) purposes. Such agent-based models will be in a perpetual state of evolution given the real-time flow of highquality, high-frequency data. The results will be available in visual form and will perhaps even be sent to decisionmakers' phones, iPads, and other screens. In some domains in which ABM is being used that are partially relevant to the social sciences, such as forest fire manage-ment and related areas of disaster mitigation and amelioration, models with these properties are already on display. Extension of such developments across all of the social sciences will lead to computationally enabled policy and should result in better management of the social and natural worlds, with large rewards for the country that can accomplish this first.
ABM has been around from the late 1980s to the early 1990s. Beginning with small-scale, abstract, and otherwise toy models, the method has now grown through advances in both hardware and software. As a result, ABM is now capable of rendering social phenomena at full scale-every person, every institution represented-and in deeply empirical ways, making systematic use of both microdata, when available, and aggregate data, either as input specifications or as target outputs. Overall, for those who look closely at the state of the technology, ABM has the potential to revolutionize the social sciences by facilitating the relaxation of unrealistic behavioral and structural specifications in conventional models. With continued advances in computing power, ever larger models based on more and more data, executing on increasingly parallel machines in less and less wall time, will progressively become a reality. In almost any business-as-usual scenario, such advances will occur and be game-changing in many fields.
Yet there are also opportunities for more-rapid, even more-guided, evolution of the ABM paradigm through specific hardware and software innovations, as I have sketched out. Different hardware-essentially uninterruptible processor cores that provide execution guarantees-could dramatically reduce run-times for large-scale agent-based models. Furthermore, the technical development of new parallel programming languages, frameworks, and software libraries would do much to advance ABM software development in the short term, while coupled hardware and software developments-perhaps in the guise of specialized boards or nodes for large-scale agent computation-could radically accelerate the entire paradigm over longer timescales. U.S. research institutions relevant to the social sciences are not well positioned to support such efforts, such as the creation of community agent-based models relevant to specific disciplines. It will probably take a farsighted administrator or an act of Congress to create the nucleus for such institutional support as is needed for the rapid expansion of perhaps the most revolutionary social science methodology since the appearance, in the waning days of World War II, of von Neumann's and Morgenstern's research on game theory, 92 work which subsequently served as the basis for much Cold War strategic theorizing.  
Association, Vol. 55, No. 292, December 1960;
and Martin Shubik, ed., Game Theory and Related Approaches to Social Behavior, New York: John Wiley & Sons, Inc., 1964.
The United States has been a leader in the application of ABM and has a large head start on other nations. However, many leading scientists in those countries also see the value in this new way of building models and are moving quickly to harness it for their own purposes-to better manage their own economies and strategic assets, but also, inevitably, to provide security from adversarial agents while fostering partnerships to shape institutions to their benefit. In the coming world in which each nation builds models of every other nation, which in turn model each of the nations modeling them, how the ecosystem of models interacts with policymakers' decisions is almost certainly impossible to forecast. Surely it will be a complex system with many surprising, emergent properties, and it will likely surprise us unless we engage with it fully.
Characterizing the complexity of strategic interaction remains an important challenge in the social sciences. One method for doing this is to apply the theory of computational-complexity and complexity classes to solving game-theoretic problems of strategic interaction. Although the mapping between strategic scenarios and computational-complexity classes is useful, it does not fully capture the dimensionality of strategic interaction that results from the presence or potential of multiple equilibria, communication, chaotic learning dynamics, and behavioral insights that affect how actors understand, analyze, and participate in strategic environments or games. In the pursuit of generality pertaining to computational needs, computational-complexity analysis may obfuscate the true difficulty in participating in or analyzing a strategic interaction. As a result, understanding the underlying structure of a particular game is crucial for refining the notion of complexity in strategic interaction.
This chapter highlights how these features of strategic interaction weigh heavily in realworld national security challenges in which actors' interests are neither purely aligned nor purely opposed, as is the case in undergoverned spaces, gray-zone competition, societal warfare, and global financial and public health crises.
The following sections examine the strengths and limitations of using computational complexity to quantify the complexity of strategic behavior of games and the actors within them. The first section discusses the core problem of strategic interaction in games in which actors are involved in interdependent decisionmaking processes. It then reviews the use of computational-complexity classes to analyze games, before turning to discuss factors that computational complexity obscures. Specifically, it considers such factors as multiple equilibria, communication, learning dynamics, having more than two players, and nonrational behavior. The next section shows that computational complexity is a useful notion of complexity and that there has been productive work to make the analysis more sophisticated. But it also shows that there are additional features of strategic environments that alter the complexity of analyzing and participating in such games, and these features affect the difficulty of extracting insight from a model of strategic interaction. Finally, the chapter offers some concluding thoughts and an appendix that gives a short, nontechnical introduction of game theory to support this premise.
Strategic interaction is a central tenet of the social systems and relationships between agents, whether they are individuals or groups. On a micro scale, couples with different preferences who enjoy each other's company negotiate meals, entertainment, and even parenting styles. On a macro scale, states develop trade agreements, compete in arms races, form political alliances, and conduct espionage. In all cases, the decisions and actions of one party affect the other parties; thus, all parties are anticipating and reacting to one another. This is the hallmark of a strategic interaction and interdependence.
Because strategic interaction is so pervasive, social scientists have developed tools and methods for understanding and predicting the outcomes of strategically interacting decisionmakers. Although not the only method, noncooperative game theory has emerged as a leading tool for analyzing such scenarios and anticipating potential outcomes based on the structure of the environment. 1 Stripping down problems of interdependent action in this way allows analysts to make precise statements about the structure of interactions between actors, such as characteristics of solutions, that describe and anticipate real-world events in similar situations. In its most general formulation, game theory is a formalism for analyzing decisionmakers who interact dynamically in an uncertain environment with imperfect information in pursuit of their (possibly uncertain) objectives. By combining mathematical concepts from probability theory, optimization, stochastic processes and several other disciplines, game theory provides tools to model and analyze a variety of strategic scenarios.
Before exploring why computational-complexity classes fail to capture the most-salient contributors to the complexity of strategic interaction as examined by game theorists, it is useful to have a core set of definitions and concepts. These definitions and concepts characterize features of strategic scenarios, players within them, and the types of solutions that can be found.
As indicated in the related chapter on gaming, the definition of a game is contested. 2 Game theorists also have a definition, which is, fortunately, more precise, although not necessarily 1 Steven Tadelis, Game Theory: An Introduction, Princeton, N.J.: Princeton University Press, 2013. aligned with the colloquial use of the term. 3 For game theorists, a game is formally defined as a situation in which multiple participants (i.e., players) interact and affect each other's outcomes within a set structure. 4 The presence of two or more active players, each affecting the others, differentiates game theory from decision theory, in which a single decisionmaker chooses among options whose outcomes are unaffected by other actors. 5 Although games are commonly referred to as strategic, the terms interdependent and contingent are also commonly used. 6  The outcomes of players' actions are referred to as payoffs, which can be distributed among the players in a variety of ways. In strictly competitive or zero-sum games, the payoffs are opposed and often symmetric if there are only two players, while asymmetries may exist when there are more. 7 Games in which players' payoffs reward cooperation (i.e., one's gains are not automatically the other's losses) are referred to as general-sum or non-zero-sum games and often require different approaches to solve because analysts can no longer assume that maximizing their own payoffs minimizes the payoffs of their opponents.
The concept of equilibrium for a game usually refers to its Nash equilibrium. A Nash equilibrium exists for a set of strategies in which each player cannot unilaterally improve their payoff by changing their strategy. 8 Games may have multiple equilibria, meaning that, absent any ability to coordinate their actions, players may get trapped in one equilibrium that may be less desirable to one or multiple players given alternative equilibria. 9
Given a particular specification of a game, one relevant question is, "How complex is the game?" With only colloquial notions of complexity, there are several reasons why such a question is relevant. First, the complexity of a game might indicate how many resources and how much effort decisionmakers might have to expend to participate in it. The more complex the game, the more resources participants must expend to determine their optimal strategies. This can foreshadow barriers to entry into certain domains for which individuals do not have the resources to fully understand or compete in the strategic environment. Such problems may include decisions by firms to enter into new markets or by states to enter into an arms race or alliance-building to balance against rivals. 
10
11
Third, as strategic decisionmaking shifts from human decisionmakers to automated algorithmic decisionmakers, complexity will partly determine how well those algorithms perform, the predictability of their decisions, and their vulnerability to adversarial attacks. 
12
Given the relative importance of complexity, it is no surprise that quantitative measures of strategic complexity have emerged. Nowhere has this been more apparent than in the notion of computational-complexity theory. 
13
At its core, the theory of computational complexity tries to quantify how difficult it is to solve a problem with a given set of characteristics. Although the rigorous definition of difficult requires a full treatment of Turing machines (an abstract mathematical model of a computer), an informal treatment is sufficient to understand computational complexity and its application to games.
One of the key questions in computational-complexity theory is whether questions whose potential answers can be easily verified can also be easily solved. As a canonical example, consider the set of numbers S = { -12, -8, -1, 2, 5, 4} and the subset-sum question, "Does there exist a subset of S that sums to 0?" Of course, if someone proposes an answer-say, A 1 = {2,5,-8}-it is easy to verify that 2 + 5 -8 does not equal 0. However, it is not as straightforward to find a way to determine whether there is a subset of numbers that sums to 0. One algorithm for doing so is simply to try all possible subsets, which, for the set S consisting of eight elements, would involve 64 such subsets. For a set of 20 numbers, a search of all possible subsets would involve more than 1,000,000 different permutations. Thus, an algorithm seeking to search across all subsets of a set of numbers scales poorly with the size of the set. 14 This notion of verifiability versus solvability is the central concept behind the classic "P vs. NP" problem at the core of computational-complexity class analysis. 15   
At this point, it is unclear how these abstract concepts of computational complexity apply to games. To draw the connection, consider the following Nash existence question: "Given a game with a set of players, strategies, and payoffs, does there exist a Nash equilibrium?" This question is parallel to the question in the previous section. The general format is, "Given an object, does that object have a certain property?" In the subset-sum problem, the object is the set S, and the property is "has a subset that sums to 0." In the Nash existence question, the object is a game, and the property is "has a Nash equilibrium." Given the parallel nature of these questions, one might be tempted to ask, "How hard is it to answer the Nash existence question as the number of players and strategies grows?" At a first pass, the answer to whether a game possesses a Nash equilibrium is trivial because Nash's famous theorem proved that every finite game has at least one Nash equilibrium. 16 However, a slightly different question is also of interest: "Given a game, what is the Nash equilibrium?" This "find-the-Nash" question has demanded much attention because it is a special type of problem for which a solution is known to exist but for which it is not clear how difficult the solution is to find. It turns out that the find-the-Nash problem has similar properties as the subset-sum problem. Specifically, it is relatively easy to verify whether a particular set of strategies forms a Nash equilibrium, but the difficulty of finding an equilibrium strategy (with any known algorithm) increases exponentially with the number of strategies. 17  For this reason, finding a Nash equilibrium is known to be computationally difficult. 18  This foundational result establishes that finding a Nash equilibrium is inherently a difficult problem and has been extended to prove other game-theoretic insights. For example, the following questions are also computationally hard: 19   • Does a second Nash equilibrium exist? • What is the socially optimal Nash equilibrium? • Does there exist a Nash equilibrium where one player's payoff is at least x?
These results also have implications for the complexity of dynamic games and games of imperfect information. Broadly speaking, introducing dynamics or imperfect information to a static game of perfect information tends to increase the number of strategies for each player. This is because a strategy specifies what a player would do at each point in time and for any amount of imperfect information. The computational-complexity theory has established that the difficulty of computing an equilibrium increases exponentially given the number of strategies, so adding dynamic components with imperfect information to a static game does increase the game's computational complexity.
One overarching feature of the computational-complexity insights is that they make minimal assumptions about the underlying game. This generality can obfuscate the complexity of finding a Nash equilibrium for a particular game with known structure. For example, if the game is a two-player zero-sum game, it is not computationally difficult to find a Nash equilibrium, because the game can be solved with a linear program. 20 In this case, the characterization in terms of computational complexity overstates the difficulty of finding an equilibrium. Then again, even though it is not computationally hard to find an equilibrium, the problem still might be intractable given the game's underlying structure. For example, the game of Go is a two-player zero-sum game, a class of game for which finding a Nash equilibrium is not computationally hard. However, because the number of strategies is, famously, greater than the estimated number of atoms in the universe, it is believed to be impossible to find an equilibrium of the game-a provable theorem that demonstrates that, if both players make optimal decisions during gameplay, the outcome of the game is knowable from the first move, with the first or second mover winning or the game ending in a draw. 21 Although the artificial intelligence program AlphaGo claimed victories over Lee Sodol, the highestranked Go player in the world, it subsequently lost to future generations of its own program, AlphaZero, demonstrating that potentially novel and even-higher-performing strategies remain to be discovered and that a gap exists between superhuman performance and optimal play. 22 Thus, in the case of a relatively simple game with a large state space, computationalcomplexity results understate the difficulty of finding an equilibrium.
All told, complexity class analysis provides a general framework for analyzing the difficulty of computing a Nash equilibrium in a game. Of course, if players try to play equilibrium strategies, then such analysis quantifies how difficult it is for players to find an optimal strategy. However, in the pursuit of generality, computational-complexity analysis may obfuscate the true difficulty in participating in or analyzing a strategic interaction. As a result, understanding the underlying structure of a particular game is crucial for refining the notion of complexity in strategic interaction.
For a specific example, consider a competitive resource allocation problem in which two military organizations must allocate disparate resources (e.g., intelligence, combat, logistics, and command and control assets) across multiple distinct battlefields, each organization trying to maximize its chances of defeating its rival on as many battlefields as possible. This game is known as the multiresource Blotto game, and, given its zero-sum nature, finding a solution to this game is technically less complex than finding an equilibrium in a game in which the players' goals are not perfectly opposed. 23 However, because the number of strategies grows exponentially with the number of different resources, solving the game with more than five or six different resource types quickly becomes intractable. This illustrates how even games that are not technically complex from a structural perspective can nevertheless become difficult to solve.
One of the most prominent characteristics of a game is whether it is competitive or cooperative in nature. The most competitive games are known as zero sum and encompass many parlor games, resource allocation games, and attacker-defender games. 24 Cooperative games are the opposite; all players share the same payoff; thus, what is good for one player is good for the other players. Common examples of cooperative games are coordinating aircraft to fight a wildfire, designing optimal organizational communication protocols, and organizing social movements. 25 Of course, there are games that are neither purely competitive nor purely cooperative, such as auctions, political negotiations and lobbying, firm price setting, deterrence, persuasion, and personnel decisions. 26 These cases of mixed cooperation and competition add complexity to understanding and participating in a game.
Communication-sometimes referred to as signaling-plays a key role in many strategic scenarios. A potential employee communicating their value to a company, a lawyer choosing evidence to reveal to a jury, and a world power promising retaliation for cyberattacks provide examples in which communication is central to strategic interaction. However, communication is not always verifiable, is sometimes noisy, and can sometimes be misinterpreted. For these reasons, including communication as part of an agent's strategy can add to the complexity of a game.
However, in two-player zero-sum games, it is always optimal to ignore any communication from the other player. 27 The reason is that, if the players' interests are diametrically opposed, neither player has an incentive to convey valuable information to the other. Therefore, any communication must be meaningless and can be ignored. This means that twoplayer zero-sum games are not riddled with the complexity that arises from strategic communication. However, in common-interest games, truthful communication can only be helpful. Specifically, if there is infinite communication ability, the optimal communication protocol for a player is simple: Truthfully reveal all information. 28 The intuition is that, if all players have the exact same goal, no player has an incentive to deceive any other player and, thus, all communication is truthful and the receiver of any piece of communication can be sure that the information is true.
Importantly, in games in which interests are neither completely aligned nor completely opposed, communication can become complex. In such scenarios, some players might have an incentive to reveal partial information. Furthermore, there might be an incentive for players to tell the truth only if certain other conditions are met. For example, two firms under price competition might be willing to truthfully share market research information only if they are sufficiently forward-looking. Otherwise, there is an incentive for one firm to deceive the other, and communication breaks down. 29 Other examples of complex communication in games are job market signaling, product quality signaling, and deterrence. 30 The analytical problems posed by communication between players are exacerbated if the differential costs and modes of signaling are considered, such as those costs that are paid immediately, through actions (e.g., the movement of military forces into a theater), versus those that might be paid 27 Joseph Farrell, "Talk Is Cheap," American Economic Review, Vol. 85, No. 2, May 1995. 28 Jasmina Arifovic and B. Curtis Eaton, "The Evolution of Type Communication in a Sender/Receiver Game of Common Interest with Cheap Talk," Journal of Economic Dynamics and Control, Vol. 22, Nos. 8-9, 1998.   29 Justin Grana, James Bono, and David Wolpert, "Reasoning About 'When' Instead of 'What': Collusive Equilibria with Stochastic Timing in Repeated Oligopoly," B.E. Journal of Theoretical Economics, Vol. 20, No. 1, January 2020. 30 Maarten C. W. Janssen and Santanu Roy, "Signaling Quality Through Prices in an Oligopoly," Games and Economic 
Behavior, Vol. 68, No. 1, January 2010;
Spence, 1973;
and Jonathan William Welburn, Justin Grana, and
31
From a security perspective, consider interactions in an undergoverned space. Suppose one party is trying to restore order but, to do so, must gather information from several informants, each with a unique preference on how order should be restored. From the perspective of the informants, each does not have the incentive to fully tell the truth, because each is trying to influence the future order. Furthermore, each informant will form beliefs about what other informants are saying and might try to strategically align or oppose their message with the messages of other informants. The party trying to restore order must weigh each piece of information knowing that each informant is trying to influence its actions. It is often not optimal for the order-restoring party to ignore all information, because some of it is true, but it must carefully reason about how the informants might be incentivized to lie and ultimately make decisions based on unreliable information. A similar logic plays out when mediators seek to prevent the occurrence of a conflict or its further escalation and conflicting parties seek to assess not only the resolve of one another but also the bias and credibility of the mediator. 
32
Another feature of strategic interaction that can add to the complexity of predicting and understanding human behavior is the existence of multiple equilibria. 
33
The simplest example of multiple equilibria is in coordination games. Consider two players who made plans to meet at a restaurant but forgot where to meet and cannot communicate. However, they know that they were going to meet at either the town's pizza parlor or its steakhouse. The players are better off if they choose the same restaurant. In this case, there are multiple equilibria: both players choosing to go to the pizza parlor, and both players choosing to go to the steakhouse. 34 Although both outcomes are plausible, it would not be clear to an external observer how to choose between the two outcomes to predict where the diners will meet. Examples from a security perspective are coordinated hardening of critical infrastructure, coordinated responses among allies to terrorist aggression, and coordinated responses to the release of a biological agent. 35 In all of these cases, there might be many possible solutions, but it is most important that several parties coordinate on a plan; which plan they choose is less important.
Again, the complexities of multiple equilibria are most pronounced in games in which players' interests are neither perfectly aligned nor perfectly opposed. For example, in twoplayer zero-sum games, all equilibria give the same payoff to the players; the equilibria are payoff equivalent. Therefore, choosing among equilibria becomes less important because the welfare of each player is constant among equilibria. This is especially poignant in zero-sum games because, if a player wins in one equilibrium, that player will win in all equilibria. Then again, when players' interests are perfectly aligned, there always exists an equilibrium in which no collective action of any number of players can make the team better off. 36  Unfortunately, neither of these properties holds for general-sum games. That is, all equilibria do not necessarily yield the same payoffs, and there does not always exist a Pareto-Optimal equilibrium. As a result, it is unclear how to predict the outcome of a strategic interaction in general-sum games. As a concrete illustration, consider a slight tweak to the restaurant coordination game. The players still want to go to the same restaurant, but now assume that player 1 slightly prefers pizza over steak while player 2 slightly prefers steak over pizza. Would player 1 go to the pizza parlor because they prefer pizza, or would player 1 go to the steakhouse because they know player 2 prefers steak? How would player 2 act with a similar line of reasoning? Without further structure on the strategic interaction, it is not clear how to choose among the possible reasonable outcomes.
Although many questions about strategic interaction are concerned with predicting ultimate outcomes, it is sometimes relevant to understand how players arrive at certain outcomes and how they adjust their decisions based on past observations. 37 How players learn and possibly converge to a stable outcome is often as important as the outcome itself. A common argument is that, if a relatively simple learning procedure does converge to an equilibrium outcome, then there is evidence that human learners would converge to the same outcome. 38  Once again, the complexities that arise when analyzing learning dynamics are most pronounced in general-sum games. In two-player zero-sum games, a simple algorithm known as fictious play yields a result equivalent to the Nash equilibrium strategies. 39 This algorithm also applies to games of common interest. Furthermore, an algorithm known as iterated best response, in which the player simply best responds to their opponent's previous action, is known to converge in team games. However, these theoretical convergence guarantees do not extend to general-sum games. It is easy to see that fictious play leads to chaotic learning dynamics in a slightly modified game of rock paper scissors. 40 Nuances in learning dynamics suggest that analyzing how players might learn and adapt their strategies is much more complex in general-sum games than in two-player zero-sum games or team games.
As the previous subsection noted, common issues that make games complex occur when players are neither purely competitive nor purely cooperative. However, many of the reductions in complexity that occur when two players have perfectly opposed objectives cease to be viable when a game contains more than two active players. For example, suppose there are three opposing states that want to conquer the territory of the others, but the cost of going to war with any rival is not worth the value of the territorial gains that victory would bring. However, if two states declare war against the third, the solo country immediately forfeits its land because it knows that it cannot win against the combined forces of the other two, allowing the victors to divide the surrendered territory evenly without paying the costs of war. In this game, the equilibrium is for any two countries to ally against one country. However, standard game-theoretic arguments do not identify which of the two countries should form an alliance; thus, the problem of multiple equilibria exists despite the game being zero sum. 41   
Although the previous results showed how complexities from communication, multiple equilibria, and learning dynamics are most pronounced in general-sum games, the complexities from bounded rationality are part of all types of games. In brief, psychological experiments have shown that human decisionmakers are subject to cognitive computational constraints, such as limited attention and working memory. 42 As a result, decisionmakers deviate from 39 Osborne, 2003, pp. 136-137.   40 Yuzuru Sato, Eizo Akiyama, and J. Doyne Farmer, "Chaos in Learning a Simple Two-Person Game," Proceedings of the National Academy of Sciences, Vol. 99, No. 7, April 2, 2002.   41 Bruce M. Russett, "Components of an Operational Theory of International Alliance Formation," Journal of Conflict Resolution, Vol. 12, No. 3, September 1, 1968.   42 Daniel Kahneman, Thinking, Fast and Slow, New York: Farrar, Straus and Giroux, 2013; George A.  Miller, "The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing the expectations of full rationality and optimal utility maximization, instead engaging in satisficing behavior using a variety of heuristics. 43  Although deviations from rationality are ubiquitous, there are few guidelines for predicting in which strategic situations real human decisionmakers will act with predictable systematic biases. For example, consider two different types of documented biases in decisionmaking: anchoring and base rate neglect. In anchoring, an individual's initial beliefs about an uncertain entity or likelihood of an event are not sufficiently updated, despite the availability of new information. 44 By contrast, base rate neglect biases occur when individuals overreact to new information as it is presented, undervaluing the larger sample provided by history. 45  These biases work in opposite directions; thus, someone trying to predict the outcome of a strategic interaction would benefit from knowing which of these two (or other) behavioral biases might be exhibited by players. Unfortunately, it is difficult and perhaps even impossible to predict which biases people will exhibit before the biases are observed.
Further compounding issues, human decisionmakers are more likely to exhibit behavioral biases when the strategic environment is already complex. 46 Stochastic dynamics, highdimensional strategy and observation spaces, and complex communication protocols can all instigate deviations from full rationality. Because nonrational behavior is complex in itself, this is a case in which the effects of complexity are compounded. General-sum games with communication and multiple equilibria are complex, even when it is assumed that players are capable of making fully rational, optimal decisions. However, a complex strategic environment encourages behavioral nonrational decisionmaking, which increases the complexity of analyzing or predicting the behavior of decisionmakers.
One benefit of computational-complexity theory is that it provides a specific quantitative measure about the computational resources needed to compute a solution to a game. This number-usually a function of the strategies-is concrete and based on mathematical principles. Unfortunately, some of the sources of complexity discussed earlier are not amenable to such a rich classification, and such quantification based on computational complexity may obfuscate the sources of strategic complexity that characterize the specific game and the strategies of the players being analyzed. For example, computational complexity might say how long it takes to find an equilibrium, but it does not address how to choose among multiple equilibria. However, there are binary measures that can be used to quantify complexity.
The first, and arguably most important, binary measure is whether agents' goals are either perfectly aligned or perfectly opposed. As discussed in the previous section, there are several nuanced considerations that arise when agents have some incentive to collaborate but also to compete.
The second binary measure is whether players communicate. In games in which players are perfectly aligned, honest communication is optimal, but designing communication protocols can be difficult. When players' goals are not perfectly aligned, players might be incentivized to tell partial truths, balancing the benefits of cooperation with the advantages derived from information asymmetries. Both players deciding how to communicate a partial truth and how to disentangle facts from falsehoods adds complexity to strategic interaction.
The third binary measure is whether players are experts interacting in a familiar domain or neophytes interacting in a novel domain. In the latter case, it is unlikely that players will act rationally, and it is likely that they will deviate from optimal behavior in systematically identifiable yet unpredictable ways. Crucially, these deviations do not just add noise to potential outcomes but can drastically alter the distribution of outcomes, increasing the complexity of analyzing and participating in strategic games.
Unfortunately, there is no principled way to combine these binary measures into one number that captures the complexity of the game. For this reason, a careful analysis of each strategic interaction is required to understand the effects of each feature that increases complexity.
The computational complexity of finding a Nash equilibrium provides valuable insight into one dimension of complexity in a game. The results in computational-complexity theory are general and apply to broad classes of games. However, there are other features of strategic interactions that alter a game's complexity on other dimensions. Specifically, such concepts as communication, multiple equilibria, and learning dynamics can all add to the complexity of understanding, predicting, and participating in a strategic interaction. These features are most pronounced in general-sum games in which players' interests are neither directly opposed nor aligned. This suggests that, although recent advances in computing strategies in two-player zero-sum games, such as Go, can handle the computational complexity that arises from a large strategy space, there are other sources of complexity in real-world general-sum strategic interaction that require additional treatment and analysis.
Bounded rationality adds another layer of complexity. Although the rational-actor model assumes optimal behavior in pursuit of internally consistent utilities, there are many reasons that players can deviate from this expectation. They can have error-prone strategies, engage in limited hierarchical thinking, or systematically miscalculate probability and rewards. However, when one is reasoning and making predictions about real humans in real strategic situations, it is often difficult to predict which, if any, of the systematic errors they would make. Because behavioral and systematic errors are well documented in real-world decisionmakers, this is another source of complexity in understanding human decisionmaking that computational-complexity theory overlooks.
Game theory is the study of multiple interacting decisionmakers (players) set in the (boundedly) rational-actor context. Specifically, decisionmakers have strategies, payoffs, information sets (specifications of what each player knows), and chance elements. These basic building blocks provide a rich set of tools to specify a game. Once a game is specified, a theorist employs a solution concept to draw conclusions on how the game's building blocks influence behavior. Although the Nash equilibrium solution concept is the most well known, its refinements, such as subgame perfect equilibrium, perfect Bayesian equilibrium, sequential equilibrium, and forward induction equilibrium, are all commonly used by game theorists. In addition, behavioral solution concepts, such as the quantal response equilibrium, the level-k solution concept, and prospect theory preferences, add additional flexibility into predicting human behavior.
The simplest version of a game is the canonical simultaneous-move game. In this game, players act only once in a deterministic environment and choose their action among a finite strategy set. These types of games are often used to introduce such concepts as dominant strategies, coordination problems, and mixed strategies. Although such simultaneous-move games capture some real-world decision problems (mismatch games, such as those between a penalty kicker and a goalie, are a common example), the single-shot simultaneous-move game lacks many of the key elements that makes real-world strategic problems complex.
Beyond simultaneous-move games are dynamic games of perfect information. In these games, players take actions at discrete moments in time. Importantly, one of the players can be a "nature" player that represents the underlying environment; thus, dynamic games can capture future uncertainty. Many parlor games, such as chess, checkers, Go, and backgam-mon, are dynamic games of perfect information. Although all of these games have only two players that take turns, turn-taking is not a restriction of dynamic games of perfect information; instead, the formalism allows for any (possibly random) specification of move order. Similarly, at some instants, the players can make moves simultaneously.
Although identical in structure to dynamic games of perfect information, Markov games provide an intuitive framework for analyzing dynamic games of perfect information when the nature player has a significant role. In Markov games, the nature player can be used to model the evolution of the parameters of the game. For example, the rules of the game can change, the players' preferences can change, the number of players can change, and the available strategies of each player can change. Furthermore, these can be either random or determined by players' past actions. Although the state change must be Markovian-meaning that transition probabilities to future states are determined only by the game's current state-a careful definition of the state space allows for long-term dependencies in the game's statespace transitions to be considered. The discrete-time Markovian framework can be extended to continuous time; players act in discrete intervals, and the time between moves is governed by a stochastic clock whose distribution is governed by chance and players' actions. These are known as semi-Markov games and are the most general formulation of dynamic games of perfect information with discrete action times.
Of course, one central feature of real-world strategic decision problems is imperfect information. In the simplest sense, a game of poker has imperfect information because each player does not know the other players' cards. However, many real-world strategic decision problems-auctions, hiring decisions, pricing decisions, investment and finance, arms races, lobbying, and almost all of international relations-are rife with imperfect information. 47  The game-theoretic formalism is rich enough to insert imperfect information into both simultaneous-move and dynamic games. In these cases, players must form beliefs about the true state of the world and take actions given their beliefs. In a general sense, semi-Markov games of imperfect information allow for environments in which the underlying rules and preferences change over time but players are not fully aware of these changes. In such scenarios, players may receive heterogeneous information asynchronously and take hidden, perfectly observable, or partially observable actions.
In total, a semi-Markov game of imperfect information (which subsumes many simpler formulations) is flexible enough to capture, in theory, many real-world strategic scenarios. How appropriate such an approach might be to understanding a given strategic scenario is less clear. To address this question of appropriateness, other conditions, such as tractability and model sophistication, are important. These features are often subjective and are left to the judgment of the modeler. So, although game theory is a flexible mathematical tool that tion of individuals and subgroups that make up a nation through tools that resemble viruses, bacteria, and other pathogens that negatively affect their hosts.
In this chapter, we first discuss the international system as a biological ecosystem. Afterward, we provide two discussions of contemporary security challenges that the United States faces in this biological framework.
A useful first step in addressing these new forms of competitive engagement is to recognize that contemporary international competition has analogues to competition in other complex adaptive systems. 
5
6
7
8
• viral disinformation, which is defined as false information propagated with the intent of deceiving and manipulating public opinion to stimulate or blunt collective action. This information is very difficult to debunk with facts or logical argument given the social value and context in which it is transmitted and reinforced and the underlying psychological propensity for emotional 10 and motivated reasoning. 11 • resilience and immunity, which is a biological metaphor for the body politic's ability to suppress or defeat foreign threats. It is analogous to biological defenses, notably the immune system's ability to identify self and non-self actors; as we will show, it can be exploited, resulting in several long-term problems. • the Darwinian Demon, which is a theoretical construct that describes an organism that is not constrained by physiological trade-offs. 12 In nature, species tend to evolve strategies that adhere to certain constraints-one can hunt prey but not photosynthesize. The Demon is not so constrained and can do everything: fly, swim, photosynthesize, burrow, etc. On the national stage, frontier technologies, such as artificial intelligence (AI), may create states that resemble Darwinian Demons that simultaneously maximize their ability to explore and exploit solutions to their strategic challenges. 13 The possibility of multiple states becoming such Demons has strong implications for long-term competition and international order.
In general, these concepts leverage structural similarities between biological or evolutionary systems and social systems. Figure 
18
Our goal in presenting these discussions is to use biological analogies as sources of inspiration for new ontological concepts and analytic methodologies for conceptualizing national security. We hope that these discussions will expand how solutions to the nation's most pressing needs are conceived and will provide possible solutions to the challenges they pose.
In nature, we can think of species playing an infinite game in which the goal is to keep playing. 18 Through evolution, species develop the means to survive and not go extinct. Importantly, to continue playing this infinite game, species evolve traits and strategies that are different from those that would exist if they played a finite game, in which the objective is to defeat an adversary in the moment rather than having the capacity to continuously adapt to new challenges. Thus, in evolution, success is never final, as surviving one challenge only means that future ones will be found. 19   The Immune System at the Level of the Individual One such strategy for long-term survival that has evolved over millennia is the vertebrate immune system. In an infinite game in which the search for a competitive edge is both con-  18 Carse, 1986. 19 The military historian Geoffrey Parker has borrowed the concept of punctuated equilibrium from evolutionary theory to characterize how the world's most powerful empires and religions, regardless of their accomplishments, eventually succumbed to future challenges, often arising as a consequence of their earlier victories 
(Geoffrey Parker, Success Is Never Final: Empire, War, and Faith in Early Modern Europe, New York: Basic Books, 2002)
tinuous and pregnant with the potential to change the game itself, resilience to an uncertain future is key to avoiding extinction. 
20
21
First and foremost, one of the most important features of the immune system is rarely recognized: The body's skin, hair, mucosal membranes, tears, saliva, and more create a protective barrier that filters out, traps, and kills pathogens that might cause harm if they gain access to its internal organs and systems. This first level of defense presents would-be invaders with barriers to entry, much as castle walls protect those behind them from foreign invaders. 
22
The general and adaptive immune systems are assisted by the body itself. Cells, whether damaged through trauma or infection, "die loudly," signaling that something is wrong. Thus, the specialized cells and resources that are committed to the body's defense are assisted by those that they protect through a vast communications network that provides warning when something goes wrong. As we will discuss later, interfering in the signaling within the body is how many of the most-difficult and most-complex threats have learned to attack.
The last part of the vertebrate immune system that makes it intelligent is the process by which it remembers pathogens that it has encountered, thus enabling learning to occur over time. This allows the immune system to react quickly to a reinfection or to the presence of a new infection by a pathogen that resembles one that has previously been encountered. Thus, the body's search for solutions to new threats proceeds by building on the tactics, techniques, and procedures that worked against old threats. This learning mechanism is distributed throughout the body-it is not controlled by any one central "commander" like the brain. Responses are quick and resilient to damage because the lymph nodes, which generate and train the immune system's warriors, are spread throughout the body. Equally important, the ability to rapidly generate, train, and deploy the defenders from multiple sites allows for the preservation of energy because the high costs of defense do need not to be carried full time.
In addition to an individual's immune system, society has developed an "extended immune system" in which defenses are enhanced through the coordinated actions and biological experiences of others. 23 This occurs primarily through two mechanisms. The first consists of the additive benefits that accrue at the societal level as the number of people who have acquired immunity to a pathogen reduces its chance to encounter vulnerable members of the group. The second is the body's response to infection-a response that produces both changes in behavior and visible signals that alert others to the presence of an infection; such a response indicates the need to isolate or otherwise avoid contact with an individual who might host a contagious pathogen. 
24
Scott Turner, The Extended Organism: The Physiology of Animal-Built Structures, Cambridge, Mass.: Harvard University Press, 2002.
A challenge that faces the existing security infrastructure in the United States is that the exponential technologies that are propelling global society through a fourth industrial revolution are also allowing for the rapid generation of novel information-based pathogens, specifically viral disinformation. 
26
27
In thinking about national security and the creation of new capabilities and concepts for their employment, one of the first major questions to arise concerns the costs, or the "means," that constrain strategic options. In the social realm, means are the financial resources, human expertise, time to plan and make decisions, and opportunity costs that arise given that resources committed to achieving one objective might be unavailable to pursue others. Regardless of how costs are considered, society's resources are finite.
In biological terms, the cost of the immune system is measured metabolically, and the immune system is energetically expensive. All of human energy comes from the food that people eat, and it is partitioned to various bodily functions. The larger the metabolic cost of the immune system is, the less energy there is for other functions. In many cases, it is not possible to increase the intake of energy, which places hard limits on what sorts of bodily functions can be sustained. This speaks to a key biophysical trade-off in terms of the traits that species can adopt and realize. 
28
Two aspects of the immune system that limit its strength are its decentralization and its localization. If the immune system were centrally managed-say, if the brain were in control-then it would not be resilient to attack (that is, in a decentralized system, there is no one weak point). If the immune system were not localized-if the body were able to counter infection only by ramping up an immune response in every part of the body-then the immune system would be too sensitive, leading to increased instances of autoimmune problems and an increase in bioenergetic demand. This is why the whole body does not react to a cut on the finger; the distributed immune response is designed to amplify a response only to the area around the cut.
How Does the Immune System Differentiate the Self from the Non-Self Entities?
The most fundamental problem that the immune system deals with is separating the self from the non-self within the body. From the perspective of complex adaptive systems, the organism's immune system is synonymous with its identity. 33 Determining self and nonself is essential; a multitude of immune disorders demonstrate the problems that arise if the system is too inclusive, accepting of too many pathogens, or too exclusive, resulting in attacks on the body's own organs or mutualistic partners (e.g., the gut microbiome). The result of failures to differentiate self from non-self is a spectrum of issues: At one end are primary and acquired immune deficiencies; at the other, immune responses that are too active and threaten the body itself (e.g., allergies and autoimmune diseases). 34  Central to the immune system's ability to distinguish self from non-self is its use of small proteins, called antigens, and other signaling molecules on the cell surface. In brief, the immune system relies on antigens and, more generally, major histocompatibility complex molecules as tags for self and non-self entities. The process of antigen recognition includes a somewhat complex process of training for T-cells-the hunter cells that seek out foreign invaders-which rely on antigen-presenting cells. These antigen-presenting cells are heterogeneous immune cells that mediate the cellular immune response by processing and presenting antigens for recognition by T-cells to prove that they belong in the body.
The threats posed by an improperly tuned immune system are severe. As a result, the body makes a significant investment in regulating the immune system's performance and ensuring 33 John H. Holland, Hidden Order: How Adaptation Builds Complexity, Heather Mimnaugh, ed., New York:  Perseus Publishing, 1996, p. 2.   34 Johns Hopkins Medicine, "Disorders of the Immune System," webpage, undated; and National Center for Biotechnology Information, "Diseases of the Immune System," in Genes and 
Disease, Bethesda, Md., 1998.
Defending self from non-self entities is complex because the boundary between them is dynamic. Context matters greatly, indicating that what is regarded as foreign or threatening is contingent on where interactions occur. The decentralized nature of the immune system means that it is not applied with the same strength across the body. Instead, it is "turned on" to a varying degree by a secondary system. Scientists have discovered that cells not generally thought of as part of the immune system actually play an important role in protecting certain organs from immune system attack. For example, many bacteria that would be harmful to bodily organs peacefully coexist on the skin, such as Staphylococcus aureus, which can create severe tissue damage in many bodily organs but is carried harmlessly at the entrance to the nostrils in one in four people. 36 Likewise, the gut microbiome contains an estimated 90 percent of the body's cells that assist in maintaining the body's health, yet if these bacteria were to move a few millimeters across the gut wall they would pose a major threat. 37  The immune system uses several mechanisms to control its response to the same pathogens in a variety of settings. Certain cells found in lymph nodes throughout the body suppress the immune system. As mentioned, the immune system uses antigens to distinguish between normal and foreign agents. In parts of the body, such as the pancreas, that are sheltered from the outside environment, dendritic cells display the antigens of their normal neighbors in a way that puts the immune system "at ease." By reading these antigens without being on alert, the immune system's T-cells learn that such cells are off limits to attack. For example, antigens in the walls of the small intestine suppress immune response and protect the local microbiome. 38  The production of immunosuppressive antigens on a local level allows for the regulation of immunological responses within the body to occur according to context, thus enabling diverse forms of competition and cooperation to occur between the same actors.
Another important set of questions revolves around the time required for the immune system to ramp up to fight an infection and to wind down after the threat has been mitigated. As 35 Klenerman, 2017, p. 74. 36 Klenerman, 2017, p. 9. 37  Klenerman, 2017, pp. 10, 87.   38 Dana-Farber Cancer Institute, "Why Doesn't the Immune System Attack the Small Intestine? New Study Provides Unexpected Answer," ScienceDaily, 
January 10, 2007.
The body stimulates its immune system in reaction to an infection in many ways. The immune system is byzantine, and even the shortest description of the ways in which the immune system ramps up in response to an infection can appear like a textbook. Briefly, a normal immune response can be broken down into four main components: (1) Pathogens are recognized by cells of the innate immune system; (2) the innate immune system triggers an acute inflammatory response to contain the infection; (3) meanwhile, antigen presentation takes place with the activation of specific helper T-cells, which then (4) coordinate a targeted antigen-specific immune response involving a number of other processes and factors.
It is important to know that the immune system responds differently depending on the pathogen. For example, for extracellular infections (e.g., bacterial infections), the body starts with a humoral immune response with B-cells and antibodies. In contrast, for intracellular infections (e.g., viral infections), the body turns to a cell-mediated immune response with activated antigen-presenting cells and cytotoxic T-cells. One particularly relevant example that is specific to viral infections is the actions of interferons, which alert the immune system to the presence of an invader, assist in identifying the invader, tell immune system cells to attack, and inhibit the replication of viruses or cancer cells. 
39
40
In general, the decentralized nature of the immune system means that there are no "direct orders" to prioritize any one place, given multiple threats or injuries. However, the localized amplification of the immune system and the feedback between body subsystems mean that immune responses are directed and proportional to specific threats. All body systems work together to some degree and are dependent on outputs from other body systems. The most relevant example is in the case of stress or injury, which leads to increases in epinephrine, which, in turn, lead to peripheral vasoconstriction and, ultimately, more blood flow to vital organs. The body does not invest more resources to heal the most-threatening injuries but instead prioritizes the maintenance of blood flow to the most-vital organs, most notably the brain. This is also manifested in someone developing kidney or liver failure (from ischemia) to maintain cerebral (brain) perfusion pressure. Put another way, the body does not "sacrifice" any one subsystem via a planning process; any that are "lost" are lost when the local immune response is unable to deal with trauma.
In this subsection, we provide some preliminary thoughts on using the workings of the immune system as a model for defending society from viral information attacks. As with conventional approaches to strategy and national security, the immune system model is ultimately constrained by trade-offs. Of interest, however, is that the immune system analogy provides insight into protective mechanisms that may have social analogues and that the mimicry of immunological processes reveals risks and costs posed by underinvesting and overinvesting in security.
The four core tenets of the immune system provide an initial framing for how the parts of a defensive system might look: (1) a boundary that filters and contains viral information so that it does not enter into the population, (2) a set of general defenses that protect society and its institutions from disinformation, (3) a reserve capacity of defenses that can be adapted to cope with any threat that is not quickly or easily contained by the general defenses, and (4) an institutional memory that provides the ability to flexibly commit and decommit resources to security, allowing the rapid restoration and adaptation of defenses. These defenses may be extended through the ability to accelerate individuals' ability to identify and fight infection within themselves and to change their own behavior in ways that break the chain of transmission to vulnerable members of society.
Of these features, the first is perhaps the most challenging and controversial in a society that places a premium on free access to information and speech. For example, starting with the naïve assumption that all disinformation is foreign in origin, a vision of national security might simply be to disallow information from the external world to enter the nation's digital communications. With this simplistic premise that all falsehoods are exogenous, it is possible to believe that no conspiracies or divisive movements may arise without foreign subversion, but such an outcome would occur at the cost of freedom of speech and the ability to engender positive transformative social and political change. Moreover, it is logical to assume that any defensive strategy that placed a premium on walling off the country from the global information environment would be equally vigilant in surveilling communications within its borders, a fact that has become evident as the world's most technologically savvy and oppressive regimes have restricted both external and internal information flows in the construction of modern surveillance states. 41 Strengthening the boundaries surrounding the information environments of democratic societies seems counterproductive, and resources should flow instead to the other three features of an information immune system.
An immunity-based defense provides an important division of labor: Defenders that represent the role of the general immune system can reside within the population and should be focused on containing threats and separating infected population members from other vulnerable members, while surging defenders eliminate the infection using tailored means. In the social context, this might be difficult to translate into practice because, even if it is clear when citizens believe notable falsehoods, precisely what should be done about it once they do is unclear. Thus, although the immune system model has been applied using the idea of vaccination or inoculation to reduce the susceptibility of population members to viral disinformation prior to exposure, 42 determining how to contain those who are infected, limit their effect on others, and ultimately rid them of infection remains problematic.
Finally, the immune system model poses a significant organizational challenge given the extreme shifts in resources that it requires. As noted, at its peak, the immune system is on par with the body's major organs in terms of cost. Yet the adaptive system largely exists to limit the energetic requirements of protecting the body from infections through a combination of memory of effective counterattacks on invaders and the surges of energy needed to perform them. In organizational terms, significant shifts in staffing, financing, and more would prove to be impractical for developing and maintaining a highly professionalized security organization.
When the immune system is engaged, its metabolic demands are high. In societal terms, an organization replicating the functions of the immune system would likely scale to be on par with other major government departments in meeting temporary surge demands while maintaining a steady-state capability for surveillance and engagement, mirroring the roles of the general and adaptive immune systems. Functional needs include low levels of resourcing that rely on local surveillance and response as part of the body's general maintenance, the scanning of every tissue in the body in search of invaders and damaged cells, 43 and the redirecting of the body's resources away from all but the most critical of functions to fight off severe infection. For perspective, the fiscal year 2021 budget request for the U.S. Department of Homeland Security was $49.7 billion, on par with that of the U.S. Department of State ($40.8 billion) and more than that of the U.S. Department of Justice ($31.7 billion). Each of these departments is dwarfed by an order of magnitude by the U.S. Department of Defense ($705 billion). 44 Yet none of these models might be appropriate for security organizations committed to surveilling the national information ecosystem.
An alternative set of organizations might be Google, Facebook, Snapchat, and Twitter, which are much like government organizations in terms of the scale on which they operate. Comparing the value that they provide to society is complex. For example, the respective market cap and revenue of each organization, shown in Table 
18
The vision of the immune system filtering and hunting for foreign pathogens lacks necessary nuance for understanding how a vibrant heterogeneous society might be defended. As previously noted, the immune system is the body's identity, but this identity is sensitive to context.
The gut microbiome provides essential capabilities that enable both the extraction of energy and nutrients from food and the regulation of bodily functions, yet it presents a high-risk threat if it comes into contact with the body outside the walls of the digestive tract. In the same way, information, even false information, might be a necessary feature of a vibrant civil society yet also harmful depending on context. The question, then, is not whether information is true or false, or foreign or domestic in origin, but rather under what circumstances it enriches the functioning of a democratic society and under what circumstances it threatens it. For example, just as falsely yelling "fire" in a crowded theater is prohibited given the risk of causing panicked patrons to trample one another in search of an exit, it might be the case that the ability to express doubts in the validity of governing processes and decisions must be tempered by the identities of the speaker and the audience. For example, is the speaker a private citizen, a government employee, or an elected official? Is the audience composed of aggrieved voters, legal experts, or students in a classroom simulation?
From an immunological perspective, the question of context is not only about information as the content of a message; it is about the joint features of the content, source, and audience. Just as the body maintains a hierarchy of bodily functions, the combination of content, sender, and audience recognizes that all citizens do not play equally in the same information space and that positions must be defended differently.
For ramping up defenses, the training of T-cells is instructive. Significant energy is expended to ensure that T-cells trained in the thymus exist in a Goldilocks range that makes them appropriately tolerant when distinguishing self from non-self. This training is critical because it limits the prospects of self-harm resulting from an overactive immune system.
In societal terms, overly aggressive defenses produce excesses, such as the internment of U.S. citizens of Japanese descent during World War II or the domestic intelligence operations that targeted the leaders of the civil rights movement during the 1960s. 
47
With the clearing of an infection, the security posture should change significantly; force drawdown and demobilization should happen quickly. However, demobilization should not be considered disarmament. When the immune system winds down, it retains a memory of the attack and consistently trains to reengage previously defeated pathogens on moreefficient grounds in the future, again conserving the energy needed in future fights. Likewise, because the immune system is distributed throughout the body, it continues to hunt for invaders at low energetic levels.
The Signaling of Self and Non-Self Entities Within Society
The immune system operates within an elaborate network of cellular signaling. Its cells regularly patrol the body, scanning in search of antigens that signal whether they belong or not. In a social context, this requires not only surveillance and tuning, as noted, but a broader understanding of the interfaces between citizens and security forces. Under what conditions should security forces interrogate citizens to determine whether they belong or not? More subtly, in what context (i.e., time and place) is an individual considered "self" or "non-self"?
Given that the immune system is decentralized and that there is variation in how it is tuned, its operations remain unpredictable in both time and place. For example, rogue T-cells periodically leak from the thymus that, lacking the discipline and restraint of rigorous training, challenge the body. 49 This process defends against foreign invaders that have learned to present proteins that are like those of the body itself. In social terms, mimicking such behavior would constitute the risky step of governors periodically violating agreements with citizens (i.e., the governed) to test the veracity of an individual's commitment to the social order. Such a process is obviously problematic and likely counterproductive in that efforts by the government's security forces to compel loyalty to society are likely to undermine the legitimacy of the government itself.
Likewise, the establishment of context and tolerance for non-self entities-a necessary condition for preserving the body's overall health-is achieved by cells releasing localized immunosuppressant antigens that reduce or turn off the immune response. In a social context, this would suggest a highly complex and differentiated social, economic, and political landscape in which behaviors and discourse would not have absolute protections but would instead be handled differently, as previously noted. The interesting question involves not so much the need for context or nuance, but rather the mechanism by which it is established. The zones by which regulation is decreased are not based on the immune system's determination (i.e., the determination of society's security institutions) but rather the cells, or citizens, that determine the level of security or regulation they require.
Signaling within the body politic becomes the locus of internal governance and regulation. Although at the systemic level communication between the governed and the governors might appear balanced and well ordered, at the micro-level it would appear highly contentious. As with the immune system's operations, we would expect to see occasional risky or overzealous challenges to citizens about whether they truly belong in society or have been singled out as deviants. Likewise, collective action by citizens can establish local boundaries of enforcement, tuning the security response to the requirements or desires of the population-which would at times demand greater flexibility to explore and experiment with new social, economic, or political practices, while at other times requiring strict conformity and commitment to collective action.
The perils of immune deficiencies and autoimmune diseases have already been noted. Importantly, many of these problems result from pathogens that attack the immune system itself, disarming it or otherwise repurposing it for other ends. 50 In some cases, viral infections attack the immune system itself, preventing it from mounting an effective response. In other cases, viral infections repurpose the body's defenders to attack healthy cells to further weaken the body and create new areas of infection and reservoirs for reproduction within the body. Although the specific mechanisms by which viruses attack and defeat the immune system itself are still being investigated, the complex and highly specialized processes of the general and adaptive immune systems clearly provide a target-rich attack surface. 51  Society too has wrestled with this problem for millennia. If the immune system is the guardian of society, then we return to Plato's The Republic to discuss the question of who guards the guardians. 52 Viral disinformation that affects the beliefs and behavior of society will not leave those who are entrusted and empowered with its defense unaffected. Thus, two pathways for threatening the social immune system must be noted. The first pathway for defeating a social immune system might follow the strategies used by viruses to disarm and attack the immune system itself and render it ineffective at fighting infections. Society must ensure that law enforcement organizations are adequately resourced (and monitored) in terms of manpower, authorities, money, technology, and more to defend themselves from direct challengers, just as military organizations are scaled to pacing threats.
The second pathway is that individuals and groups that have been entrusted with the power and privilege to commit sanctioned acts of violence could be repurposed toward mate- 50 Robert S. Fujinami, "Viruses and Autoimmune Disease-Two Sides of the Same Coin?" Trends in Microbiology, Vol. 9, No. 8, August 2001.   51 Maria K. Smatti, Farhan S. Cyprian, Gheyath K. Nasrallah, Asmaa A. Al Thani, Ruba O. Almishal, and Hadi M. Yassine, "Viruses and Autoimmunity: A Review on the Potential Interaction and Molecular Mechanisms," Viruses, Vol. 11, No. 8, August 19, 2019. 52 Plato, The Republic of 
Plato,
trans. Allan Bloom,
New York: Basic Books,
[375 B.C.]
Review, Vol. 98, No. 3, June 2008.
Establishing baselines of social functions and processes would also provide a critical link for connecting citizens and security forces representing the general and adaptive responses to threats. Just as the immune system listens for alerts from cells that are under attack or otherwise damaged, a society that is adapted to be secured according to the principles of the immune system's operations would have distinct and observable changes in behavior when under the influence of viral disinformation. This requires a strong civil society in which constructive participation in social, economic, and political processes is both observable and inconsistent with information infections.
In nature, the outcomes of species interactions are driven by the evolutionary need for fitness (i.e., the ability to survive and reproduce). Although it is often assumed that the pursuit of fitness is achieved through dominance or the competitive exclusion of other species, there is no willful intent by any given species to exterminate another-examples of this happening 53 Although the decline of the Soviet Union is often thought of as an economic collapse caused by increasingly costly military competition, scholarship has shown that the regime's intellectual "guardians" turned on its ideology, rendering it vulnerable to Western influence that robbed the communist government of its legitimacy and ability to access or mobilize society's resources to continue the Cold War. in nature are simply the result of strategies that have evolved for the purpose of long-term survival. Moreover, nature is replete with examples of higher levels of fitness being achieved through cooperation, including mutualism, and evolutionary transitions that build increasingly large and complex organisms and communities from individuals. 54 The quest to survive leads to the creation of niches, which describe how a species interacts with its environment and others within it. 55 While natural selection results in the selection and propagation of an organism's genotypes, niches are defined by organisms' phenotypic properties and represent the specialized roles that they develop to live within complex ecosystems affecting flows of biomass and energy (e.g., food webs and environmental structures). 56 Such ecological interactions are governed by species' traits, which are selected for through trade-offs according to their contributions to fitness and biophysical constraints.
Individuals, firms, and nations also carve niches for themselves, competing for natural, technological, and societal resources. In many ways, these socioeconomic interactions resemble ecosystems. In socioeconomic systems, there are similar constraints, in terms of financial, technical, and social capital. As a result, states have differentiated traits and occupy the niches within regional and global systems. This is a form of constrained optimization with distinct trade-offs: Given what a nation has in terms of financial wealth, technical ability, and social capital (i.e., its core competencies 57 ), what are its best available socioeconomic strate-gies? New technologies (or acquired cumulative knowledge-based applications) are sought after and continually created to change the socioeconomic trade-offs that constrain nations.
The strategies that species use and that end up defining the niches they occupy are found through the combined processes of genetic drift and natural selection. Genes encode the phenotypes of individuals, which translate into traits and strategies that provide affordances for competition with rivals. The dominant force shaping the traits and the strategies that species employ is natural selection. Through competitive exclusion, certain traits are selected for, and species ultimately sort themselves into niches. In addition, genetic drift through random mutation and such neutral processes as stochastic environmental shocks also affect which genes and traits are present in a population, thus effectively creating an exploratory frontier in the biological trait space. 58 Drift and selection are always factors that affect which genes are present in a population, and the presence of environmental stochasticity means that there is a continual source of randomness to the realized fitness of different species.
Ultimately, randomness is one of the key ingredients for Darwinian evolution. Through the input of randomness into natural selection, species are constantly exploring the openended trait space. 59 Whatever traits a species acquires provide a competitive advantage that maximizes its fitness, which biologists describe as the number of offspring that can survive and reproduce. Essentially, fitness can be interpreted as minimizing the risk of going extinct; species play an infinite game in which the goal is to keep playing. 60  Such processes do not only apply in biology. The history of individual states may also be viewed as having been influenced by randomness, or at least the micro-level choices of individuals whose consequences cannot be predicted when embedded in complex networks of national and international interaction and feedback. 61 For example, despite being one of the leaders in the adoption of field artillery, Charles the Bold, Duke of Burgundy, died in Nancy, France, in 1477 after leading a cavalry charge against Swiss pikemen rather than waiting for the arrival of his cannons. This decision ultimately resulted in the erasure of Burgundy as an independent political unit because the duke's lands were subsequently divided between the French king, Louis XI, and the Hapsburg heir, Maximilian. 62 Just as in ecosystems, the selec- 58 Andreas Wagner, Robustness and Evolvability in Living Systems, Princeton, N.J.: Princeton University Press, 2007, p. 227. 59 Kenneth O. Stanley, Joel Lehman, and Lisa Soros, "Open-Endedness: The Last Grand Challenge You've Never Heard of," O'Reilly Media, December 19, 2017. 60 Carse, 1986.   61 Philip E. 
Tetlock and Aaron Belkin, eds., Counterfactual Thought Experiments in World Politics: Logical, Methodological, and Psychological Perspectives, Princeton, N.J.: Princeton University Press, 1996;
and Philip E. Tetlock, Richard Ned Lebow, and Geoffrey Parker, eds
For the remainder of this discussion, we consider the way in which evolutionary competition drives the acquisition of biological traits, the constraints on the search for traits, and the implications posed by a hypothetical organism-a Darwinian Demon-that could adapt in the absence of those constraints. We then consider the implications of its societal equivalent-the Societal Demon-that might be closer to reality than its biological cousin because of the increasingly capable applications of frontier technologies to the discovery of new technologies. We conclude by discussing the transition from Demons to Angels and the need to become equally adept at employing technology for cooperative purposes and employing it for competitive ones.
Long-term competition between species is not simply an ecological brawl, in which the metrics of fitness capture only instantaneous interactions. 63 Instead, species are locked into a continual arms race, in which they are constantly adapting to changing conditions and exploring the trait space to find a strategy that provides them with a stable niche in which they can survive. At the heart of evolution is the core tension between exploration, which is when actors search for new solutions to problems, including developing novel biological traits or technologies, and exploitation, which is when actors improve upon their existing problemsolving approaches, which may enhance existing traits and technological capabilities. 64 If the environment becomes stable for some period, species will sort into niches. That is, some species will die off, and those that survive will have evolved traits that work-those that minimize their risk of extinction and maximize their biological fitness. Over time, they will become more and more specialized in their respective niches, losing traits that are costly and that do not confer any marginal gain in fitness. However, if the environment were to suddenly change, these highly specialized species could potentially experience dramatic decreases in fitness. 65 To weather these environmental changes, species require the ability to maintain a robust set of diverse traits as part of their biological strategies.
In nature, robustness is promoted through random mutations leading to drift in the trait space. Moreover, evolving robustness points to the selection of the mutation rate itself and spe- 63 A brawl is an open-ended contest for survival in which the winner is typically determined by some variant of "last person standing." Risk and free-for-all Starcraft are examples. Note that brawls are built by taking a two-player game and adding more players, thus converting two-player, zero-sum games into general-sum games. Brawls are distinct from races, in which the objective is to be the first player to attain a particular achievement, a high score, or a competitive position that terminates the contest. See George Skaff Elias, Richard Garfield, and K. Robert Gutschera, Characteristics of Games, Cambridge, Mass.: MIT Press, 2012. 64 March, 1991.   65 Niles Eldredge, Macroevolutionary Dynamics: Species, Niches, and Adaptive Peaks, New York: 
McGraw-Hill, 1989.
Critically, there are trade-offs between the traits that species can acquire to achieve a competitive advantage; the laws of physics and the environment present themselves as constraints on what traits can be acquired, sustained, and propagated. To be extremely strong requires large amounts of muscle mass. This muscle mass guarantees that a person will be slow relative to a person who is lithe. It is too bioenergetically costly to both photosynthesize and have the appendages required for predation. Furthermore, ecological competitiveness (i.e., biological fitness) emerges from a combination of multiple traits or strategies that are also constrained by these trade-offs. One species might evolve to adopt a generalist strategy-that is, a strategy in which the species can do many things reasonably well by expanding the dimensionality of its niche-while another species might adopt a specialist strategy, evolving to be very good at doing one thing.
The success of adopting a specialist or generalist strategy depends on the stochasticity of the environment. Generalists are better able to minimize their risk of extinction (i.e., they do well in infinite games), which is important in stochastic environments in which change and novelty are persistent; specialists, however, can win in terms of competitive exclusion (i.e., in finite games), but this is a risky strategy and confers long-term competitive advantage only when the environment is stable. This point raises the issue of timescales, because nothing in nature is stable in the long term and species are constantly exploring the trait space to find strategies that minimize their risk of extinction.
Being the most abundant species or being able to exterminate a competitor is never entirely the goal; it is simply the outcome of species searching for a competitive advantage in the trait space. Given the nature of all species constantly seeking a competitive advantage over one another in a multidimensional trait space, the notion of optimality is never entirely appropriate. One can consider the multidimensional trait space as defining a fitness landscape, and species move over this landscape looking for local optima. However, as they evolve and change, they alter the shape of the fitness landscape itself. 68 If an "optimal combination 66 Andreas Wagner, "Robustness and Evolvability: A Paradox Resolved," Proceedings of the Royal Society B: Biological Sciences, Vol. 275, No. 1630, January 7, 2008.   67 William P. Barnett, The Red Queen Among Organizations: How Competitiveness Evolves, reprint ed., Princeton, N.J.: Princeton University Press, 2016. 68 Stuart A. Kauffman and Sonke Johnsen, "Coevolution to the Edge of Chaos: Coupled Fitness Landscapes, Poised States, and Coevolutionary Avalanches," Journal of Theoretical 
Biology, Vol. 149, No. 4, April 21, 1991.
The constraints that characterize the trade-offs in the multidimensional trait space are a fixed outcome of the biophysical properties of an organism's physiology. In noting this, an important thought experiment appears: What if there were an organism that was not constrained by these biophysical trade-offs? This organism, called Darwin's Demon, 69 would be able to photosynthesize and predate on other organisms. It would be able to fly, burrow, and breathe underwater. In general, the Demon would defy the physical and bioenergetic tradeoffs that limit which traits and strategies organisms are able to adopt. Why does this Demon not exist in nature? In addition to there being strong physiological trade-offs in certain traits, the bioenergetic costs associated with maintaining multiple appendages or traits are too costly for any one organism to have. Essentially, it is not evolutionarily stable to acquire as many traits as possible because doing so would create an energy deficit that would end up diminishing fitness.
The realization that the number of traits an organism can possess is constrained raises the prospect of being able to change the trade-offs between traits. What if the materials used to construct human bodies were improved? Might it then be possible to both photosynthesize and be a hunter-gatherer? Might it be possible to be both fast and strong? This is essentially what Darwin's Demon points to; the Demon is constructed from different materials with properties that allow it to actualize many traits at once and greatly reduce the bioenergetic costs of maintaining those traits. For society, this is the critical difference: New technologies might allow the emergence of Societal Demons.
Societal Demons do not play by the same rules as everyone else. They have technologies that defy the trade-offs that determine the traits and strategies that limit others. They reconfigure the basic materials that serve as the building blocks for what traits they can employ. For example, a societal analogy to biologically available energy in nature is a nation's economic output. One important rule to change to become a Societal Demon is to improve the economic efficiency underpinning the internal workings of public and private (security) enti- 69 Janusz 
Uchmánski, Anita Kaliszewicz, and Violetta Hawro, "Darwinian Demons," Academia, Vol. 1, No. 9, September 2006;
and David C. Krakauer, "Darwinian Demons, Evolutionary Complexity, and Information Maximization," Chaos: An Interdisciplinary Journal of Nonlinear Science, Vol. 21, No. 3, September 2011.
Another trade-off is between system size and the ability to execute decisions as a collective. As systems get bigger, simply in terms of the number of individuals that they are composed of, information spreads more slowly. That is, information must traverse many more nodes before all actors whose participation is needed for successful collective action are informed. This effect is seen in large businesses and public agencies. 71 The constraint is in the information technology used to source and disseminate information. In society, information is disseminated through many digital platforms, from email to Twitter, and the trade-off is evidenced in terms of the signal-to-(harmful)-noise ratio. While society's ability to communicate has increased in terms of both speed and volume, the noise in the information that is communicated has risen dramatically too. This has effectively reduced the speed at which relevant, accurate, and truthful information is created, communicated, and assimilated by the actors that make up social systems. Developing technologies that enable individuals to find the signal in the noise and carefully manage decisionmakers' attention would be one step toward becoming a Societal Demon.
Preserving information quality and transmission speed becomes increasingly important as social systems increase in the number of nodes and overall connectivity and become more prone to information drift: random mutations in information as it is passed from node to node (like in the telephone game played by kids). Irrelevant information may proliferate and overwhelm decisionmakers. Moreover, the natural degradation of information quality seen with increases in system size is likely to be exacerbated as adversaries take measures to sow disinformation and subversion. The preservation of information quality and the speed and integrity of communications within a society might be a key enabler of competitiveness and cohesion in long-term competition.
What would a Societal Demon look like, then, in terms of its ability to maintain quality information as a core necessity even as it grows in size and complexity? Certainly, for example, the weaponization of information through social media demands new technologies for automated fact-checking. Likewise, research into the reproducibility of scientific findings hints at the value of ensuring that the information used to support decisionmaking is reli- 70 The efficiencies with which states and military organizations convert resources into capabilities and power have been long-standing concerns within national security and sit at the core of net assessment and its concern with competitive strategies. See Peter deLeon and James Digby, Workshop on Asymmetries in Exploiting Technology as Related to the U.S.-Soviet Competition: Unclassified Supporting Papers, Santa Monica, Calif.: RAND Corporation, R-2061/ 
1-NA, 1976;
Krepinevich and Watts, 2015;
and Thomas G. Mahnken, ed.
72
73
74
While it is obvious that it is good to be more efficient economically, or to have an ability to make decisions faster, or to have the ability to maintain the quality of information in organizational and social communication, it is less obvious how to identify new technologies that might facilitate such improvements. Again, we can imagine a Demon that has an infinite ability to adapt. If it does not know how to photosynthesize yet, it can learn to do so instantaneously. Clearly this is impossible in nature, but exponential technologies in society are reducing the time it takes to discover and diffuse novel solutions for new and old problems. These abilities reflect the growing capacity for basic research to rapidly translate into applied solutions. Automation, hybrid human-AI teaming, and gamification are all examples of technologies that help people learn faster and participate in the collective search for and adoption of new solutions and, in some instances, overcome "paralysis by analysis." 
75
Evolutionary biologists know that sometimes organisms cooperate to achieve greater successes in competition at higher levels of organization than they can achieve independently, and that the great transitions in evolution (e.g., the emergence of multicellular organisms from single-celled ones and the emergence of complex organisms with specialized and differentiated organs) occurred as individuals specialized-shedding some previously critical functions-while becoming increasingly interdependent, relying on others. 79 This allowed for the creation of larger, more-complex units with fitness levels that were previously unattainable. Although natural selection is characterized as a perpetual competition between species, this is an incomplete description. Cooperation, mutualism, and the coexistence between species are equally important in the development and maintenance of ecosystems.
The interconnectedness of today's world means that states are intimately connected on a global scale, and one of the challenges posed by long-term competition is recognizing that even the most capable rivals of the United States cannot be strictly defined as "the other" as long as it remains beneficial for all great powers and regional rivals to remain integrated into a single global international system. 80 From an evolutionary perspective, the prospect that leading states could evolve into Darwinian Demons highlights perhaps the greatest 77 X, homepage, undated.
78 Henry Etzkowitz, "Innovation in Innovation: The Triple Helix of University-Industry-Government Relations," Social Science Information, Vol. 42, No. 3, September 1, 2003.   79 Eörs Szathmáry and John Maynard Smith, "The Major Evolutionary Transitions," Nature, Vol. 374, March 16, 1995, pp. 227-232.   80 Rebecca Lissner and Mira Rapp-Hooper, "A Foreign Policy for the Day After Trump," Foreign Affairs, 
September 30, 2020;
Mira Rapp-Hooper and Rebecca Friedman Lissner, "The Open World: What America Can Achieve After Trump," Foreign Affairs, Vol. 98, No. 3, May-June 2019
95, No. 6, November-December 2016;
and Fareed Zakaria, "The New China Scare: Why America Shouldn't Panic About Its Latest Challenger," Foreign Affairs, Vol. 99, No. 1, January-February 2020.
What is the alternative, then, if the incentives to create new technologies are driven by competition and if these very technologies increase the intensity and scale of that competition? Long-term survival might require a collective break from competition as the driving motivation behind technological advancement. Instead, the scale and connectivity of global society might reward embracing cooperation (or at least coordination) as a necessary component to long-term survival. To put it another way, instead of becoming societal Darwinian Demons, states must learn to become Darwinian Angels that seek new technologies that promote cooperation and increase the chances of collective survival.
Competition is one of many forms of exchange between socially interdependent actorsother modes of interaction are cooperation, collaboration, coordination, and, of course, coercion and compellence. Each provides an alternative way of thinking about the pursuit of one's interests. For many decades, observers of international relations have noted that the international system has become increasingly heterogeneous as actors other than states have become increasingly prominent in both quantity and quality. 81 In acknowledging this change, observers have reimagined the basic logic about the organizing principles of the international system as moving away from authority-driven systems (or command systems) or marketdriven mechanisms of simultaneous coordination toward networks in which interactions are based on reciprocity and asynchronous payoffs between actors. 82  In such a system, access to resources and security might be better secured through the accumulation of trust, reputation, and the ability to delay needed payoffs from exchanges. Such cooperative practices may be regarded as the interactive skills of Angels and present the possibility of achieving security by situating oneself in key positions in networks of exchange. Such a change means accepting risk in the short term, or "lower fitness" in evolutionary terms, yet such a change carries the prospect of enhancing long-term security and survival by creating new federations that collectively mitigate, manage, and diffuse risks. When discussing how cooperative interactions between individuals affect selection processes, Richard E. Michod noted, "Cooperation drives the passage from one level of fitness to another, because cooperation trades increased fitness at the higher level for decreased fitness at the lower level." 83  If this conjecture is correct, cooperation is more than an act of altruism that leaves one vulnerable to exploitation. Instead, it is a pathway to achieving a robust status by occupying a niche that others in the system value and are committed to protecting. The structure and persistence of complex ecosystems provide inspiration. As Geerat J. Vermeij has noted,
The organizational properties that enable biological entities to cope with unpredictable circumstances may likewise have originated as adaptations to everyday problems, but they more directly transform unpredictable phenomena to predictable ones. They do so by cooperation, creating multiple novel combinations of preexisting components, preventing threats from spreading, or creating larger biological units that have a longer life span and therefore the means to retain and accumulate information about rare events. 84   To address the challenges posed by global, long-term competition, the United States would be well served as a competitor by improving its capabilities to be a strategic cooperator. Increasingly complex challenges, such as global pandemics, climate crises, and financial contagions, may all be regarded as Great Filters confronting humanity. 85 Great Filters are thought of as the existential biological, technological, and societal challenges that must be overcome for organisms and societies to continue to thrive. Withstanding these challenges will require the development of robust and adaptive global governance regimes that must simultaneously be developed through cooperative agreements and vigorously defended from external challengers and internal defection.
The discussions in this chapter have drawn on lessons from evolutionary biology and ecology to deepen our understanding of international competition. In each case, we argue that analogies involving the inner workings of natural systems can offer valuable lessons for competing in and securing human societal systems. In the example of fighting viral disinformation, the innate and general immune systems provide an attractive model for defense. However, the speculations that we have provided indicate that real challenges exist in bringing this analogy into action. Although research that has applied lessons from the immune system has focused on preventing infection from viral disinformation, efforts to replicate how the immune system fights infection once the body has been infected will prove more complex. For example, the immunity model suggests that effective defenses will require significant on-demand surges in resources, technologies for surveilling the information content circulating within society, high-quality training for security personnel (and AI), high levels of 84 Geerat J. Vermeij, "Security, Unpredictability, and Evolution: Policy and the History of Life," in Raphael D. Sagarin and Terence Taylor, eds., Natural Security: A Darwinian Approach to a Dangerous World, Berkeley, Calif.: University of California 
Press, 2008, p. 36.
Our second example applied a thought experiment about Darwinian Demons-those organisms that are unconstrained by biological trade-offs-to international competition. Technological change is producing new capabilities for global actors to master technologies for finding technologies, which has made the prospect of competing states with nearly unlimited adaptive potential a possibility, thus bringing Darwin's Demon to life in the social world. We argue that conflict between multiple Societal Demons would be catastrophic and that the world's most capable global competitors might look to the strategic advantages that accrue from cooperative expertise as an alternative way of meeting their security needs in the long term. Such a transition from Demons to Angels might be crucial for surviving the major challenges that humanity faces in the future. Whether the conjectures provided in this chapter ultimately play out as we describe, we believe that there is value in continuing to look to nature as a rich source of lessons about long-term survival, particularly in open-ended infinite games, including those characterized as UGS.
events that fit this broad definition, 2 and different games for different purposes can look quite different from one another. Yet most games still leverage the same basic elements of actors represented by human players making decisions in a complex policy environment using a set of rules to determine what actions they can take and what the impacts of those actions are on themselves, other actors, and the environment. These elements combine to create tools that focus on human decisionmaking, particularly in group settings and in competition with other players over time, which provide a critical opportunity to observe strategic decisionmaking processes and outcomes.
Games are used by a wide variety of policy professionals. Ever since the Prussian army began using wargames for training in the 19th century, gaming has been used by major militaries for military planning, concept development, training, and education. 3 Since at least the 1950s, games used to support defense and national security analysis have examined questions that extend far beyond force-on-force combat and address political, economic, social, information, and infrastructure decisionmaking. 4 Today, games are valued for their ability to foster innovation in the force, 5 support decisionmaking in complex contexts, 6 and bring stakeholders together to work on emerging and contentious issues. 7  Gaming practitioners use two lenses to explain how games provide value to national security policy. The first lens treats games as an art form that produces an understanding about the stories told by national security professionals. 8 This group of gaming practitioners tends to view games as a different type of research than scientific analysis, pointing to such traditions as artistic research as models for the process and goals of games-based research. 9  The second lens treats games as a tool for data generation and analysis that is analogous to other forms of social scientific inquiry. Gaming practitioners with this perspective argue that the social sciences offer approaches to analyzing human behavior at the individual and group levels. Advocates for treating games as a science argue that applying social scientific standards from observational and experimental approaches to games helps align them with the approaches used by other national security policy tools. This can make gaming outputs more useful for research by making insights easier to integrate into broader studies using multiple methods and can make findings from a game more digestible to senior policymakers who are not directly involved in the game. 10  Although the artistic approach to gaming has dominated the field to date (it is no accident that the best-known text in the field is titled The Art of Wargaming), 11 the prospect of science and technology investments to support engagements in UGS warrants consideration of how scientific approaches to gaming might be advanced given the prominence, and often uniqueness, of gaming as a research tool. 12  One helpful way to contextualize the role of games in policy analysis and decision support is to examine how games fit into the broader universe of tools for policy research. Put differently, what types of questions are games well suited to inform? John Hanley has advanced a conceptualization of the relationship between research tool and question as a spectrum from statistical indeterminacy to structural indeterminacy, shown in Figure 
19
Traditional kinetic wargames usually feature two sides, who represent opposing militaries and make decisions about the conduct of attrition-based warfare. In turning from purely military problems to UGS, more decision centers are added. For example, the views of political decisionmakers, interagency perspectives from across government functions, the views of other nations, and the views of nongovernmental actors are all identified as key perspectives in UGS, and thus need to be represented in games examining these spaces. At the same time, the ability of these actors to make various decisions that interact and can be combined means that an actor's complete decision space is unknowable. This complexity means that decisions are rarely made with full information, about either the state of the environment or the objectives of different actors. This uncertainty presents theoretical and practical problems to game-theoretic exploration because the interactive structure of the game is not fully evident and the interactive state space cannot be fully explored using computational means. 16 Therefore, when compared with game-theoretic or statistical alternatives, gaming might be the best available tool for providing a broad look at interaction in UGS.
As the frequent use of the term wargame suggests, much of the focus on analysis using games has centered on force-on-force kinetic warfare. However, there has also been a long history of games that are focused on UGS. Beginning in the 1950s, the RAND Corporation began experimenting with "political-military" games, work that was later extended by the Massachusetts Institute of Technology, Harvard University, and the Joint Staff's wargaming arm. 17 Many of these games focused on how a crisis might unfold in light of the emergence of nuclear weapons, and the focus of action was on political and psychological decisionmaking, not military attrition. 18 These games generally took the form of free-form or seminar-style games that were focused on expert interactions and shaped by a carefully written scenario. 19 Later itera- 16 Grana, 2022. 17 Rex Brynen, "Gaming the Nonkinetic," in Pat Harrigan and Matthew G. Kirschenbaum, eds., Zones of Control: Perspectives on Wargaming, Cambridge, Mass.: MIT Press, 2016. 18 Reid B. C. Pauly, "Would U.S. Leaders Push the Button? Wargames and the Sources of Nuclear Restraint," International Security, Vol. 43, No. 2, Fall 2018. 19 For descriptions of free-form political-military game design and scenario constructions, see H. 
A. DeWeerd, A Contextual Approach to Scenario Construction, Santa Monica, Calif.: RAND Corporation, P-5084, 1973;
William M. Jones, On Free-Form Gaming, Santa Monica, Calif.: RAND Corporation, N-2322
-RC, 1985;
and Jones, 1986.
26
27
The first approach to game design attempts to build formal models that capture the dynamics of interdependent choices of military, social, economic, or political operations in UGS explicitly. In theory, this model then becomes the "game board" on which players make decisions and experience the modeled consequences of their choices. This approach often takes as its intellectual starting point the geographic maps, technical performance specifications, and detailed combat rule sets based on the standard practices of rigid, rule-based wargaming and seeks to build similar tools to capture social or nonkinetic phenomena (e.g., cyber effects and deception). However, the nature of UGS presents four immediate problems.
The structure of problems associated with UGS challenges game designers because many aspects of UGS are not well understood and have not been captured by parsimonious models that commonly form the foundation of physics-based game mechanics. The lack of compelling causal models has been noted as a limitation in games ranging from irregular warfare to operations in the information environment. 
28
The second problem occurs when credible models are available but cannot be easily translated into game mechanics. Kinetic game rule sets are built with the assumption of generalizability. We can develop a probability that a tank unit will destroy an adversary tank unit that might be conditioned by a few simple factors (ratio of forces, terrain, etc.), which can be applied in many circumstances. However, when the types of social actions of interest in UGS are examined, the relevance of different factors, magnitudes, and even directions of effects can change depending on the specific context and might have a fair degree of uncertainty associated with them. For example, the influence of casualties on willingness to fight can be positive (a rally-around-the-flag effect) or negative (war weariness) depending on the specific population and context. 31  As a result, social science models on UGS topics tend to be either highly general or highly specific to a particular case. 32 In the former situation, models might be too abstract to provide much of a basis for game rules. After all, the fact that casualties might influence the population to some degree and in some direction does not provide particularly helpful guidance in determining game outcomes under specific combinations of conditions. In the latter situation, game rules derived from the model are not very portable; thus, considerable effort is needed to repurpose them between games. 33 For example, a model of the effects of casualties in the United States during World War II is not likely to inform contemporary popular behavior. Projects can opt to spend time developing transferable models by compiling evidence from many cases and developing a generalizable model that is suitable to serve as a basis for game mechanics, but such efforts are a considerable undertaking that might be outside the scope of a gaming effort. 34  ficult to build in a way that allows meaningful player interaction without placing significant burdens on players and designers.
The fourth problem with Approach 1 is the limited record of model-heavy games meeting players or sponsors' expectations. The rise of commercial computer games over the past 30 years has raised the bar of player expectations. The massive scale of commercial games allows studios to invest hundreds of millions of dollars in designing, marketing, and maintaining top-tier titles. Even games that serve as training platforms, which can be broadly used, tend to have budgets that are an order of magnitude smaller than those of top-tier commercial games. Games for research and analysis, which are less reusable, generally have substantially smaller budgets than training games. As a result, games that are built to conduct research and analysis cannot compete with the polish or sophistication of commercial titles. In some cases, such as photorealistic rendering, the gap might not be critical for the usefulness of the game. However, in other areas, such as human-computer interaction (HCI) and computational architecture, the resource gap can affect game design, conduct, and participation in meaningful ways.
Security and classification requirements limit the extent to which developments in commercial spaces can be leveraged in DoD-bespoke gaming products. 
41
Taken together, model-based games both struggle to represent the true complexity of UGS and tend to decenter human players. This results in games in which the model is too complicated to be tractable, the model lacks empirical grounding, or the model creates barriers to game play.
The alternative approach that is commonly used in games focused on UGS is to minimize the complexity of the game environment and rules by allowing most of the complexity to exist in the minds of players. This style of game, often called seminar-style or free-form gaming, 42  has little in the way of explicit modeling or formal rules and instead depends on scenarios to define the initial context in broad terms. Details about the environment; about the interests, abilities, and available actions of the players; and about how actions affect others are provided by the players or expert adjudicators in the game discussions and actions. Players are free to suggest a wide variety of possible actions, and interactions between players are worked out in discussion between experts. In this approach, the games mostly exist as mental models until a particular relationship or capability becomes relevant to game play, at which point it is revealed through discourse.
This approach solves many of the concerns of Approach 1. By depending on mental models to govern what is and is not allowable, players have nearly infinite flexibility and autonomythe human is profoundly at the center of this type of game. Additionally, these games tap into the practical knowledge of players to better understand how those with expertise in a given undergoverned space think and apply their insights to the policy problem of interest. That is, there is no need to attempt to shoehorn understanding into generalizable meso-level models. However, this approach raises new problems.
The expertise of the players is essential for determining the credibility of seminar-style or free-form games. These games rely on synthesizing the mental models of their participantsif participants' mental models are poor, then the results will not be useful to support research. Vetting this type of game requires assessing the expertise of the players and how well their expertise translates into the game's environment, which often pushes into novel, hypothetical, and future strategic conditions. This problem is particularly acute in the adjudication of emerging areas in which what constitutes expertise and experience is unclear-adjudicators profoundly shape the play of the game, but they might be no more expert than the players. 
43
A second problem for using seminar-style or free-form games for research is the question of how information is transmitted from the players' mental models to the research team. Research on expert judgment emphasizes that these mental models often take the form of heuristics, which might or might not be explicit in the minds of the players, much less legible to an observing researcher. 
44
In practice, analysis of free-form games by and large depends on external observers listening to the arguments put forward by players to rationalize their actions and then reporting on those beliefs. In some cases, surveys of individual players are used to supplement these data, but this happens inconsistently enough that there is not a strong body of research on best practices for administering these additional data collection instruments. Put simply, approaches to measuring why players are making their decisions have not systematically improved since the 1950s. What is more, the many dimensions of complexity in UGS 
(social, economic, political, etc.)
A third problem is that relationships within complex systems and the dynamics that they exhibit exceed the capabilities of boundedly rational humans to observe and manipulate them without assistance. 47 Humans have finite cognitive capabilities that limit their ability to collect, process, and manipulate information in logically consistent ways. 48 Experts use patterns drawn from past experience to develop heuristic judgments that limit the information needed to make a decision. Although these judgments can be strikingly accurate in familiar contexts, these rules of thumb break down in new contexts, and experts are often unaware when such breakdowns occur. 49 As systems become more complex and human capacity remains finite, these shortcuts are also more likely to lose logical consistency.
In summation, in this approach, a dependence on players' mental models allows flexibility on the part of the players and the adjudicators, but it can also be limited by a lack of knowledge about the best elicitation processes for (1) communicating mental models, (2) linking mental models with key concepts that relate UGS to national security, and (3) ensuring that mental manipulations are logically consistent.
The previous section laid out two traditional approaches to coping with the complexity of UGS in games-the first based on developing increasingly sophisticated formal models and the second relying on exploiting the mental models of players to drive game play. Both approaches tend to fall short: Rigid approaches tend to lose focus on the human, and freeform approaches lose the transparency and consistency that comes with formal documentation of a model. An ideal approach to gaming UGS would rest on a balance between the two approaches. Our assessment of the state of the art in both approaches identifies active efforts to bridge the divide between them. First, iterative use of both modeling and Approach 2 games can offer a way to bridge the divide. Second, clever game and interface design can maximize flexibility and opportunities for meaningful interaction in Approach 1 games. Third, drawing on social science research methods can add structure and transparency to the process of explicating and communicating mental models, thus improving the internal validity of Approach 2 games.
One approach to bridging the gap is to use games and models iteratively. 50 Work begins with constructing a model, which is then used as the basis for an Approach 2 game. Players are then able to provide feedback about the model, both directly (such as by stating that adjudication results do not align with expectations and discussing) and indirectly (such as by suggesting additional actions that could be taken). The initial model provides a degree of structure to the game, which makes it easier to organize new information that comes in from players. Conversely, the flexibility of an Approach 2 game allows new ideas to be discovered and incorporated through game play. These additions are then used to refine the model, which, in turn, can be used as the basis for additional game play. Over time, the United Kingdom: Cambridge University Press, 2001; 
Kahneman and Klein, 2009;
Klein, 1999;
and Philip E. Tetlock, Expert Political Judgement: How Good Is It? How Can
Monica, Calif.: RAND Corporation, WR-1179
, 2017;
and Peter P. Perla and Ed McGrady, Systems Thinking and Wargaming, Arlington, Va.: Center for Naval Analysis, 2009.
One open question is whether using this approach will break down in the face of increasing detail. The existing literature on iterative use of models and games tends to favor parsimonious models rather than more-detailed digital twins or agent-based models. 51 It is not clear whether the process can be successfully replicated for more-detailed approaches and, if it cannot, what the key factors might be that would prevent progress. For example, it might be that building a model with any substantial degree of detail requires multiple iterations through the cycle, meaning that substantial time and cost must be dedicated to the effort. Time and resources can be particularly difficult to secure in the case of research on UGS, in which senior-level interest-and, thus, resources-can be fleeting. However, it also might be that more-detailed models simply have too many factors and relationships for human players to meaningfully engage with all of them. In this case, there might be some limitation on the approach that cannot be overcome simply by applying more resources.
Developments in technology have often been seen as key in managing the pitfalls of gaming complex problems. Policy gaming and computers grew up together in the 1950s and 1960s. Many early games were quick to take advantage of the abilities of computers to handle complex and tedious calculations and to build and solve mathematical models that would have been too cumbersome to manage by hand. 52 As early as the late 1960s, time-share computers-a critical step toward the internet-were being used to enable distributed game play over a distance to support Defense Advanced Research Projects Agency research. 53 This work recognized what are seen as key affordances offered by computers today-the ability to handle models that are too large to manage on paper, the ability to perform cumbersome bookkeeping tasks, and the ability to engage with players who are separated by time and space. With each new wave of technology comes the hope for breakthroughs in game design that will allow for improvements to Approach 1 games.
The earlier chapters in this report detailed recent evolutions in computational approaches that offer promise in better modeling UGS, which could translate into improvements in game design. Advances in Agent-Based Modeling show great promise. 54 Because agentbased models and games share a fundamental focus on causal mechanisms, their advancement might naturally complement the needs of Approach 1 games in ways that earlier "toy" 51 On parsimonious models, see Chapter Twelve 
(Davis, 2022)
(Winkelman, 2022;
Connable, 2022;
and Axtell, 2022)
Bartels, 2020, pp. 155, 157-158.
Beyond these efforts to piggyback on more-general improvements in the modeling of UGS, the continuing evolution of commercial games has raised hopes for additional advancements in games that can support policy analysis. First, advances in artificial intelligence (AI) and machine learning suggest the value of these tools in creating decision aids and other types of support to manage complex games. Second, HCI capabilities have changed how players interact with game environments in ways that open up new design options. Although both areas of research are still in the process of maturing sufficiently to offer truly valuable inputs into policy games, clear avenues for research are becoming evident.
Recently, the potential role of AI in military games has received increased attention, in part because of its championing by former Deputy Secretary of Defense Robert Work. 57 Advocates of AI for games point to the advances displayed in competitions against highly skilled players in recreational games. Recent achievements show compellingly that it is possible to learn superhuman strategies even in games in which finding Nash equilibria would be computationally expensive or intractable. 58 Go was thought by many to be beyond reach for AI systems until DeepMind's AlphaGo system defeated Lee Sodol, an 18-time world champion, in 2016. 59 Additional advances can be seen in AI systems built for StarCraft II, a popular real-time strategy game with an action space consisting of hundreds of player actions-much larger than that of Go. AlphaStar, a recently introduced AI architecture with a fairly complex modular structure that brings together several advances in deep reinforcement learning, was able to beat a world grand master. 60 The recent successes of AI in such games as Go and Star-Craft show that achieving superhuman performance in nontrivial games is possible if sufficient training data and a simulation engine for self-play are available.
However, many of the characteristics that have made these efforts successful are generally missing in policy games, particularly in games focused on UGS. Fundamentally, policy games are most helpful before a problem has been fully structured, and UGS are defined by the uncertainty surrounding available actions and causal effects. Even if the action space can be defined, the high dimensionality of UGS problems will require additional training. The use of AI might add to the difficulty of understanding results because explaining the causal logic of the algorithm's reasoning presents a serious research problem in its own right. 61 Neither training data nor the kind of simulation engine used for recreational games is available for policy games, which are generally run only a small number of times. Similarly, the presence of incomplete information will limit the viable types of algorithms and increase training requirements. Few formal verification tools exist in AI; often, the only way to ascertain whether an algorithm will work in practice is to test it on a data set-typically a large onerepresentative of the data encountered in the real world. An additional complication when applying AI to UGS is the need to manage multiple decisionmaking agents acting at the same time, which makes it difficult for a learning agent to find an optimal policy. All of these problem areas are the focus of ongoing research efforts but remain essentially unsolved today. A practical implication of these limitations for complex UGS games is that determining the suitability of an AI algorithm for employment in games will likely require a lengthy and iterative process of experimentation and model refinement.
These limitations notwithstanding, AI may be deployed in support of a game in at least four ways. The first potential application is in controlling individual agents or groups of agents, particularly adversarial forces. This is the most commonly envisioned application outside the gaming community but is likely one of the hardest to achieve. Using AI as a player or an agent will require addressing three key issues: how to design goals and rewards for the AI to optimize against, how to train the AI on timescales that allow for uninterrupted interaction with human players, and how to validate models to support the credibility of game results. All three issues are difficult to overcome given the limited structure around UGS and the absence of large training sets or simulations.
Second, a game environment might use AI as part of its model to add complexity to the environment. For example, an event recognition algorithm could be developed to identify certain patterns of activities by collective actors, which could trigger certain outcomes in the 60 Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al., "Grandmaster Level in Star-Craft II Using Multi-Agent Reinforcement Learning," Nature, Vol. 575, No. 7782, 2019.   61 Matt Turek, "Explainable Artificial Intelligence (XAI)," webpage, Defense Advanced Research Projects Agency, undated.
environment. The technical challenges to be solved for this kind of use of AI likely would be lower than those of using AI as a strategic player.
The third and perhaps most promising application of AI in UGS is as a tool that helps a human player make sense of the complex game environment. Rules-based applications have long existed, and machine learning-based solutions have been explored to support multidomain command and control and operations. 62 Key advancements that are needed to support greater integration of AI into games include how to represent uncertainty, how best to integrate human operator preferences, and how to make human-understandable suggestions. 63  Fundamentally, these tasks are required for the AI to provide a human player with suggestions that can be thoughtfully integrated into the player's decision calculus. For example, a tool that provides a recommended course of action without an explanation often leaves a player with a dilemma whereby they must choose between an option that they developed manually and the machine-generated recommendation, the benefits of which might not be apparent. In this model, the players are effectively deciding whether they trust the algorithm more than their own judgment.
Fourth, game evaluators could rely on AI to monitor and assess large-scale games by extracting patterns from unstructured data and offering tentative explanations of causality within game play, such as player motivations. As with many of these other techniques, the need for repeated play to extract useful information might drive the costs of this approach beyond the anticipated benefits of analysis.
Emerging HCI capabilities may provide important opportunities to meet three important objectives. First, HCI may facilitate new modes of game play, such as moving game boards into virtual, augmented, and mixed-reality (VAMR) spaces, thus expanding opportunities for player participation and new game designs for persistent and extended games and scenarios. Second, HCI capabilities may enable new ways of collecting information on how players use information, allocate their attention, develop roles and responsibilities within teams, frame problems, and develop and choose between prospective courses of action in games. The ability to operationalize measures from many research fields might be a key to bringing experimental and empirical methods and measures into gaming practice. Third, HCI may enable new ways to examine how computing resources can assist in, or further complicate, 62 For examples, see 
Sherrill Lingel, Jeff Hagen, Eric Hastings, Mary Lee, Matthew Sargent, Matthew Walsh, Li Ang Zhang, and David Blancett, Joint All-Domain Command and Control for Modern Warfare: An Analytic Framework for Identifying and Developing Artificial Intelligence Applications, Santa Monica, Calif.
In some cases, the relevant literature might be driven by the topic of games, which will tend to point researchers toward such areas as political science, international relations, sociology, economics, anthropology, and military history. Given the challenges inherent in the issues associated with game assessment, there might not be agreement on measures, but, even in these cases, existing discussions can help gamers map their approaches to those employed in established research from other fields. To take an obvious example, deterrence has long been seen as difficult to measure because it requires demonstrating that something would have happened had the deterrent not been in place. Security studies have long grappled with this problem, so referencing this literature can ensure that games avoid adopting approaches that have been widely critiqued and are sensitive to ongoing debates in the field.
Beyond measures of interest that are based on particular topics, broader areas of study that touch on the purpose and mechanics of games might produce measures that are relevant to evaluating a variety of games. Gaming is a social activity with human decisionmaking and interactions at its core. Understanding the empirical markers of many activities in these fields might allow better understanding and measurement of the interactions and processes that unfold during any game.
First, we can consider approaches centered on the individual. Social cognition-the study of how people make sense of others and themselves through such topics as attention and encoding (transforming an external stimulus into an internal representation), organizing memory, stages of cognitive response, affect, and social intelligence during planningprovides one potentially valuable set of concepts and methods. 70 It appears plausible that such topics as models for learning, factors that affect learning (e.g., environmental and emotional regulation), trainability of human cognitive abilities, and cognitive biases and heuristics also provide a relevant basis for future measures and metrics. 71 Gaming measures and metrics could be further advanced through the assessment of alternative cognitive processes (e.g., remember, understand, apply, analyze, evaluate, create) and the use of different types of knowledge (e.g., factual, conceptual, procedural, metacognitive). 
72
Because games for policy are generally interested in the interactions of multiple players, group performance is another important research area to bring into evaluating games. It covers such topics as group formation, group influence, group decisionmaking, social motivational losses, and unique groups. 73 For example, work on group decisionmaking in intelligence analysis might be transferable to group decisionmaking in national security games. 74  Communications is another field of interest. For example, recent work leveraging a narrow area within communications-narrative methods-shows the potential for applying narrative analysis in games to better understand what is happening within them and to differentiate between learning by individuals and learning by groups in the game. 75  For games with an exploratory objective, a potentially relevant research field is creative cognition, which draws on experimental methods from cognitive science. This covers such topics as the specific cognitive processes and structures that result in creativity, schema and mental models, models of generative and exploratory processes, methodological approaches for evaluating creativity, the conditions under which creativity emerges, and organizational creativity. 76 This research can suggest helpful lenses for comparing the relative utility of game designs for specific tasks, such as synthesis or idea generation.
Table 
19
Traditionally, the environment of policy games is created through a mix of narrative scenarios, maps of physical terrain or concepts, and modeling that describes the relationships between elements. Our vision calls for leveraging the emerging complex computational models that represent societies to improve on traditional model-based games by generating in simulation high-dimensional, interdependent representations of populations in UGS.
Our initial vision is based on the use of an agent-based model in which the population is represented at the level of the individual with characteristics that influence how individuals will interact with other agents and in response to player actions. The use of an agent-based model may allow populations within the model to be dynamic and adaptive over an openended interaction space and, at least with some degree of abstraction, to continue to maintain the properties of a socially complex, internally consistent and plausible society despite novel actions by human players or other agents. 78 These features would allow the arena concept to move away from the traditional design choice between a formalized model and free play by generating traceable models that also can accommodate multidimensional player choice.
Additionally, this approach would allow a much greater degree of complexity, to better mimic society. Meeting this requirement will require using models that address both (1) a stratified social system that is characterized by inequalities resulting from specialization and competition within the society and (2) the characterization of each agent's status in a broad context that includes social, economic, political, military, and environmental properties. 79  Put simply, agents would be differentiated across several characteristics, allowing players to explore how different social segments might interact differently. 78 Models of this type are inspired by early works in the field of artificial life and the development of openended systems. See Philip W. Anderson, Kenneth J. Arrow, and David Pines, eds., The Economy as an 
Evolving Complex System, Boca Raton, Fla.: CRC Press, 1988;
W. Brian Arthur, Steven N. Durlauf, and David A. Lane, eds., The Economy as an Evolving Complex System II, Boca Raton, Fla.: CRC Press, 1997;
Lawrence E. Blume and Steven N. Durlauf, eds., The Economy as an Evolving Complex System, III: Current Perspectives and Future Directions, London, United Kingdom: Oxford University Press, 2005;
John H. Holland, Hidden Order: How Adaptation Builds Complexity, Heather Mimnaugh, ed., New York: Perseus Publishing, 1996;
and Christopher G. Langton, ed.
Another feature of the contest arena is that it would be explicitly multiplayer, requiring that teams engage beyond simple zero-sum or purely competitive interactions. Instead, a competition arena game would feature many teams, each with different goals, preferences, capabilities, and resources. The result would be that each team would need to consider both competitive and cooperative strategies and discover the goals and motives of multiple teams.
The heterogeneity among player teams could be substantial. For example, teams might represent consolidated states with a global or regional presence, substate actors ranging from insurgents to trade associations, or transnational nonstate actors, such as foreign fighters and international relief organizations. Each organization would have different capabilities, objectives, levels of access to the local population, and knowledge of the environment-all of which would shape how the team would interact with the population-rather than using a onesize-fits-all solution, which can occur in traditional game constructs. The arena would better align with the conditions of infinite games or brawls; 80 the arena would involve many players competing and ultimately succeeding by surviving and continuing to play. An arena that supported infinite game play could be achieved in a different fashion for each player rather than through traditional game victory conditions in which one player's victory is another's defeat.
To enable flexible game play, every effort would be made to allow players to generate action not envisioned by the original designers. Overcoming this traditional weakness of Approach 1 games will likely require progress in several areas. First, the game's model will need to include a population that exhibits a variety of complex social interactions and structures, which provides a naturally diverse menu of interactive opportunities that consists of military and nonmilitary actions. Progress in this area will ensure that the model is capable of capturing a wider range of potential actions. Second, advanced HCI might reduce the barriers to manipulating models in unanticipated ways, thus allowing the game to provide a level of openness to 80 On infinite games, see James P. 
Carse, Finite and Infinite Games, New York: The Free Press, 1986;
and
Simon Sinek, The Infinite Game, New York: Portfolio, 2019. On brawls, see George Skaff Elias, Richard Garfield, and
K. Robert Gutschera, Characteristics of Games, Cambridge, Mass.: MIT Press, 2012.
Today, such approaches usually require the intervention of both subject-matter experts to interpret player actions and technicians to modify models. This approach is not only costly and time-consuming, but it can also limit the traceability of key sequences of cause and effect because adjudicator mental models are potentially both opaque and not aligned with player mental models. Developing systematic processes and tools that empower players to modify models easily and quickly within games would simultaneously enable greater creativity and more directly capture players' expertise than current practices offer.
Traditional games are generally limited to a few hours, days, or weeks at the longest, which can make it difficult for the multitude of small, strategic, and organizational choices that undergird long-term competition to unfold and be examined if events are considered with much granularity. To take advantage of the detailed and dynamic game board, contest arena games would be designed to run for weeks, perhaps even months, by providing a persistent environment. This much longer period of play would allow far more opportunities for adaptive dynamics to appear and influence decisionmaking. This long period of play would also more realistically simulate the reality of day-to-day decisionmaking during long-term competition, in which periods of heightened activity punctuate stretches of time in which other priorities come to the fore.
Transitioning to a digital gaming environment would free games from some of the most mundane yet significant constraints-the availability of appropriate game spaces and the costs of travel to support colocation. A physical game that is backstopped by the digital tracking of physical assets would enable the rapid breakdown and reconstitution of game states or their synchronization across multiple player sites.
Long periods of game play would require new models for player recruitment and sustained participation. This would involve new thinking about compensation, the provision of read-ahead and preplay materials, and the creation of a digital infrastructure to enable participation from the variety of computing environments found in national securityadjacent organizations. When games are played in a matter of hours or days, it is often easy to "borrow" civilian staff, military officers, and outside academics and contractors from their day-to-day routines. Far longer periods of play will likely require new arrangements between games and individual participants, as well as the institutions they represent. One existing model for securing such attention can be found in the U.S. Naval War College's Halsey Alfa games, in which professional military education students participate as part of their coursework every year. 81 Other options are contracted player teams and a dedicated staff function.
To manage the complexity of the environment, artificial society, multiplayer teams, and long periods of game play, players likely will need various forms of assistance in performing cognitive and operational tasks within the game. For example, monitoring a game environment that changes dynamically around the clock would be difficult for a team working eight-hour shifts without tools for change detection that were able to detect and call attention to important changes in the game environment. This opens the space for a variety of computational capabilities that could run the gamut from information monitoring, dashboards, and alerts to advanced applications of AI for use as decision aids or in task automation. Enabling these capabilities would require a set of predefined, published application program interfaces (APIs) to allow machine-to-machine communication across player teams and the arena and the necessary development time prior to game play to allow teams to enter the arena battle ready.
A contest arena presents new opportunities to study how players and player teams might best employ computational resources. Specifically, how players internally organize as a team and apply computing resources to information collection, sensemaking, decision support, and operational oversight and automation could provide an invaluable window into how AI might ultimately affect military organizations and operations in highly complex, open-ended strategic environments.
Both the contest arena and the physical and digital spaces in which players deliberate would be instrumented to make data capture as low cost and unobtrusive as possible from the perspectives of players and researchers. This would involve favoring interfaces that serve as passive means of data collection and building in tools to easily add active data-capture instruments that align with best practices in relevant fields. For example, the arena might allow easy randomization of players to support survey experiments as part of game play. The goal would be to enable the contest arena to harness improvements in data collection and measurement as a means for better understanding player sensemaking processes, mental models, and decisionmaking. Assessment functions would work to measure how player teams understand the complex behavior of the artificial society and to trace how this understanding translates into the processes of generating, evaluating, and implementing strategies and operations given the environment and other players.
The contest arena presents four opportunities to advance understanding of decisionmaking in infinite games or long-term competition in UGS. First, it offers an opportunity for various stakeholders to engage directly with both one another and models to provide rich feedback to the research teams. Second, the contest arena allows direct measurement and comparison of how well decisionmakers' mental models of a complex social system align with the causal processes governing the behavior of the artificial society within the arena. Third, the arena offers a setting to study organizational designs for long-term competition, particularly the interface between human and computational principals and agents. Finally, the contest arena may create a virtual "sandbox" for testing policies before they are implemented in the real world.
Traditional games have long been valued for their ability to engage stakeholders and generate compelling stories about how and why change needs to occur to effectively deal with specific strategic challenges. 82 Approach 2 games in particular are seen as opportunities to bring diverse stakeholders into a space to learn from each other directly. The contest arena concept offers the possibility of reaching a larger population of participants by supporting more player teams without some of the costs of travel that are traditionally associated with games. 83  At the same time, by competing through an artificial population with features that resemble those of real-world UGS, players may be challenged to reason and act within the bounds of an internally consistent, complex representation of an undergoverned space and may simultaneously improve that representation by creating new model inputs. Thus, a contest arena game would enable a type of direct feedback about the model that is often difficult to secure in Approach 1 games, particularly as models become more complex.
Ability to Compare Team Models with "Ground Truth"
The use of causal models in the contest arena enables research teams to directly compare the understanding of cause-and-effect relationships held by players with the underlying model that governs the behavior of the actors that make up the arena. This direct comparability allows researchers to assess the relative and absolute ability of different teams and approaches to correctly understand and model the strategic dynamics of UGS. It also potentially allows researchers to assess the relative predictive power of alternative approaches to strategic assessment and action. Such research would build on prior scientific efforts to understand, predict, and prescribe policy actions within simulated social systems. 84 In particular, the arena would allow researchers to explore an environment in which multiple teams are predicting causal 82 McGrady, 2019. 83 However, this benefit might be counterbalanced by the longer timeline of the contest arena game.
outcomes and taking action to change the arena simultaneously, allowing for deeper study of interaction effects than research to date.
The ability to observe team decisionmaking in UGS in a controlled environment provides an opportunity to better understand the relative successes or failures of organizational designs and processes. Research questions might focus on the value of AI assistance in performing different tasks, differences in communication styles depending on team style and composition, or the openness of decisionmaking processes. Although such studies can be conducted in other settings, games provide greater access to decisionmaking processes, a greater ability to collect data, and more opportunity for organizational experimentation than is often possible in sensitive national security organizations during a real-world crisis. The ability to explore organizations over time as they adapt and evolve in the face of changing facts on the (arena) ground, where the perspectives of all actors are equally accessible, might provide unique analytic opportunities to better understand organizational behavior in UGS and long-term competition.
One common application of games in policy spaces is to act as a test bed for new policies before the costs of implementation are incurred. Although game results are rarely perfectly predictive, games that are focused on implementation can reveal potential reactions and unanticipated consequences of a new approach and can allow policies to be refined and adjusted before implementation is attempted. 85 Such games can also enable controlled comparison, such as A/B testing, between alternatives that might be too expensive, ethically fraught, or practically impossible to attempt in the real world. A contest arena game would provide a forum to test policies in the context of UGS. This approach might be particularly valuable for considering strategic dynamics over substantial periods of time, particularly as multiple actors modify their behavior in response to one another's policy actions. Such changes are central to adaptive planning, but they can be difficult to observe in the field, where data are missing, sparse, or lost, thus making long-term assessment a challenge.
Gaming remains one important method for researching national security competition and conflict. However, approaches to gaming suffer from their lack of ability to handle complexity, resulting in a push toward either increasingly model-based formal methods of game adjudication that limit player freedom and might not be externally valid (Approach 1) or highly flexible, expertly adjudicated games that might allow highly creative strategies to emerge 85 For an example of the impact of games for this application in a different context, see Elizabeth M. Bartels, Jeffrey A. Drezner, and Joel B. Predd, Building a Broader Evidence Base for Defense Acquisition Policymaking, Santa Monica, Calif.: RAND Corporation, RR-A202-1, 2020. but might lack internal consistency and validity (Approach 2). Recent efforts have offered incremental improvements to both approaches, but they have failed to offer a genuinely novel approach to gaming that can provide new types of insights on active policy questions.
Emerging technologies could enable a new class of games that might offer a third way of designing and playing games. In this approach, agent-based models serve as game boards in which diverse, socially complex societies represent open-ended, evolving populations over which multiplayer teams compete. Through advancements in AI, HCI, and distributed computing, new models and interfaces can address many of the weaknesses of traditional Approach 1 games while providing the ability for players to modify models in real time that might allow the preservation of the strengths of Approach 2 games. Together, new approaches to game design and conduct might be able to place decisionmakers in increasingly complex and instrumented environments, allowing better insights into organizational and strategic decisionmaking in long-term competition and engagement in UGS.
Collectively, increasingly complex games may both elicit and expose the effects of complexity on strategic and organizational behavior and develop decisionmaking expertise that might not be attainable with traditional games. The contest arena concept that we have proposed draws inspiration from Herbert A. Simon's classic observation that it is the environment, not the decisionmaker, that drives complex behavior:
We watch an ant make his laborious way across a wind-and wave-molded beach. He moves ahead, angles to the right to ease his climb up a steep dunelet, detours around a pebble, stops for a moment to exchange information with a compatriot. Thus he makes his weaving, halting way back to his home. . . . Viewed as a geometric figure, the ant's path is irregular, complex, hard to describe. But its complexity is really a complexity in the surface of the beach, not a complexity in the ant. On that same beach another small creature with a home at the same place as the ant might well follow a very similar path. 86   A contest arena, or other new approaches to designing increasingly complex games, could provide the beaches needed to better understand how to engage in UGS and compete over the long term in infinite games.
Despite the historical importance of UGS, DoD and the NSE have struggled to understand and to act productively in these environments. In many cases, UGS-related issues fall between the seams in structures and processes. For example, effective action in these environments often requires the synchronization of instruments of national power that are divided between different organizations, thus making coordination difficult, slow, and prone to confusion. Similarly, the need to act on different timescales-moving quickly to adapt to new conditions while also needing to assess the impact of activities over years and even decades-challenges processes.
These barriers to successful engagement are known to practitioners but have persisted because of the perception that they serve other, higher-priority needs, particularly in DoD. Without change to these key processes, those seeking to manage complex adaptive environments with competitive ambiguity will continue to face an uphill battle.
To better position the U.S. government to manage UGS, new approaches are needed. One way to conceptualize the needed change is to envision a shift from a finite to an infinite game. Traditional military thinking imagines politics and security as a series of contests conducted between defined competitors at bounded points in space and time. However, such a vision obscures the requirements and opportunities presented by infinite games, in which objectives are not to achieve an unambiguous victory over a competitor but rather to endure for as long as possible. Given this difference, a set of alternative premises about surprise, death versus defeat, and power versus strength arise that provide alternative concepts for motivating actions and measuring their success or failure.
New concepts motivated by learning and adaptation, such as the Act-Sense-Decide-Adapt (ASDA) cycle, offer an opportunity to envision national security as a learning process within infinite games-while still admitting the prospect of finite games being played within them. In doing so, engagements might not build toward conflict but rather support the discovery of interests that could give rise to enduring mutualism and encourage institutions and norms that could enable stable and prosperous relations to emerge. Put differently, such an approach broadens the aperture of possibilities that decisionmakers should consider, including more and perhaps more-advantageous courses of action than are provided by traditional assessments.
This type of approach also offers a blueprint for the integration of research, analysis, operations, and strategy. The ASDA cycle argues that knowledge must be continuously pursued and aggregated to test, refine, and challenge beliefs and models that motivate action and evaluate options. Moreover, successful engagements, particularly those that develop robust governance institutions, may not simply demand changes in approaches but may admit the possibility of evolving preferences and goals. Thus, not only should the ways and means of operations change, but the very ends being pursued by strategy might change as well.
Investments in the Social Sciences Are Needed Throughout this report, we have seen that challenges posed by UGS were intimately connected with human decisionmaking, behavior, and interaction. These are areas of inquiry in which the social sciences are crucial for gaining a solid empirical understanding; however, the state of these fields is insufficient to meet the requirements of decisionmakers who need to understand and engage in UGS. Two classes of interrelated investments are needed and can be placed into context within the workings of the ASDA cycle.
First, better models of humans and systems of humans are needed to advance the state of the art regarding the Sensing phase of the ASDA cycle. For sensing to assist a decisionmaker's understanding of the world, new techniques for collecting and connecting data and theory are needed-particularly at granular levels and frequencies that exist beneath the administration of formal governance institutions. Equally important is the need for theories that admit more complexity than contemporary approaches offer-in terms of heterogeneity of actors, their goals and behaviors, and their interdependency. Such work will also require a change from studying the rare and unusual to collecting and analyzing baseline information as a way to contextualize extraordinary moments. As new tools are developed, they will help make sense of the links between probing actions and observed changes in the environment, ultimately bringing new information into decisionmaking processes.
Second, research into the social sciences is needed to benefit the decisionmaking step of the ASDA cycle, thus continuing the earlier steps of data collection and assessment by converting analytic outputs into decision-relevant inputs. Here, the challenges are how to design decisionmaking processes and establish criteria for evaluating policy options based on robustness, resilience, and the ability to adapt to perpetual novelty. On this matter, answers are needed for questions about how to render decisionmaking more open to multiple stakeholders, reduce bureaucratic barriers within organizations, and maintain conceptual and resource commitments to continual exploration.
Collectively, advances in how social systems are understood and how such knowledge can be employed to greater effect in the pursuit of national security would enhance long-term competitiveness and U.S. ability to engage and adapt in UGS of all kinds.
Contributors to this report identified characteristics of promising policy analytic tools and rationales for engaging in UGS and long-term competition more broadly. First, tools should assist in the development of options that do more to explore what is possible than conven-tional planning tools that emphasize the exploitation of what is already known. Actions, particularly small ones, may be justified given the value of information they provide (successful probes), even if they are not understood as the most efficient path toward a goal.
Analytic tools should seek robust and resilient options rather than optimal ones. Such a shift admits to the immutable presence of uncertainty in multiple dimensions: the state of the world (parameters), the causal or relational structure of the world (model), and the value of different outcomes that may result (preferences). 1 Some sensitivities to these uncertainties are the need to model and evaluate data at multiple temporal and spatial scales and the need to consider alternative boundary conditions on system participants and other scoping conditions (e.g., available resource goals and policy priorities). It also requires the ability to mix multiple methods (e.g., data analytics, field surveys, case studies, human played games, and formal modeling and simulation) to create alternative competing analyses and integrated and federated studies.
In total, future analytic processes and products should be strengthened by their openness and ability to accept inputs from multiple stakeholders while finding a basis for motivating collective action among those stakeholders. This is particularly important in the context of UGS, given that these are domains in which a single decisionmaker or actor, without the cooperation of others, lacks the power to effectively control events and dictate outcomes.
The advances in artificial intelligence (AI) remain impressive but narrow. AI systems have proven to be adept at performing well-defined tasks, and many impressive results have emerged from the application to competitive games in which AI systems have displayed superhuman performance. Yet significant gaps remain in terms of harnessing computational power to understand strategic interaction and open-ended systems in which the space of possible interactions are massive, the results of interdependent choices are nonzero sum, and the conditions that bound choices and criteria from evaluating outcomes are perpetually altered. These higher-order features of complexity are likely to remain stubborn boundaries for AI.
Within the context of the ASDA cycle's phases, whenever problems are well specified, AI will play an important role. These most likely will occur during phases in which probing actions and the sensing of their effects will reward acting at speeds or scales that strain human decisionmaking and organizations. Alternatively, within the Decide and Adapt phases, information is interpretated, models are developed and discarded, preferences are explored, and new forms of organization and operations are considered. In these phases, computation may play significant roles, but the types of autonomy in decisionmaking and action achievable in other phases may give way to interactive systems, placing greater stress on the shared ability of humans and machines to handle uncertainty and ambiguity.
Research and Analysis to Support UGS Will Need More-Robust Infrastructure and Organizations
Engaging in UGS will require support from an adaptive planning capability, such as the ASDA cycle, and will demand analytical and data collection capabilities with considerably higher capacity and flexibility than existing infrastructure and tools offer today. The potential for rapid and continuous change in strategy and operations might require new ways to couple research and analysis with the decisions they inform and the capabilities that decisionmakers desire. Here, robust investments in research infrastructure could bridge the gap between the stability of resourcing and focus needed to develop and accumulate basic research and the ability to pivot rapidly toward prospective applications to keep pace with continuously shifting policy and operational needs.
Ironically, new approaches may revisit older visions of grappling with complexity in science. Specifically, when imagining approaches for coping with organized complexity, Warren Weaver drew inspiration from the interdisciplinary operations analysis teams of World War II that brought together mathematicians, physicists, and engineers with physiologists, biochemists, psychologists, and social scientists. 2 Connecting interdisciplinary research teams with operations exposed researchers to the practical problems for which there existed a demand for new solutions; it also allowed operators to better understand and appreciate how research processes and teams could be tailored to their needs. As Weaver noted, this should not be a template for all of science but would constitute an important step in aligning decisionmakers with operations and research communities-a set of relationships that will be strained-as the demands to adapt at faster rates and with greater magnitudes of change intensify in UGS.
We hope that the chapters in this report start a larger dialogue among scientists, technologists, and policymakers working at the interface between governance and national security. We believe UGS, while likely to remain an amorphous concept, present an opportunity to foster productive debate and assist in identifying threats, risks, and opportunities across increasingly heterogeneous and interdependent domains of competition.  
SOURCE: Head Modernisation and Strategic Planning-Army, Army's Future Land Operating Concept, 2009, p. 31. Used with permission.
5 
25 
25 
).
).
11 
11 
NOTE: RDT&E = research, development, test, and evaluation.
4 
4 
SOURCE: Adapted from United Nations, 2020, p. 6.
27 
27 
15 
15 
SOURCE:
76 
SOURCE
a For more information on this, see Aaron B. Frank, "Toward Computational Net Assessment," Journal of Defense
a For more information on this, see Aaron B. Frank, "Toward Computational Net Assessment," Journal of Defense
17 Justin Grana, "Difficulties in Analyzing Strategic Interaction: Quantifying Complexity," in Aaron B. Frank and Elizabeth M. Bartels, eds., Adaptive Engagement for Undergoverned Spaces: Concepts, Challenges, and Prospects for New Approaches, Santa Monica, Calif.: RAND Corporation, RR-A1275-1, 2022. SOURCE: U.S. Army War College, 2020.
SOURCE
2 
2 
15 
15 
62 
62 
On ASDA, see Joint Publication 3-0, Joint Operations, Washington, D.C.: U.S. Joint Chiefs of Staff, Octo-
ber 22, 2018. On JPC, see Head Modernisation and Strategic Planning-Army, Army's Future Land Operating Concept, Canberra, Australia: Australian Army Headquarters, 2009.   
Notably, experts in comparative social sciences (i.e., comparative politics, comparative sociology, and comparative anthropology) would be less surprised and more sensitive to the heterogeneity displayed by governance organizations, practices, and agreements around the world.
John F. Padgett, "Evolvability of Organizations and Institutions," in David S. Wilson and Alan Kirman, eds., Complexity and Evolution: Toward a New Synthesis for Economics, Cambridge, Mass.: MIT Press, 2016.
Scott D. Sagan, "Nuclear Alerts and Crisis Management," International Security, Vol. 9, No. 4, Spring  1985; John Lewis Gaddis, We Now Know: Rethinking Cold War History, reprint ed., New York: Oxford University Press, 1998;
Graham Allison and
5Eliot A. Cohen, "World War IV," Wall Street Journal, November 20, 2001; R. Woolsey, "WWIV: Who We're Fighting-And Why," Richmond Journal of GlobalLaw & Business, Vol. 4, No. 1, January 1, 2004.   
Antullio Echevarria II, Operating in the Gray Zone: An Alternative Paradigm for U.S. Military Strategy, Carlisle, Pa.: U.S. Army War College Press, 2016; Melissa M. Lee, "Subversive Statecraft," Foreign Affairs, December 6, 2019; Herbert R. McMaster, Battlegrounds: The Fight to Defend the Free World, New York: Harper, 2020.
Melissa M. Lee, Crippling Leviathan: How Foreign Subversion Weakens the State, Ithaca, N.Y.: Cornell University Press, 2020.
Lee, 2019; Lee, 2020; McMaster, 2020; Alina Polyakova, "The Kremlin's Plot Against Democracy," Foreign Affairs, Vol
. 99, No. 5, October 2020.
Ethan B. Kapstein and Jacob N. Shapiro, "Catching China by the Belt (and Road)," Foreign Policy, blog post,
April 20, 2019.
Scott Borgerson, "The Coming Arctic Boom: As the Ice Melts, the Region Heats Up," Foreign Affairs,Vol. 92, No. 4, 2013.    
Elana Wilson Rowe, Arctic Governance: Power in Cross-Border Cooperation, Manchester, United Kingdom: Manchester University Press, 2018; Øystein Tunsjø, "The Great Hype: False Visions of Conflict and Opportunity in the Arctic," Survival, Vol. 62, No. 5, September 2, 2020.
  15  This comment was made in a set of interviews performed during this project under the condition of nonattribution. For more information, see Chapter Five of this report (Gabrielle Tarini and Kelly Elizabeth Eusebi, "Adaptation, Complexity, and Long-Term Competition in UGS: Perspectives from Policymakers and Technologists," in Aaron B. Frank and Elizabeth M. Bartels, eds., Adaptive Engagement for Undergoverned Spaces: Concepts, Challenges, and Prospects for New Approaches, Santa Monica, Calif.: RAND Corporation, RR-A1275-1,
2022).16  Stephen Flanagan and Bruce McClintock, "How Joe Biden Can Galvanize Space Diplomacy," Politico, January 15, 2021; Bruce McClintock, Katie Feistel, Douglas C. Ligor, and Kathryn O'Connor, Responsible Space Behavior for the New Space Era: Preserving the Province of Humanity, Santa Monica, Calif.: RAND Corporation, PE-A887
-2, 2021.17  Allan Dafoe, AI Governance: A Research Agenda, Oxford, United Kingdom: Centre for the Governance of AI, Future of Humanity Institute, University of Oxford, July 2017; Dan Ward and Robert Morgus,
Will Fitzgibbon, "Panama Papers FAQ: All You Need to Know About the 2016 Investigation," International Consortium of Investigative Journalists,August 21, 2019.  
  27  Katie Benner, "U.S. Charges 3 North Koreans with Hacking and Stealing Millions of Dollars," New York Times,February 17,  
2021.28  U.S. Department of Justice, "Assistant Attorney General John C. Demers Delivers Remarks on the National Security Cyber Investigation into North Korean Operatives," Press Release February  
17, 2021.29  Mark Bevir, Governance: A Very Short Introduction, illustrated ed., New York: Oxford University Press,  2012, p. 3.   
Giacomo Persi Paoli, Judith Aldridge, Nathan Ryan, and Richard Warnes, Behind the Curtain: The Illicit Trade of Firearms, Explosives and Ammunition on the Dark Web, Santa Monica, Calif.: RAND Corporation, RR-2091-PACCS, July 19, 2017; Giacomo Persi Paoli, The Trade in Small Arms and Light Weapons on the Dark Web: A Study, New York: United Nations, UNODA Occasional Papers, October 2018.
Timothy Lloyd, "Leaked Police Docs Reveal Crypto's Role in Dark Web Bioweapons Trade," Decrypt, July 16, 2020.
The standard source is MauriceMatloff, ed., American Military History, Vol. 1, 1775-1902, Conshohocken, Pa.: Combined Books, 1996, pp. 309-316.  
  9  Brian Linn, The Philippine War, 1899-1902, Lawrence, Kan.: University Press of Kansas, 2000; Graham Cosmas, An Army for Empire: The United States Army in the Spanish-American War, Harrisburg, Pa.: WhiteMane, 1994; Birtle, 2001, pp. 147-174
.10  The standard account is Edward Coffman, The Regulars: The AmericanArmy, 1898-1941, Cambridge,  Mass.: Belknap,  
2004.   11  Francis Fukuyama, U.S.-Soviet Interactions in the Third World, Santa Monica, Calif.: RAND Corporation,OPS-004, 1985.   
For a recent comprehensive assessment along these lines, see MarekMadej, ed., Western Military Interventions  After the Cold War: Evaluating the Wars of the West, London, United Kingdom: Routledge, 2020.
  27  Angel Rabasa, Steven Boraz, Peter Chalk, Kim Cragin, Theodore W. Karasik, Jennifer D. P. Moroney, Kevin A. O'Brien, John E. Peters, Ungoverned Territories: Understanding and Reducing Terrorism Risks, SantaMonica, Calif.: RAND Corporation, MG-561-AF, 2007.   
Aaron Gregg, "Pentagon Shaves $6.5  Billion by Selling Obsolete Equipment, Overhauling Bureaucracy," WashingtonPost, February 14, 2020.   
This section borrows heavily from unpublished research and writing conducted for the Department of the Air Force on improving the provision of assistance to African partner air arms.
Population Reference Bureau, 2016 World Population Data Sheet, Washington, D.C., 2016.   
Adam R. Grissom, "Shoulder to Shoulder Fighting Different Wars: NATO Advisors and Military Adaptation in the Afghan NationalArmy," in Theo Farrell and James Russell, eds., Military Adaptation in  Afghanistan, Palo Alto, Calif.: Stanford University, 2013b.   
Malte Brosig, Cooperative Peacekeeping in Africa: Exploring Regime Complexity, London, United Kingdom: Routledge, 2015.
  116  For a particularly powerful discussion of this link, see Ben Connable, Embracing the Fog of War: Assessment and Metrics in Counterinsurgency, Santa Monica, Calif.: RAND Corporation, MG-1086-DOD, 2012.
Krasner and Risse, 2014, p.  
They define these concepts as follows:• "Consolidated states possess the ability to authoritatively make, implement, and enforce central decisions for a collectivity" • "Limited statehood concerns those areas of a country in which central authorities (governments) lack the ability to implement and enforce rules and decisions and/or in which the legitimate monopoly over the means of violence is lacking. The ability to enforce rules or to control the means of violence can be differentiated along two dimensions: (1) territorial, that is, parts of a country's territorial space, and (2) sectoral, that is, with regard to specific policy areas" • "Failed or failing states are those that have more or less lost the state monopoly on the use of force and/or do not possess effective capacities to enforce decisions" (p. 549). 5 Commission on Global Governance, Our Global Neighbourhood: The Report of the Commission on Global Governance, Oxford, United Kingdom: Oxford University Press, 1995, p. 2. 6 Melissa M. Lee, Crippling Leviathan: How Foreign Subversion Weakens the State, Ithaca, N.Y.: Cornell University Press, 2020, p. 18.
Krasner and Risse, 2014;
Lee usefully distinguishes between state consolidation, the degree to which the state can govern evenly over its territory, and state institutionalization, "the power and strength of state administrative institutions"(Lee, 2020, p. 20).
Thomas Risse and Eric Stollenwerk, "Legitimacy in Areas of Limited Statehood," Annual Review ofPolitical Science, Vol. 21, No. 1, 2018.  
  13  Rivke Jaffe, "The Hybrid State: Crime and Citizenship in Urban Jamaica," American Ethnologist,Vol. 40,  No. 4,  
2013.
14
, p. 4).15  Disorder is a situation in which the rulers, the ruled, or both "fail to abide by a set of defined rules" (Ana Arjona, "Wartime Institutions: A Research Agenda," Journal of ConflictResolution, Vol. 58, No. 8, December 1, 2014, p. 1374).
Michael N. Barnett, "Humanitarian Governance," Annual Review of Political Science, Vol. 16, No. 1, 2013.    
The role of the state can vary widely in alternative governance arrangements, from no state provision to quite a great deal. The state just cannot be the exclusive governance provider, because then it would no longer qualify as alternative governance.
Although this section describes various nonstate actors that provide some form of governance, it is important to keep in mind that the alternative governance arrangement of a particular place or issue is constituted by the governance provided by all actors involved, including, at times, the state.
  24  Melani Cammett and Lauren M. MacLean, "The Political Consequences of Non-State Social Welfare: An Analytical Framework," in Melani Cammett and Lauren M. MacLean, eds., The Politics of Non-State Social Welfare, Ithaca, N.Y.: Cornell University Press, 2014.
Adrian Florea, "Rebel Governance in De Facto States," European Journal of International Relations,  Vol. 26, No. 4, December 1, 2020.   
See the classic analysis of Los Angeles in Mike Davis, City of Quartz: Excavating the Future in Los Angeles, New York: Vintage, 1992.
This often occurs in parallel with tax avoidance by many of the same people, which further restricts the state's capacity for governance. Gabriel Zucman estimates that while 8 percent of total global financial wealth is held offshore, in Latin America, 22 percent of financial wealth is moved out of view of national tax authorities; in Africa, that number is 30 percent; in Russia, 52 percent; and, in the Gulf countries, 57 percent(Gabriel Zucman, The Hidden Wealth of Nations: The Scourge of Tax Havens, trans. Teresa Lavender Fagan,  Chicago, Ill.: University of Chicago Press, 2015, p. 53). See also Néstor Castañeda, David Doyle, and Cassilde Schwartz, "Opting Out of the Social Contract: Tax Morale and Evasion," Comparative PoliticalStudies,  Vol. 53, No. 7, June 1, 2020.  
  47  Abrahamsen and Williams, 2009, p. 3. See,  for example, Teresa P. R. Caldeira, City of Walls: Crime,Segregation, and Citizenship in São Paulo, Berkeley, Calif.: University of California Press, 2001;
and Tessa G.
If the answer is that no one obeys any rules or regulations and no one receives goods and services, the place in question is ungoverned.
Justin Kelly and Mike Brennan, "OODA Versus ASDA: Metaphors at War," Australian ArmyJournal,  Vol. 6, No. 3, Summer  
2009; Head Modernisation and Strategic Planning-Army, Army's Future Land Operating Concept, Canberra, Australia: Australian Army Headquarters, 2009.   2  Interviewees consisted of experts that formerly served or currently serve in government positions, research laboratories, federally funded research and development centers, private industry, and academia. (A list of interviewees is provided at the end of this chapter.) All interviews were conducted under protocols that ensured that individual interviewee comments were presented as not for attribution. The specific comments of interviewees are anonymized in this chapter, though their individual contributions are acknowledged at its conclusion.
RAND Interview D6B0, December 2020.
RAND Interview E0A1, January 2020.
RAND Interview A5A8, November
  20  RAND Interview B6D1, September 2020.
We needed a better understanding of the network of who held power, who was best connected, who [were] the right people to leverage. How do you get information about local actors when you don't even have Peace Corps volunteers there, if it is a hostile or ungoverned space?59    In this case, the lack of consistent information about various political and military actors in Syria made it difficult for the United States to vet, understand, and trust irregular forces. Another policymaker noted the similar challenge posed by understanding criminal cartel55  RAND Interview B1A8, December 2020.56  RAND Interview E1C7, December 2020.57  RAND Interview C4D1, October 2020.58  RAND Interview C3C1, December 2020; RAND Interview D3A8, October 2020; RAND Interview A9D2, December 2020.
RAND Interview D4A2, November 2020; RAND Interview D7A7, November 2020; RAND Interview B4D0, July 2020.
U.S.Department of Defense, Quadrennial Defense Review Report, Washington, D.C., February 6,  
2006,  pp. 9-15.   
Center for Global Development, "Phase Zero: The Pentagon's Latest Big Idea," July 20, 2007; Lauren Fish, "Painting by Numbers: A History of the U.S. Military's Phasing Construct," War on the Rocks, November 1, 2016.
G. John Ikenberry, After Victory: Institutions, Strategic Restraint, and the Rebuilding of Order After Major Wars, Princeton, N.J.: Princeton University Press, 2000.
Such a formulation is certainly an incomplete one because the origins of the modern, liberal rules-based order were not exclusively determined by the enlightened self-interest of the United States, nor has the United States necessarily or unequivocally adhered to the bounds on its power.
Harry G. Summers, On Strategy: A Critical Analysis of the Vietnam War, New York: Presidio Press, 1995.
Brian Michael Jenkins, Why the North Vietnamese Keep Fighting, Santa Monica, Calif.: RAND Corporation, D-20153-ARPA/AGILE, April 9, 1970.   
Arthur K. Cebrowski and John J. Gartska, "Network-Centric Warfare: Its Origin and Future," U.S. NavalInstitute, Vol. 124, 1998.   
James M. Sands, Parimal J. Patel, Peter G. Dehmer, Alex J. Hsieh, and Mary C. Boyce, "Protecting the Future Force: Transparent Materials Safeguard theArmy's Vision," AMPTIAC Newsletter, Vol. 8, No. 4,  2004.    
Michael D.  Rettig and Whitney Grespin "The Spaces in Between: Mitigating Threats in Undergoverned Spaces," Small Wars Journal,
October 17, 2013.
Nathan Bennett and G. James Lemoine, "What VUCA Really Means for You," Harvard Business Review,  Vol. 92, Nos. 1-2, February 1, 2014.    
Robert Coram, Boyd: The Fighter Pilot Who Changed the Art of War, New York: Hachette Book Group,  2002.    
Justin Kelly and Mike Brennan, "OODA Versus ASDA: Metaphors at War," Australian ArmyJournal,  Vol. 6, No. 3, Summer 2009.   
I will often use the term social science as shorthand to refer to scientific inquiry into a range of social, behavioral, political, and economic sciences.
National Research Council, Investing in Research Infrastructure in the Behavioral and Social Sciences, Washington, D.C.: National Academies Press, 1998; R. Duncan Luce, Neil J. Smelser, and Dean R. Gerstein, eds., Leading Edges in Social and Behavioral Science, New York: Russell Sage Foundation, 1990.
National Science Foundation, Bridging the Gap: Building a Sustained Approach to Mid-Scale Research Infrastructure and Cyberinfrastructure at NSF, Washington, D.C., October 1, 2018.
Rebecca R.Thompson, E. Alison Holman, and Roxane Cohen Silver, "Media Coverage, Forecasted Posttraumatic Stress Symptoms, and Psychological  Responses Before and After an Approaching Hurricane," JAMA NetworkOpen, Vol. 2, No. 1, January 4, 2019.  
  21  Health and Retirement Study, homepage, undated; Institute for Social Research, Survey Research Center, "Panel Study of Income Dynamics," webpage, undated-a; Institute for Social Research, Survey Research Center, "PSID Sponsors," webpage, undated-b.
Susan D. Hyde, "Experiments in International Relations: Lab, Survey, and Field," Annual Review ofPolitical Science, Vol. 18, No. 1, 2015.  
  24  Courtney Kennedy, Nick Hatley, Arnold Lau, Andrew Mercer, Scott Keeter, Joshua Ferno, and Dorene Asare-Marfo, "Assessing the Risks to Online Polls from Bogus Respondents," Washington, D.C.: Pew Research Center,
February 18, 2020.
Phil Sharp and Susan Hockfield, "Convergence: The Future of Health," Science,Vol. 355, No. 6325, February 10, 2017.   
Sarah A. Nowak, Luke Joseph Matthews, and
Sarah A. Nowak and Andrew M. Parker, "Social Network Effects of Nonlifesaving Early-Stage Breast  Cancer Detection on Mammography Rates," American Journal of Public Health, Vol. 104, No. 12, 2014.    
Inter-university Consortium for Political and Social Research, "Data Management & Curation," webpage, undated.
Irving Louis Horowitz, "The Life and Death of Project Camelot," American Psychologist, Vol.
21, No. 5,  1966; Robert A. Nisbet, "Project Camelot: An Autopsy," Public Interest, Vol. 5, Fall 1966; Mark Solovey,  "Project  Camelot and the 1960s Epistemological Revolution: Rethinking the Politics-Patronage-Social Science Nexus," Social Studies ofScience, Vol. 31, No. 2, April 1, 2001.   2  Funding for social science research at the National Science Foundation (NSF) has been a very small slice of NSF's overall budget for research, limiting the extent to which expertise resides within the U.S. government's cadre of research sponsors, managers, and evaluators. From the 1960s to the 2000s, NSF social science research obligations as a percentage of NSF total research obligations hovered between 3.2 percent and 7.4 percent, with an average of 4.8 percent. Funding data from 1960 to 1991 are drawn from Otto N.Larsen,  Milestones and Millstones: Social Science at the National Science Foundation, 1945-1991, New York: Routledge, 1992. Funding data from 1991 to 2010 are drawn from NSF's fiscal year budget requests (see National Science Foundation, "Budget Internet Information System," webpage, last updated October 2020).
Robert Popp, Stephen H. Kaisler, David Allen, Claudio Cioffi-Revilla, Kathleen M. Carley, Mohammed  Azam, Anne Russell, Nazli Choucri, and  Jacek Kugler, "Assessing Nation-State Instability and Failure," in 2006 IEEE Aerospace Conference, Big Sky, Mont.: IEEE, 2006; Steven C. Bankes, Robert J. Lempert, and Steven W. Popper, Pre-Conflict Anticipation and Shaping (PCAS): Models-2-Shaping Integration, Topanga, Calif.: Evolving Logic, September 2005.
I have deliberately used quotation marks here to emphasize how difficult it has been to define culture and how differently it has been defined and measured across the social sciences.
The pursuit of generalized knowledge of social systems is referred to as nomothetic research, while findings that are contingent or applicable to a specific case are referred to as ideographic. For discussions, see Douglas V. Porpora, "On the Prospects for a Nomothetic Theory of Social Structure," Journal for the Theory of SocialBehaviour, Vol. 13, No. 3, 1983;
Joseph M. Bryant
Many studies attributed as social science research are done by non-social scientists. Some of what nonsocial scientists consider social science is really journalism or political commentary.
Important developments in this direction, however, include recent efforts by DARPA that have made social science processes, models, and outputs the objects of study. Notable programs in this regard are Next-Generation Social Science, Ground Truth, and Systematizing Confidence in Open Research and Evidence (SCORE). See DARPA, "DARPA Next Generation Social Science," webpage, undated-a; DARPA, "Putting Social Science Modeling Through Its Paces,"
April 7, 2017;
 13  Again, DARPA's SCORE program is a notable exception with its efforts to develop tools to assist in determining whether research itself can be reproduced or replicated and whether scientific findings might be regarded as reliable. SeeAdam Rogers, "Darpa  Wants to Build a BS Detector for Science," Wired, July 30, 2017; Rajesh Uppal, "DARPA SCORE Program Aims to Develop Automated Tools to Score Social and Behavioral Research Important for National Security," International Defense Security & Technology, August 20, 2019; and Yang Yang, Wu Youyou, and Brian Uzzi, "Estimating the Deep Replicability of Scientific Findings Using Human and Artificial Intelligence," Proceedings of the National Academy of Sciences,Vol. 117, No. 20, May 19, 2020.   
For a canonical discussion, see Stuart J. Russell and Peter Norvig, Artificial Intelligence: A Modern Approach,
2nd ed., Upper Saddle River, N.J.: PearsonEducation, Inc., 2005, Ch. 13.   
Russell and Norvig, 2005, pp. 519-522.    
See Raymond Reiter, "A Logic for Default Reasoning," Artificial Intelligence, Vol. 13, Nos.1-2, 1980; and  David Poole, "A Logical Framework for Default Reasoning," Artificial Intelligence, Vol. 36, No. 1, 1988.   
Micah Zenko, "Millennium Challenge:  The Real Story of a Corrupted Military Exercise and Its Legacy," War on the Rocks,
November 15, 2015.
Herbert R. McMaster, Crack in the Foundation: Defense Transformation and the Underlying Assumption of Dominant Knowledge in Future War, Carlisle Barracks, Penn.: Army War College, 2003.
Geist and Blumenthal, 2019.
White House, National Security Strategy of the United States of America, Washington, D.C.,
This chapter will shift across the components of the NSE, from policy deliberations in the U.S. Department of State, through military planning in the services and U.S. Department of Defense (DoD), to the offices involved in intelligence-gathering and all its forms. One of its central theses is the need for integrated assessment, planning, and implementation across these functions.
Burton R. Clark, "Organizational Adaptation and Precarious Values:A Case Study," American Sociological Review, Vol. 21, No. 3, 1956.   
The term deep uncertainty may refer to conditions under which we do not know (or cannot agree on) how best to characterize uncertainty about future values of key variables (parametric uncertainty) or the nature of causal mechanisms (structural uncertainty). Added to this is a normative component in which we do not know or cannot agree on the appropriate criteria, limit values, or priorities for assessing how well outcomes perform in achieving our goals.
Derived from Lewis Carroll's Through the Looking Glass, a Red Queen's race is one in which "it takes all the running you can do to keep in the same place"-furious activity leads to no real advancement.
Herbert A. Simon, "Rational  Choice and the Structure of the Environment," Psychological Review, Vol.63,  No. 2, 1956.    
Note that a zero-regret strategy does not necessarily ensure a good outcome. That is, although the rank order of preference among available alternatives may lead to a preferred candidate relatively, it may be that
Decision Making Under Deep Uncertainty Society, "DMDU Society," webpage, undated.
Vincent A. W. J. Marchau, Warren E. Walker, Pieter J. T. M. Bloemen, and Steven W. Popper, eds., Decision Making Under Deep Uncertainty: From Theory to Practice, Cham, Switzerland: Springer, 2019.
  12 Rosenhead, Elton, and Gupta, 1972.   
Robert J. Lempert, "Robust Decision Making (RDM)," in Vincent A. W. J. Marchau, Warren E. Walker, Pieter J. T. M. Bloemen, and Steven W. Popper, Decision Making Under Deep Uncertainty: From Theory to Practice, Cham: Springer International Publishing, 2019.
Horst W. J. Rittel and Melvin M. Webber, "Dilemmas in a General Theory of Planning," Policy Sciences,Vol. 4, No. 2, June 1, 1973.    
National Research Council, Understanding Risk: Informing Decisions in a Democratic Society, Washington, D.C.: The National Academies Press, 1996.
  17  Steven W. Popper, Robert J. Lempert, and Steven C. Bankes, "Shaping the Future," Scientific American,Vol. 292, No. 4, April 1,  
2005. 18 Nidhi Kalra, Stéphane Hallegatte, Robert Lempert, Casey Brown, Adrian Fozzard, Stuart Gill, and Ankur Shah, Agreeing on Robust Decisions: New Processes for Decision Making Under Deep Uncertainty, Washington, D.C.: World Bank, June 2014.
Steven C. Bankes, "Exploratory Modeling for Policy Analysis," Operations Research, Vol. 41, No. 3, June 1,  1993.    
Jerome H.Friedman and Nicholas I. Fisher, "Bump Hunting in High-Dimensional Data," Statistics and  Computing, Vol. 9, No. 2, April 1, 1999.    
Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone, Classification and Regression Trees, Boca Raton, Fla.: Routledge, 1984.  
  27  SiddharthaDalal, B. Han, Robert Lempert, A. Jaycocks, and A. Hackbarth, "Improving Scenario Discovery Using Orthogonal Rotations," Environmental Modelling & Software, Vol. 48, October 1, 2013.   
A senior former State Department official once related to the author that the only times during his career that he attended a briefing for which the room's lighting was dimmed so that overhead or projected slides could be shown was when he was visiting in the Pentagon.
Paul K. Davis and Steven W. Popper, "Confronting Model Uncertainty in Policy Analysis for Complex Systems: What Policymakers Should Demand, " Journal on Policy and Complex Systems, Vol. 5, No. 2, Fall 2019.
  3  For fuller development and application of this approach, see Steven W. Popper, "Robust Decision Making and Scenario Discovery in the Absence of Formal Models," Futures & Foresight Science, Vol.1, No. 3-4, 2019.   
White House, National Security Strategy of the United States of America, Washington, D.C., December 2017.
Melissa M. Lee, "Subversive Statecraft: The Changing Face of Great-Power Conflict," Foreign Affairs, December 4,
2 Yanzhong Huang and Joshua Kurlantzick, "China's Approach to Global Governance," The Diplomat,
  3  Michael Schuman, "Keep an Eye on Taiwan," The Atlantic,
October 10, 2020;
Thomas Rid, Active Measures: The Secret History of Disinformation and Political Warfare, New York:  Farrar, Straus and Giroux, 2020, pp. 17-33.    
Adam B. Ellick and Adam Westbrook, "Operation Infektion: Russian Disinformation: From Cold War to Kanye, a 3-Part Video Series," New York Times, 2018.
U.S. Department of Justice, Report on the Investigation into Russian Interference in the 2016 Presidential Election, Vol. I, Washington, D.C., 2019.
Herbert R. McMaster, Battlegrounds: The Fight to Defend the Free World, New York: Harper, 2020.    
Herbert R.  McMaster cites an old Russian joke about a Russian farmer with a single cow. If granted one request by the Russian equivalent of a genie, the farmer's foremost wish is the death of his neighbor's second cow. Tearing others down, then, can be an objective in itself (seeMcMaster, 2020, p. 40).
Henry Farrell and Bruce Schneier, Common-Knowledge Attacks on Democracy, Cambridge, Mass.: Berkman Klein Center, Harvard University, 2018.
  12  George F. Kennan, "Policy Planning Staff Memorandum," Washington, D.C.: U.S.Department of State,  No. 269, 1948.   
Christopher Paul, "Confessions of a Hybrid Warfare Skeptic," Small Wars Journal,March 3,  
In contrast, organizing around priorities can shortchange anything not on the priority list driven by headlines and recent events.
Paul K. Davis, David Gompert, and Richard Kugler,  Adaptiveness in National Defense: The Basis of a New Framework, Santa Monica, Calif.: RAND Corporation,IP-155, 1996.   
Digital copies of Boyd's famous six-hour briefing "Patterns of Conflict," circa 1986, can be found online. For a discussion applying this concept to the business world, see Chet Richards, Certain to Win: The Strategy ofJohn Boyd, Applied to Business, Bloomington, Ind.: Xlibris US, 2004.  
  26  Ron Kohavi and Stefan Theme, "The Surprising Power of Online Experiments," Harvard Business Review, October 2017.
There is need for social science relating to intervention operations that go beyond aloof quantitative analysis of aggregate data and get deeply into the system, as with field work or detailed case studies. For a
Davis, Gompert, and Kugler, 1996;
  37  This is possible if top-level objectives are critical components, in which case a threshold level of effectiveness must be achieved for each. Stoplight charts can then quickly convey why a given option failed: Any option with even one failed component (shown in red)
fails.38  Deep uncertainty exists "when the parties to a decision do not know or do not agree on the system model(s) relating actions to consequences or the prior probability distributions for the key input parameters to those
Christopher Paul, "How Do  Terrorists Generate and Maintain Support?" in Paul K. Davis and Kim Cragin, eds., Social Science for Counterterrorism: Putting the Pieces Together, Santa Monica, Calif.: RAND Corporation, MG-849-OSD, 2009.
Paul K. Davis, Eric V. Larson, Zachary Haldeman, Mustafa Oguz, and
These relate to what J. L. Mackie called INUS conditions when discussing causality. The acronym stands for insufficient but nonredundant part of an unnecessary but sufficient condition. A brief discussion appears in JRank Science & Philosophy, "Causality: Inus Conditions," webpage, undated.
National Academies of Science Engineering and Medicine, Communicating Science Effectively: A Research Agenda, Washington, D.C.: National Academies Press,
2017; Dietram A. Scheufele, "Science Communication as Political Communication," Proceedings of the National Academy of Sciences, Vol. 111, 2014; and Baruch Fischhoff, "The Sciences of Science Communication," Proceedings of the National Academy of Sciences, Vol. 110, 2013. 2 Lawrence Frey, Carl H. Botan, and Gary Kreps, Investigating Communication, New York: Allyn & Bacon, 2000; Thomas R. Lindlof and Bryan C. Taylor, Qualitative Communication Research Methods, Thousand Oaks, Calif.: Sage, 2017; Werner J. Severin and James W. Tankard, Communication Theories: Origins, Methods, and Uses in the Mass Media, New York: Longman, 1997; and David M. Boje, Narrative Methods for Organizational & Communication Research, Thousand Oaks, Calif.: Sage, 2001. 3 Ida Nadia S. Djenontin and Alison M. Meadow, "The Art of Co-Production of Knowledge in Environmental Sciences and Management: Lessons from International Practice," Environmental Management, Vol. 61, 2018.
Lori Peek, Jennifer Tobin, Rachel M. Adams, Haorui Wu, and  Mason Clay Mathews, "A Framework for Convergence Research in the Hazards and Disaster Field: The Natural Hazards Engineering Research Infrastructure CONVERGE Facility," Frontiers in Built Environment, Vol. 6, July 2020.
James D. Fearon, "Deliberation as Discussion," in J. Elster, ed., Deliberative Democracy, Cambridge, Mass.: Cambridge University Press, 1998.
David C. Gompert, Hans Binnendijk, and Bonny Lin, Blinders, Blunders, and Wars: What America and  China Can Learn, Santa Monica, Calif.: RAND Corporation, RR-768-RC, 2014;
Demir, 2017.
For more, see Xialing Lin, Patric R. Spence, and Kenneth A. Lachlan, "Social Media and Credibility Indicators: The Effect of Influence Cues," Computers in HumanBehavior, Vol. 63, 2016.  
  30  Joseph S. Dumas and Beth A. Loring, Moderating Usability Tests: Principles and Practices for Interacting, Amsterdam, Netherlands: Elsevier,
2008.31  Todd Helmus, "Social Media and Influence Operations Technologies: Implications for Great Power Competition," in Thomas F. Lynch III, ed., Strategic Assessment 2020: Into a New Era of Great Power Competition, Washington, D.C., National Defense University, 2020.
Nancy Roberts, "Wicked Problems and Network Approaches to Resolution," International Public Management Review, Vol. 1, No. 1, 2000.   
Annette Boaz, Stephen Hanney, Robert Borst, Alison O'Shea, and Maarten Kok, "How to Engage Stakeholders in Research: Design Principles to Support Improvement," Health ResearchPolicy and Systems,  Vol. 16, No. 1, 2018.   
Horst W. J. Rittel and Melvin M. Webber, "Dilemmas in a General Theory of Planning," Policy Sciences, Vol.4, No. 2, June 1, 1973; Head and Alford, 2015.    
National Research Council, 2008; Thomas Webler and Seth Tuler, "Four Decades of Public Participation  in Risk Decision Making," Risk Analysis, Vol. 41, No. 3, 2018.   
Adriana de Souza e Silva and Daniel M. Sutko, eds., Digital Cityscapes: Merging Digital and UrbanPlayspaces, Bern, Switzerland: Peter Lang, 2009.  
  81  Mark Roman Miller, Fernanda Herrera, Hanseul Jun, James A. Landay, and Jeremy N. Bailenson, "Personal Identifiability of User Tracking Data During Observation of 360-Degree VR Video," Scientific Reports, Vol.10, No. 1, 2020.   
Robert J. Lempert, "Scenarios That Illuminate Vulnerabilities and Robust Responses," Climatic Change,  Vol. 117, No. 4, 2013.   
When then-Vice Chief of Staff GEN James McConville emphasized this point, he used the following language: "Winning matters. . . . When we send the United States Army somewhere, we don't go to participate, we don't go to try hard. We go to win. That is extremely important because there's no second place or honorable mention in combat" (Sean Kimmons, "New Chief of Staff: Taking Care of People Key to Winning the Fight," U.S.Army, October 8, 2019).
  3  Center for Army Lessons Learned, MDMP: Lessons and Best Practices, Leavenworth, Kan.: U.S.Army,  Handbook No. 15-06, March 2015
.4  TAA and many other processes are described in detail and regularly updated in U.S. Army War College, How the Army Runs:A Senior Leader Reference Handbook, 2019-2020, Carlisle, Pa.,  
2020.5  Brian W. Head and John Alford, "Wicked Problems: Implications for Public Policy and Management," Administration & Society, Vol
. 47, No. 6, 2015.   
Rodney D. Fogg, "From the Big Five to Cross Functional Teams: Integrating Sustainment into Modernization," U.S.Army, November 4, 2019.  
  7  The creation of Army Futures Command is an example; however, even within the U.S. Army, there are multiple commands that are relevant stakeholders.
On the operational side, similar limitations can be seen moving up from somewhat constrained tactical scenarios toward operational and strategic concerns, such as larger-scale operations or decisions with greater political degrees of freedom. Both the generating and operating examples highlight the inherent challenges of modeling problems with fewer and fewer constraints, but the application of these techniques on the generating side is a much greener field to tackle.
A digital twin of an organization is "a dynamic software model of any organization that integrates operational and contextual data to understand how an organization operationalizes its business model, connects with its current state, responds to changes, deploys resources and delivers customer value" (Gartner, "Quick Answer: What Is a Digital Twin of an Organization?" webpage,
July 29, 2021)
  10  Werner Kritzinger, Matthias Karner, Georg Traar, Jan Henjes, and Wilfried Sihn, "Digital Twin in Manufacturing: A Categorical Literature Review and Classification," 16th IFAC Symposium on Information Control Problems inManufacturing INCOM 2018, Vol.
 51, No. 11, 2018.   
Khari Johnson, "UiPath Acquires ProcessGold and StepShot to Help Businesses Identify RPA Opportunities," VentureBeat,
October 15, 2019.
For an example of process, the recently revised DoDD 5000.01 calls for "deliver[ing] performance at the speed of relevance" (DoDD 5000.01, The Defense Acquisition System, Washington, D.C.: U.S. Department of Defense,September 9, 2020, p. 4).
Nicholas Thompson, "Inside the Apocalyptic Soviet Doomsday Machine," Wired, September 21, 2009; Paul Scharre, Army of None: Autonomous Weapons and the Future of War, New York: W. W. Norton & Company, 2018; P. W. Singer and August Cole, Ghost Fleet: A Novel of the Next World War, Boston, Mass.: Eamon Dolan/Houghton Mifflin Harcourt, 2015.
Jackson Barnett, "AI Needs Humans 'on the Loop' Not 'in the Loop' for Nuke Detection, General Says," FedScoop,February 14, 2020.    
Such a model-including an early iteration at a higher level of abstraction-could augment Hedgemony, which, much like the actual PPBE process, is intended for human players only.
Andrew Krepinevich and Barry Watts, The Last Warrior: Andrew Marshall and the Shaping of Modern American Defense Strategy, New York: Basic Books, 2015; and Thomas G. Mahnken, ed., Competitive Strategies for the 21st Century: Theory, History, and Practice, Palo Alto, Calif.: Stanford University Press, 2012.
An alternative counterargument that strategic objectives are simply too complicated to optimize is of similar merit, but we defer a lengthier treatment of this question with the compromise assumptions that these objectives can be modeled at different resolutions and that an appropriate level that would provide the utility outlined in this chapter is within the art of the possible.
Cornelius Chang and Robin Groeneveld, "Slowing Down to Speed Up," McKinsey Organization Blog,
March 23, 2018.
Jocelyn R. Davis and Tom Atkinson, "Need Speed? Slow Down," Harvard Business Review,
May 1, 2010.
Assuming that there was a level of transparency and interpretability in the output and operational conditions that afforded the time to do so, human beings could still leverage the outputs for the same deliberative consensus-building they were used to with an expanded set of options and autogenerated logic.
Louis Coppey, "What Does AlphaGo vs Lee Sedol Tell Us About the Interaction Between Humans and Intelligent Systems?" Medium,
March 15, 2018.
The focus of DMN is better integration between operational decision points and business processes, with an acknowledgment that strategic and tactical decisions might not fit as well in the envisioned use cases. However, when the processes themselves are the development and execution of strategic planning, the line between strategic and tactical becomes a little blurry.
Thierry Biard, Alexandre Le Mauff, Michel Bigand, and Jean-Pierre Bourey, "Separation of Decision Modeling from Business Process Modeling Using New 'Decision Model and Notation' (DMN) for Automating Operational Decision-Making," in Luis M. Camarinha-Matos, Frédérick Bénaben, and Willy Picard, eds., Risks and Resilience of Collaborative Networks, Cham, Switzerland: Springer International Publishing, 2015.
Constantin Cronrath, Abolfazi R. Aderiani, and Bengt Lennartson, "Enhancing Digital Twins Through Reinforcement Learning," 2019 IEEE 15th International Conference on Automation Science and Engineering (CASE), Vancouver, Canada, August 2019.
If making strategic decisionmaking machine-readable and -auditable narrows or eliminates that gap, great care would need to be given to securing the entire apparatus because it could very easily be turned against the United States. U.S. adversaries and competitors are no doubt considering building twins of the United States, and it does not want to inadvertently help them do so should there be some unintended or unauthorized access gained (e.g., the Office of Personnel Management data breach).
Peter W. Singer, "In the Loop? Armed Robots and the Future of War," Brookings Institution,
January 28, 2009.
Competition is increasing for early shaping of military events, with or without a digital twin. For example, see Kristin Huang, "As China's Military Confidence Grows, It's Now Looking to 'Design' How War Is Fought," South ChinaMorning Post, November 13, 2020.   
NATO, Research and Technology Organization, Human Behavior Representation in Constructive Simulation, Neuilly-sur-Seine Cedex, France,  
September 2009, p. ES-1.   
Justin Kelly and Mike Brennan, "OODA Versus ASDA: Metaphors at War," Australian ArmyJournal,  Vol. 6, No.  
3, Summer 2009. 3 Philip J. Root, "Rethinking Competition," briefing, Defense Advanced Research Projects Agency, June 30, 2020.
Kelly and  Brennan specifically state that ASDA does not propose a solution to the challenges identified in the introduction-the four elements necessary to forecast human behavior with sufficient accuracy to be useful. SeeKelly and Brennan, 2009, p. 40. Root does not center his argument on forecasting. However, Root's incorporation of ASDA into a proposed plan for understanding complex human behavior in UGS is inherently a call for forecasting: Only historically informed but forward-looking ex ante forecasting analysis effectively supports policy decisionmaking, particularly when the policy challenge is designed around a rapidly shifting (as opposed to primarily historically informed) series of competition problems
.8  This chapter cites a few cases (e.g., Performance Moderated Functions Server[PMFServ]) in which modelers attempted to create an authentic human model and simulation. None of these has been incorporated into constructive or training simulations in large-scale use by the U.S. military or Western allies. Considerable other work was done on human behavioral modeling for both military and nonmilitary uses in primarily the 1990s and 2000s. Some modeling pursued authenticity, but most of the efforts pursued realism. Such models as the Adaptive Control of Thought-Rational (ACT-R) model and the Strengths, Opportunities, Aspirations, and Results (SOAR) model are cited.For  other examples, see Richard W. Pew and Anne S. Mavor, eds., Modeling Human and Organizational Behavior: Application to Military Simulations, Washington, D.C.: National Academies Press, 1998; and, more recently, Simon Farrell and Stephen Lewandowsky, Computational Modeling of Cognition and Behavior, New York: Cambridge University
Press, 2018.9  For a primer on precision and accuracy, seePaul Humphreys, Extending Ourselves: Computational Science, Empiricism, and Scientific Method, New York: Oxford  
University Press, 2004.   10  General Research Corporation, Operations Analysis Division, CARMONETTE, Vol. I, GeneralDescription, technical manual, McLean, Va., November 1974.   
These approaches are not mutually exclusive. They are categorized here to help describe the types of efforts observed over the past 17 years, both as seen in uniform and as seen while at RAND.
We have engaged with one non-U.S. organization that is taking a go-slow approach to large-scale constructive simulation, but that case is the exception.See Connable et al., 2018, Chapter 3, and specifically  p. 134, for reflections from interviews with simulation team designers referring to this process. In addition to conducting these interviews, our research team has engaged with multiple contract design teams over the past five years. Each of these teams has told us of their restraints and contractual constraints. In some cases, we have observed these issues firsthand as design consultants.
  25  See, for example, Jon Harper, "Army Spending Big on Training, Modeling, Simulation," National Defense, December 2, 2019; and U.S.Army, RDT&E Budget Item Justification, Synthetic Training Environment Refinement and Prototyping, Washington, D.C., March 2019. This Army budget document shows that the Army's Synthetic Training Environment contract for simulation is worth over $232,000,
000.26  In many cases, design teams are pressured to deliver a technology demonstrator showing basic behaviors within the first year of development. This forces the team to make a series of rapid and generally irrevers-
Connable et al., 2018, p. 134. Another troubling barrier is verification, validation, and accreditation (VVA). Government-funded simulation efforts are often required to undergo VVA before they can be certified for use, but human behavior representation is extraordinarily hard to verify or validate using the same kinds of technical approaches used for the VVA of, for example, a model and simulation of a tank engine.
  28  Examples of big sims are JCATS, JWARS, OneSAF, COMBAT XXI, WARSIM, STORM, and
JICM.29  Joseph S. McDonnell, "Distributed Soldier Representation: M&S Representations of the Human Dimensions of the Soldier," presentation, NDIA Annual Systems Engineering Conference 2015, Orlando, Fla.: U.S. Army Research Laboratory, October 26
-29, 2015.30  Examples of theoretical models are Guillaume Deffuant, David Neau, Frederic Amblard, and Gérard Weisbuch, "Mixing Beliefs Among Interacting Agents," Advances in Complex Systems, Vol. 3, No. 1, January 2000; Noah E. Friedkin and Eugene C. Johnsen, Social Influence Network Theory: A Sociological Examina-
DARPA has previously sponsored work in this area, and our RAND colleagues have provided insight through DARPA-funded and other government-sponsored research. See, for example, Paul K. Davis, Angela O'Mahony, Timothy R. Gulden, Osonde A. Osoba, and Katharine Sieck, Priority Challenges for Social and Behavioral Research and Its Modeling, Santa Monica, Calif.: RAND Corporation, RR-2208-DARPA, 2018; and Paul K. Davis, Angela O'Mahony, and Jonathan Pfautz, eds., Social-Behavioral Modeling for Complex Systems, Hoboken, N.J.: John Wiley & Sons, 2019.
Having participated in tens of conferences, working groups, and symposiums on irregular warfare and conflict area analysis and simulation, I heard some version of this phrase from several people in a significant majority of these events held from 2010 through 2019.
For example, see Alexander Kott and Peter S. Corpac, COMPOEX Technology to Assist Leaders in Planning and Executing Campaigns in Complex Operational Environments, Arlington, Va.: Defense Advanced Research Projects Agency, 2007.
For instance, see W. Brian Arthur, "Inductive Reasoning and Bounded Rationality," American EconomicReview, Vol. 84, No. 2, May 1994.    
For example, see John H. Holland and John H. Miller, "Artificial Adaptive Agents in Economic Theory,"  American Economic Review, Vol. 81, No. 2, May 1991.   
P.Hogeweg and B. Hesper," Mathematical and Computer  Modelling, Vol. 13, No.  
6, 1990.   
Joshua M. Epstein and Robert Axtell, Growing Artificial Societies: Social Science from the Bottom Up, Cambridge, Mass.:MIT Press, 1996.   
Matthew O. Jackson, Social and Economic Networks, Princeton, N.J.: Princeton University Press, 2008.   
Herbert A. Simon and Charles P. Bonini, "The Size Distribution of Business Firms," American EconomicReview, Vol. 48, No. 4, September 1958.   
For example, see Stephen Lewis Scott, Computational Modeling for Marine Resource Management, dissertation, Fairfax, Va.: George Mason University, 2016.
Martin A. Nowak and Robert M. May, "Evolutionary Games and Spatial Chaos," Nature, Vol. 359, October 29, 1992.  
  47  Bernardo A. Huberman and Natalie S. Glance, "Evolutionary Games and Computer Simulations," Proceedings of the National Academy ofSciences, Vol. 90, No. 16, August 1993.   
Robert Jervis, "Cooperation Under the Security Dilemma," World Politics, Vol. 30, No. 2, January 1978; Andrew Kydd, "Game Theory and the Spiral Model," World Politics, Vol. 49, No. 3, April 1997; F. Warren McFarlan, "Information Technology Changes the Way You Compete," Harvard Business Review, Vol. 62,
May 1984;
Sendhil Mullainathan and Richard H. Thaler, "Behavioral Economics," Cambridge, Mass.: National Bureau of Economic Research, NBER Working Paper 7948, October 2000.
In this context, adversarial attacks are attacks that manipulate inputs to algorithms to affect outputs of computations.
Sanjeev Arora and Boaz Barak, Computational Complexity: A Modern Approach, New York: Cambridge University Press, 2009.
James D. Fearon, "Signaling Foreign Policy Interests: Tying Hands Versus Sinking Costs," Journal of  Conflict Resolution, Vol. 41, No. 1, February 1997.    
Andrew Kydd, "Which Side Are You On? Bias, Credibility, and Mediation," American Journal of Political  Science, Vol. 47, No. 4, October 2003.    
John C. Harsanyi and Reinhard Selten, A General Theory of Equilibrium Selection in Games, Cambridge, Mass.:MIT Press, 1988.   
J.Stephen Lansing, "Complex Adaptive Systems," Annual Review of Anthropology, Vol. 32, No. 1, October  2003.    
Simon A. Levin, "Preface," in Simon A. Levin, ed., The Princeton Guide to Ecology, Princeton, N.J.: Princeton UniversityPress, 2009, p. vii.  
  7  W. Brian Arthur, "Foundations of Complexity Economics," Nature Reviews Physics, Vol. 3, No. 2, February 2021, p.  
136.8  Thomas Hobbes, Hobbes: Leviathan: Revised Student Edition, Richard Tuck, ed., New York: Cambridge  University Press, 1996.   
In this context, resilience can be defined simply as the ability to remain in the game regardless of what competitors might do, what new actors might appear, or how the payoffs of interactions themselves might change.
  21  Paul Klenerman, The Immune System: A Very Short Introduction, New York: Oxford University Press,
2017.22  Claude Combes, The Art of Being a Parasite, trans. Daniel Simberloff, Chicago, Ill.: University of ChicagoPress, 2005, pp. 12-15.   
Klenerman, 2017, p. 28.   
The fourth industrial revolution generally refers to the entrance of AI, machine learning, the Internet of Things, and other adaptive technologies into industrial and economic use. See Klaus Schwab, "The Fourth Industrial Revolution: What It Means, How to Respond," World Economic Forum,
January 14, 2016;
For examples, see Axelle Devaux, "How to Contain the Disinformation Virus," RAND Blog, April 9, 2020; Miriam Matthews, Katya Migacheva, and Ryan Andrew Brown, Superspreaders of Malign and Subversive Information on COVID-19: Russian and Chinese Efforts Targeting the United States, Santa Monica, Calif.: RAND Corporation, RR-A112-11, 2021; and Peter W. Singer and Eric B. Johnson, "The Need to Inoculate Military Servicemembers Against Information Threats: The Case for Digital Literacy Training for the Force," War on the Rocks, February 1, 2021
.28  This argument fits within a broader understanding of research on biology and the energetic trade space that prioritizes the commitment of resources for critical functions, maintenance, and growth. See
C. Le Page, P. Génin, M. G. Baines, and J. Hiscott, "Interferon Activation and Innate Immunity," Reviews inImmunogenetics, Vol. 2, No. 3, 2000.    
Philippa Marrack, James Scott-Browne, and Megan K. L. MacLeod, "Terminating the Immune Response," Immunological Reviews, Vol. 236, No. 1, July 2010.
David Cunningham, There's Something Happening Here: The New Left, the Klan, and FBI Counterintelligence, Berkeley, Calif.: University of California Press, 2004.   
Greg Witkop, "Systematizing Confidence in Open Research and Evidence (SCORE)," webpage, Defense Advanced Research Projects Agency, undated.
Seluanov et al., 2018.
Carl T. Bergstrom and Jevin D. West, Calling Bullshit: The Art of Skepticism in a Data-Driven World, New York: Random House, 2020.
Nicky Case, "How to Become a Centaur," Journal of Design and Science,
January 8, 2018.
These approaches also parallel two 19th-century approaches to wargaming, rigid kriegspiel and free kriegspiel.See Perla, 2012, pp. 42-45.    
John R.Emery, "Moral Choices Without Moral Language: 1950s  Political-Military Wargaming at the RAND Corporation," Texas National SecurityReview, Vol. 4, No. 4, Fall 2021.    
Brynen, 2016;
Yuna Huh Wong, Sebastian Joon Bae, Elizabeth M. Bartels, and Benjamin Smith, Next-Generation Wargaming for the U.S. Marine Corps: Recommended Courses of Action, Santa Monica, Calif.: RAND Corporation, RR-2227-USMC, 2019, pp. 40-43.
Stephen Downes-Martin, "Adjudication: The Diabolus in Machina of War Gaming," Naval War CollegeReview, Vol. 66, No. 3, Summer 2013.    
Daniel Kahneman and Gary Klein, "Conditions for Intuitive Expertise: A Failure to Disagree," American Psychologist, Vol.64, No. 6, 2009; and Gary Klein, Sources of Power: How People Make Decisions, Cambridge, Mass.: MIT Press, 1999.   
Lorin W. Anderson and David R. Krathwohl, eds., A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives, London, United Kingdom: Pearson, 2001.
Any project of this size and scope owes a debt of gratitude to many people. An enormous number of colleagues assisted us in developing the ideas discussed in this report, and there are several that we wish to acknowledge specifically.
First, we would like to thank 
Phil Root
Marta Kastens
DARPA
Edward Glaeser
Triumph of the City: How Our Greatest Invention Makes Us Richer, Smarter, Greener, Healthier, and Happier, New York
Penguin Press
John F. Padgett
Walter W. Powell
Princeton University Press
W. Brian Arthur
Economy, New York: Oxford University Press
Universal Laws of Growth, Innovation, Sustainability
Pace of Life, in Organisms, Cities, Economies
Jonathan S. Blake
Elisa Jayne Bienenstock
Elizabeth M. Bartels
Paul K. Davis
Bryan Rooney
Adam Russell
Zev Winkelman
Elizabeth Bartels
Paul Steinberg
Acknowledgments I would like to thank 
Elizabeth M. Bartels
Aaron B. Frank
Timothy Gulden
Angela O'Mahony
Dan Patt
Katie Hynes
Arwen Bicknell
Acknowledgments I'd like to thank 
Aaron Clark-Ginsberg
Nicholas Rush Smith
Aaron B. Frank
Elizabeth M. Bartels
We greatly appreciate the substantial contributions of the dozens of leaders and experts we interviewed from across the NSE. Their participation ultimately enabled this chapter to be written. We are also indebted to 
Aaron B. Frank
Elizabeth M. Bartels
I would like to thank 
Adam Russell
Elizabeth M. Bartels
I appreciate the opportunity offered to me by 
Aaron B. Frank
Kaleb McDowell
DARPA/DSO SCORE program
Phil Root
SCORE Team Lead
Amber Sprenger
The thoughts expressed here flow directly from a collection of long-term collaborations with numerous researchers but particularly 
Katherine Carman
Melissa Finucane
RAND
I would like to thank 
Dr. Jon-Philippe K. Hyatt
Aaron B. Frank
Elizabeth M. Bartels
RAND
Paul Steinberg
Gabrielle Tarini
I would like to thank the editors and reviewers of this report for their contribution to the development of this chapter.
The author expresses his gratitude to the 
Acquisition and Technology Policy program
RAND's National Defense Research Institute
Defense Advanced Research Projects Agency
Aaron B. Frank
Elizabeth M. Bartels
The authors thank the 
Acquisition and Technology Policy program of RAND's National Defense Research Institute
Defense Advanced Research Projects Agency
Aaron B. Frank
Elizabeth M. Bartels
I would like to thank the peer reviewers, 
Tim Gulden
Dan Patt
Emily Ward
Arwen Bicknell
Monette Velasco
Paul Steinberg
Aaron B. Frank
Elizabeth M. Bartels
Ben Connable
Michael J. McNerney
William Marcellino
Aaron Frank
Henry Hargrove
Marek N. Posard
S. Rebecca Zimmerman
Natasha Lander
Jasen J. Castillo
James Sladden
Will to Fight of Military Units, Santa Monica, Calif
Michael J. McNerney
Ben Connable
S. Rebecca Zimmerman
Natasha Lander
Marek N. Posard
Jasen J. Castillo
Dan Madden
Ilana Blum
Aaron Frank
Benjamin J. Fernandes
In Hyo Seol
Christopher Paul
Andrew Parasiliti
National Will
Why Some Nations Keep Fighting
Others Don
Santa Monica
See Barry G. Silverman
Jason Cornwell
Kevin O'Brien
Human Performance Simulation
James W. Ness
Darren R. Ritzer
Victoria Tepe
Methods in Human Performance Research Toward Individual and Small Unit Simulation, Washington
See Peter Amstutz
Mitha Andra
Daniel Rice
Goal Seeking: A Cognitive Approach for Small Combat Unit Constructive Simulation, 2012 Spring Simulation Multiconference, Society for Modeling & Simulation International
Ruth Luscombe
Helen Mitchard
Exploring Simple Human Behaviour Representation Using Agent Based Distillations, Edinburgh, Australia
Defence Science and Technology Organisation
17 These include ACT-R, the 
Cognitive Enrichment Network Education Model
SOAR. See
John R. Anderson
Michael Matessa
Christian Lebiere
ACT-R: A Theory of Higher Level Cognition and Its Relation to Visual Attention," Human-Computer Interaction, Vol
12
B. Chandrasekaran
John R. Josephson
Cognitive Modeling for Simulation Goals: A Research Strategy for Computer-Generated Forces
Nasser Ghasem-Aghaee
Tuncer I. Ören
Cognitive Complexity and Dynamic Personality in Agent Simulation," Computers in Human Behavior, Vol
23
John C. Giordano
Paul F. Reynolds
Jr.
David C. Brogan
Constraints of Human Behavior Representation," Proceedings of the 2004 Winter Simulation Conference, Charlottesville, Va
Emma Norling
Frank E. Ritter
Towards Supporting Psychologically Plausible Variability in Agent-Based Human Modelling," Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems
Dario D. Salvucci
Glenn Gunzelmann
Barry G. Silverman
Toward Realism in Human Performance Simulation
V. Tepe
D. R. Ritzer
Science and Simulation of Human Performance, Bingley
Ron Sun
Multi-Agent Interaction: From Cognitive Modeling
Jeremy Owen Turner
Michael Nixon
Ulysses Bernardet
Steve DiPaola
Integrating Cognitive Architectures into Virtual Character Design, Hershey, Pa
Lynn Spencer
Peter Weyhrauch
Dynamic Representation for Evaluating the Effect of Moderators and Stress on Performance (DREEMS)
U.S. Army Human Behavior Working Group
Fort Belvoir
Robert L. Axtell
Hayek Enriched by Complexity Enriched by Hayek," Advances in Austrian Economics
Robert L. Axtell
J. Doyne Farmer
Agent-Based Modeling in Economics and Finance: Past, Present, and Future," Complexity Handbook, Stuttgart, Germany: University of Hohenheim
Robert L. Axtell
Alan Kirman
Iain D. Couzin
Daniel Fricke
Thorsten Hens
Michael E. Hochberg
John E. Mayfield
Peter Schuster
Rajiv Sethi
Challenges of Integrating Complexity and Evolution into Economics
David S. Wilson
Alan Kirman
Toward a New Synthesis for Economics, Cambridge, Mass
Thomas S. Kuhn
Vintage Books
1957
I thank the editors of this report for their editorial support and encouragement. 47 
James D. Fearon
Rationalist Explanations for War," International Organization, Vol
49
Kristopher W. Ramsay
Annual Review of Political Science, Vol. 20
Emily Ward
Special thanks go to 
Marek Posard
Bryan Rooney
Elizabeth M. Bartels
James R. Watson
Simon Levin
Srikar Valluri
Anne Devan-Song
Michael J. Gaines
We would like to thank 
Adam Russell
Steven Wax
Marta Kastens
Shin-Hyung Ahn
James Ryseff
Adrian Salas
Defense Advanced Research Projects Agency
DARPARAND National Security
DARPA
Acquisition and Technology Policy Center of the RAND National Security Research Division (NSRD)
National Defense Research Institute (NDRI)
Office of the Secretary of Defense
Joint Staff
Unified Combatant Commands
Navy
Marine Corps
For more information on the 
RAND Acquisition and Technology Policy Center
Second, we would like to thank our reviewers, 
Dan Patt
Angela O'Mahony
Timothy Gulden
Third, we would like to thank the leadership of the 
Acquisition and Technology Policy Center
Associate Directors Yun Kang
Caitlin Lee
The chapters in this report build on three key considerations:
1. What are UGS? 2. Why should they be engaged in? 3. How can that be done more effectively?
The authors have extended these questions to discuss the investments that sponsors of sci- 
Another way to collect ideas and create coherent knowledge involves a combination of gaming, game-structured simulation, and model-based analysis.
The virtues of human wargaming have been rediscovered in recent years. Books on the subject exist, 56 as do professional conferences. 57 Another chapter in this report is devoted to gaming. 58 The shortcomings of human wargaming are also well known (Table 
12
In testing, we integrated the will-to-fight factor results with the parameterized five-factor personality scores for each agent to produce realistic (yet still not authentic) traits. For example, we applied a formula to generate an individual agent trait score for devotion, which we define as dedication to a cause, country, unit, and mission. In our tabletop exercise and simulations, we used devotion as a threshold check when agent will to fight dropped to the point at which it would produce undesirable behaviors, such as freezing or flight. 63 The formula is as follows:
As of our latest implementation, this formula is still a subject-matter-expert-informed, system-of-systems, realistic stand-in for what will eventually need to be an authentically derived formula.
In 2020, working with colleagues in both the government and private enterprise, we created the basic framework for a functional biopsychosocial human behavioral model. We have shown the viability of this holistic approach in peer-reviewed research reporting, in tabletop exercises, and in multiple constructive simulations. We have also integrated proxies for complex fatigue models and for resting heart rate variability-a measurement that shows considerable promise for exploring the links between the five-factor psychological model and models of human physiology. 64  The next step in the development of the biopsychosocial model is to integrate functioning subsystemic models for the four main factors-psychological, physiological, cognitive, and social-into a functional input model and then into a constructive simulation. 63 All scores were translated to a consistent 20-point scale in both the tabletop exercise and the simulation. 64  
Even without a functioning system-of-systems biopsychosocial model, one can take the components identified in this chapter to create a digital twin model that can be used to support such activities as rapid-cycling human behavior forecasting.
A digital twin is a computerized replication of an entity, in this case an individual person. By building a biopsychosocial model that represents the key individual subsystems and traitssimply an aggregation of the kinds of models and data that are typically collected and used The COMPOEX System-of-Systems An interacting family of models can provide astonishing, yet legitimate and plausible outcomes  The economic and cultural ties that connect people, states, and regions are the source of both cooperation and conflict. 1 Even during this prolonged period of economic growth and peace 2 -or at least the absence of open warfare between great powers-the international system remains a competitive environment that presents many threats from global actors, most notably Russia and China. The international system also encompasses environments in which conflict might need to be contained (e.g., Syria) or in which the rules of the road are actively contested (e.g., cyberspace). In this chapter, we examine international competition waged through exponential technologies-most notably, modern social networking technologies whose reach and value scales exponentially with each new user 3 -through the lens of biological evolution and adaptation.
The proliferation and expansion of capabilities for exerting influence through social networks-such as ubiquitous access to tailored information-have exposed vulnerabilities that can be manipulated to create fissures within states by malevolent actors who are seeking and exploiting new methods of influence. 4 These methods include the intentional manipula-As mentioned, the energetic cost of the immune system is high. For example, a 2°F fever in a 175-lb male uses approximately 250 calories per day, which, for comparison, is between the total number of calories used by the heart, 168, and the brain, 373. 29 Moreover, research on modified mice has shown that the energetic costs of mounting a defense against an infection are even higher in the absence of an adaptive immune system; 30 if necessary, the body will divert resources away from many nonessential functions, thus imposing further costs in terms of decreased biological and social functioning. 31  Given these costs, many additional considerations arise that add nuance to how the immune system is understood because it must be constrained in its use. For example,
• How strong should the immune response to an infection be?
• How does the immune system differentiate the self from the non-self entities? • How does the immune system consider context? • How does the immune system ramp up and wind down?
• What bodily functions are prioritized for protection? Answers to these questions, which we present in the following subsections, offer nuance to the inner workings of the immune system and provide additional detail for developing and testing concepts for defending society.
The vertebrate immune system provides resilience to both known and unknown pathogens. However, could it be better? Why do we not have stronger immunity? Perhaps surprisingly, the strength of the vertebrate immune system varies from species to species. Sharks are said to never get sick, and whales and other large animals do not get cancers. 32 So, why do we? Ultimately, this comes down to evolution and the trade-offs between fitness and immunity. There is not an increasing monotonic relationship between fitness and immunity. Instead, it is concave, with some optimum largely defined by one's environment (i.e., one's needs). We Games have long been an important part of defense analysis that are used to understand new strategic and operational problems, develop strategies and concepts, and assess the potential shortcomings of plans. The ability of games to help policy professionals explore the key elements of new problems and the relationship between them makes them a highly effective tool to help decisionmakers make sense of undergoverned spaces (UGS). However, existing approaches to games for doing research and analysis tend to fall short, either by exhibiting the same types of pathologies as modeling and simulation efforts or by failing to generate credible information to systematically advance understanding. In this chapter, we explore the potential value of gaming in policymaking for UGS, describe two common failure modes, and offer several approaches for improving games to explore these spaces. We conclude the chapter by offering a vision for a new game concept-a contest arena-which combines advances in several areas that could improve the ability of games to inform adaptive planning in UGS.
The U.S. Department of Defense (DoD) defines a wargame as the "representation of conflict or competition in a synthetic environment, in which people make decisions and respond to the consequences of those decisions." 1 In practice, various terms have been used to describe associated with mathematical models based on empirical data. 14 Farther up the spectrum, problems are characterized by strategic indeterminacy, so game theory can be used. 15 At this point on the spectrum, the structure of the problem is known-the actors, their available choices, and their payoffs-and analysis allows an understanding of how behavior is likely to unfold. At the upper end of the spectrum is structural indeterminacy, which is when the key elements of the policy problem and how they relate to one another are not yet known. This is where games come into play. 14   
Gaming Structural
There Is High Dimensionality of Interaction and Competition
The third problem with Approach 1 is that expanding from a focus on the military to address political, economic, social, information, and infrastructure concerns means that games for UGS have a dramatically wider scope than traditional wargames. 35 The need to cover multiple, interrelated topics is often referred to as the problem of dimensionality. Other chapters in this report have discussed several conceptual and technical challenges that are associated with developing and maintaining high-dimensional models of UGS, such as the demands of high-resolution modeling; disparate data; computational power; and the management of a federated infrastructure of dynamically changing models and submodels. 36  Beyond the difficulty of managing such models, attempting to use them in support of games adds additional challenges. First, unless models are carefully designed to allow flexibility, it is often difficult to add options for players to take decisions that are not prespecified by the model's designers. If the model cannot be changed to incorporate player knowledge, the game offers few opportunities for the game designers to learn from the players and, thus, is of less value for research. 37 Players might also become frustrated if their desired action is not allowed, disincentivizing player engagement and biasing the results of the game. 38 A second challenge of high dimensionality models is that they often have complicated interfaces that require more time, patience, and expertise to learn than either designers or players possess. 39 Third, it can be difficult to understand how actions generate effects-that is, the underlying causal model can be opaque. 40 The model acts as a black box that produces results that either align with players' expectations-and, thus, are accepted as legitimate without adding to researchers' understanding of the problem-or do not align and are easily dismissed by players who cannot unpack the causal processes that are generating the results. High-dimensional models are difficult to build in the first place, and they are even more dif-
The approaches that we have discussed seek to move from the extremes of Approach 1 and Approach 2 gaming to positions closer to the middle. But what if it were possible to borrow from both approaches by depending on high-complexity modeling while still centering human sensemaking and decisionmaking? In this section, we present one such possibility, which we have dubbed a contest arena. 77 In many ways, this concept draws on the same types of developments discussed for improving the state of Approach 1 and Approach 2 gaming but goes a step further in seeking a synthesis of the two as a third way to design and conduct games.
In defining the contest arena concept, we retain the standard elements of a game-the environment, players, and rules that govern the relationships among them. However, we envision technological tools, most prominently agent-based models, that serve as interactive game boards that can evolve and change over time. Players can access and interact with this board through various HCI and VAMR technologies. These technologies digitally capture and facilitate the exchange of information between the human players and the models that make up the game board. Moreover, through the capture of more of the players' cognition, 77 The authors are grateful to Adam Russell, formerly of the Defense Advanced Research Project Agency's Defense Sciences Office, for his invaluable leadership in the development of this concept. While it is customary to end reports with findings or conclusions, such an ending would be out of place here. Findings or conclusions should be reserved for the end of a journey. Instead, we are at the beginning. The preceding chapters are the first of many steps to develop the concepts, approaches, and tools needed to help the U.S. Department of Defense (DoD) and National Security Enterprise (NSE) navigate undergoverned spaces (UGS) in ways that are built on robust theoretical and empirical foundations. Therefore, instead of conclusions, we end with a few themes that we believe provide promising pathways for future progress.
UGS will endure as a strategic challenge that DoD and the NSE must face, regardless of whether the United States prioritizes counterterrorism, confrontation of regional powers, or long-term competition with global rivals. UGS represent areas in which nonstate threats can emerge, regional conflicts can spark into proxy wars, and great powers can shift the status quo to their benefit. Thus, although the strategic goals and motives for intervention may differ, the need to make sense of and successfully act in these spaces is critical. Today, much of the focus on UGS is driven by the strategic demands of great-power competition in U.S. strategy. Here, UGS are important as an arena for competition, which can manifest in such ways as proxy warfare, gray-zone activities, or competitive rulemaking. At the same time, actors ranging from subnational political organizations to multinational firms are also acting in these spaces to secure their own interests. This creates an environment in which multiple actors could be both cooperating and competing across overlapping issue areas, generating a complex adaptive environment that requires greater demands for understanding and for nuanced action in pursuit of the national interest. Paul K. Davis is an adjunct senior principal researcher at the RAND Corporation. His research focuses on strategic planning for complex systems and analytic methods for aiding decisionmakers. He has a Ph.D. in theoretical chemical physics.
Kelly Elizabeth Eusebi is a research assistant at the RAND Corporation. She has a background in complex systems and holds bachelor's degrees in economics and international relations from Michigan State University.
Aaron B. Frank is a senior information scientist at the RAND Corporation. His research focuses on the intersection of social science theory and computational methods and on their application to intelligence and strategic assessment. He has a Ph.D. in computational social science.